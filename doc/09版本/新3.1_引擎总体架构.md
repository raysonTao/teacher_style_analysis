## 3.1 引擎总体架构

本研究以"基于课堂录像的教师风格画像分析"为核心目标，提出**SHAPE (Semantic Hierarchical Attention Profiling Engine，语义层次化注意力画像引擎)**，构建一个集多模态特征提取、风格映射建模、画像生成与可视化反馈于一体的分析体系。

### 3.1.1 传统方法的局限性分析

现有的课堂分析与教师评价方法主要存在以下局限：

**(1) 固定分段导致语义割裂**

传统方法多采用固定时间窗口（如10秒）对课堂视频进行分段，这种机械式切分忽略了教学话语的语义边界。初步实验发现，固定10秒分段导致约23.4%的样本出现语义割裂现象：
- **逻辑推导被截断**（35%）：完整的"因为...所以...因此"逻辑链被分割到不同片段
- **概念定义不完整**（28%）："所谓X，就是..."的定义句被截断
- **案例讲解跨段**（37%）：多句案例描述被人为分割

这种割裂导致教学意图识别F1值下降约5.2%，风格识别准确率下降约2.1%（详见3.2.3节消融实验）。

**(2) 粗粒度意图识别无法区分教学策略**

传统对话行为识别多采用粗粒度四分类（提问、指令、讲解、反馈），无法有效区分不同教学风格的特征性语言模式。例如：
- "讲解"类过于宽泛，无法区分"逻辑推导型"教师的推理讲解与"理论讲授型"教师的概念定义
- "提问"类无法区分启发性提问与事实性提问，难以刻画"启发引导型"风格

**(3) 简单融合忽略模态交互**

早期融合（Early Fusion）直接拼接特征，晚期融合（Late Fusion）固定权重加权，均未考虑：
- 不同模态在不同样本上的重要性差异（样本自适应性）
- 模态之间的交互关系（跨模态增强）
- 决策依据的可解释性（注意力权重可视化）

实验表明，简单拼接相比跨模态注意力融合，准确率下降6.2个百分点（详见3.4.2节）。

**针对上述局限，本研究提出三项核心创新**：
1. **语义驱动分段策略**（3.2节）：保证教学话语的语义完整性
2. **层次化细粒度意图识别（H-DAR）**（3.3节）：10类细分体系精细刻画教学策略
3. **SHAPE跨模态注意力融合**（3.4节）：样本自适应的模态交互机制

### 3.1.2 SHAPE引擎的设计理念

在教育信息化与人工智能技术的背景下，教师课堂行为与教学风格的客观识别与分析是推动教学质量评价科学化的重要方向。本研究借助**多模态学习分析（MMLA）**框架，综合运用计算机视觉、语音识别与自然语言处理等技术，对教师在课堂中的非言语行为与语言特征进行量化建模，从而构建教师风格画像，实现教学风格的客观、可解释识别。

系统总体思路遵循**"数据采集 → 特征提取 → 模态融合 → 风格映射 → 画像生成"**的技术路线，核心设计理念在于：

1. **语义完整性优先**：摒弃固定时间窗口分段，采用语义驱动策略保证每个分析单元的语义完整性
2. **细粒度教学建模**：从粗粒度4类扩展到层次化10类，精细刻画教学策略差异
3. **跨模态自适应融合**：通过注意力机制实现样本自适应的模态重要性调整
4. **端到端可解释**：从原始数据到风格标签的完整可追溯路径，增强模型可信度

### 3.1.3 引擎四层架构

SHAPE引擎采用四层架构设计，如图3.1所示：

**【建议插入图3.1：SHAPE引擎四层架构图】**

（图应包含：数据层 → 特征层 → 融合层 → 应用层，每层标注关键技术和创新点）

#### **第一层：数据预处理层（Data Preprocessing Layer）**

**核心功能**：课堂视频采集、多模态数据同步、语义驱动分段

**关键技术**：
- **数据同步**：采用音频波形匹配算法（Cross-Correlation）实现视频与音频的精确对齐
  - 设视频音轨为 $a_{v}(t)$，独立音频为 $a_{s}(t)$
  - 时间偏移量 $\tau^{\ast} = \arg\max_{\tau}\sum_{t}^{}a_{v}\lbrack t\rbrack \cdot a_{s}\lbrack t + \tau\rbrack$

- **语义驱动分段**⭐（创新点1，详见3.2节）：
  - ASR全文转写 → 句子边界检测 → 依存句法分析 → 话语边界检测
  - 输出：$N \approx 150 \sim 200$ 个语义完整单元/45分钟课
  - 性能：语义完整率从76.6%提升至95.3%（+18.7%）

#### **第二层：特征提取层（Feature Extraction Layer）**

**核心功能**：三模态并行特征提取，生成深度语义表征

**三模态Pipeline**（并行处理，总耗时0.82s/单元）：

**(1) 视觉模态（20维）**：
- YOLOv8教师检测 → DeepSORT追踪（ID稳定性93.8%）
- MediaPipe骨骼点提取（33个关键点）
- ST-GCN时空图卷积建模
- 输出：$F_v \in \mathbb{R}^{20}$（步态、手势、位置移动等）

**(2) 音频模态（15维）**：
- Wav2Vec 2.0深度声学表征（vs MFCC +6.4%）
- 情感分类头微调（5维情感特征）
- 输出：$F_a \in \mathbb{R}^{15}$（韵律、情感、停顿等）

**(3) 文本模态（35维）**⭐（创新点2，详见3.3节）：
- Whisper Large-v3 ASR转写
- BERT编码 → H-DAR层次化10类意图识别
- 输出：$F_t \in \mathbb{R}^{35}$（意图分布、关键词密度等）

#### **第三层：融合分类层（Fusion & Classification Layer）**

**核心功能**：跨模态注意力融合，7类风格分类

**SHAPE五模块网络**⭐（创新点3，详见3.4节）：
1. **特征投影层**：$F_v, F_a, F_t \to F'_v, F'_a, F'_t \in \mathbb{R}^{512}$（统一特征空间）
2. **跨模态注意力层**：计算6个注意力权重 $\alpha_{i \rightarrow j}$，实现模态交互
3. **BiLSTM时序建模**：捕捉课堂时序依赖
4. **注意力池化层**：自适应聚合关键片段
5. **风格分类器**：输出7类风格概率分布

**性能提升**：
- vs Early Fusion（拼接）：+6.2%准确率
- vs Late Fusion（加权）：+3.8%准确率

#### **第四层：画像生成层（Profiling & Application Layer）**

**核心功能**：风格画像生成、可解释性分析、可视化输出

**三大输出**：
1. **风格分类结果**：主导风格 + 置信度 + Top-2风格（覆盖率98.1%）
2. **模态贡献度分析**：基于跨模态注意力权重 $\alpha$（例：情感表达型 $\alpha_{\text{audio}}=0.62$）
3. **典型片段提取**：基于注意力池化权重 $\beta$（Top-K关键时刻回放）

**可解释性设计**（详见5.2.3节）：
- SHAPE原生可解释性：注意力权重 $\alpha, \beta$ 可视化
- SHAP特征归因：70维特征的贡献度排序
- 教育语义映射：模型输出 → 教育术语转换

### 3.1.4 实验总体设置

为验证SHAPE引擎的有效性，我们设计了系统的实验方案。

#### （1）数据集与划分

**数据来源**：mm-tba数据集 + 网络自建数据集

**样本分布**：209个完整课堂视频，7类教学风格

**数据集划分**：
- 训练集：$D_{\text{train}} = 125$样本（60%）
- 验证集：$D_{\text{val}} = 31$样本（15%）
- 测试集：$D_{\text{test}} = 53$样本（25%）

**类别平衡性处理**：使用加权交叉熵损失

$$\mathcal{L}_{\text{weighted}} = - \sum_{i = 1}^{N}{\sum_{k = 1}^{7}w_{k}} \cdot y_{i,k}\log\left( {\widehat{y}}_{i,k} \right)$$

其中，类别权重 $w_{k} = \frac{N}{7 \cdot n_{k}}$（$n_{k}$ 是类别样本数）

#### （2）实验环境配置

**硬件配置**：
- GPU：NVIDIA RTX 3090（24GB VRAM）
- CPU：Intel Xeon Gold 6248R（3.0GHz, 48核）
- 内存：128GB DDR4

**软件环境**：
- 操作系统：Ubuntu 20.04 LTS
- 深度学习框架：PyTorch 2.0.1 + CUDA 11.8
- Python版本：3.10.12

**训练超参数**：
- 优化器：Adam，初始学习率 $\eta_{0} = 10^{-4}$
- 学习率调度：余弦退火（Cosine Annealing）
- Batch Size：32
- Epochs：50（早停策略：验证集损失连续10轮不下降）
- 正则化：Dropout(0.3) + 权重衰减（$\lambda = 10^{-4}$）

#### （3）评估指标体系

**分类性能指标**：

**准确率（Accuracy）**：
$$\text{Accuracy} = \frac{1}{N}\sum_{i = 1}^{N}\mathbb{1}\left( {\widehat{y}}_{i} = y_{i} \right)$$

**宏平均F1（Macro-F1）**：
$$\text{Macro-F1} = \frac{1}{K}\sum_{k = 1}^{K}F1_{k}$$

其中，$F1_{k} = 2 \times \frac{\text{Precision}_{k} \times \text{Recall}_{k}}{\text{Precision}_{k} + \text{Recall}_{k}}$

**Cohen's Kappa系数**：
$$\kappa = \frac{p_{o} - p_{e}}{1 - p_{e}}$$

Kappa值解释：$\kappa < 0.4$（一致性差），$0.4 \leq \kappa < 0.75$（中等），$\kappa \geq 0.75$（实质性一致）

**统计显著性检验**：

**配对t检验（Paired t-test）**：用于比较两个模型的性能差异
$$t = \frac{\bar{d}}{s_{d}/\sqrt{n}}$$

其中，$\bar{d}$ 是均值差异，$s_{d}$ 是标准差，在显著性水平 $\alpha = 0.05$ 下，当 $|t| > t_{\alpha/2,n - 1}$ 时拒绝原假设。

**McNemar检验**：用于消融实验，检验模块移除的影响
$$\chi^{2} = \frac{\left( n_{12} - n_{21} \right)^{2}}{n_{12} + n_{21}}$$

当 $\chi^{2} > \chi_{0.05,1}^{2} = 3.84$ 时，认为模块移除的影响显著。

**Cohen's d效应量**：
$$d = \frac{\mu_1 - \mu_2}{\sigma_{\text{pooled}}}$$

效应量解释：$d < 0.2$（小效应），$0.2 \leq d < 0.8$（中效应），$d \geq 0.8$（大效应）

#### （4）基线方法设计

为验证SHAPE引擎各组件的有效性，我们设计了以下基线方法：

**单模态基线**：
- Video-only：仅使用视觉特征（$F_v$）
- Audio-only：仅使用音频特征（$F_a$）
- Text-only：仅使用文本特征（$F_t$）

**简单融合基线**：
- Early Fusion：直接拼接三模态特征 $F_{\text{concat}} = [F_v; F_a; F_t]$
- Late Fusion：加权平均单模态预测 $P_{\text{final}} = w_vP_v + w_aP_a + w_tP_t$

**组件消融基线**：
- w/o Semantic Segmentation：使用固定10秒分段
- w/o H-DAR：使用关键词规则或单层BERT-10类
- w/o Cross-Modal Attention：移除跨模态注意力，使用简单拼接

### 本节小结

本节介绍了SHAPE教师风格画像引擎的总体架构：
1. **设计动机**：针对传统方法的三大局限（语义割裂、粗粒度意图、简单融合）
2. **四层架构**：数据层（语义分段）→ 特征层（多模态深度表征）→ 融合层（跨模态注意力）→ 应用层（画像生成）
3. **三项创新**：语义驱动分段、H-DAR细粒度识别、SHAPE跨模态融合
4. **实验设计**：数据集、环境、指标、基线方法

后续章节将详细介绍三项核心创新的设计与实验验证（3.2-3.4节）、多模态特征提取实现（3.5节）、引擎整体性能评估（3.6节）。
