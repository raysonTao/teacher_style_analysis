# 数学公式模板

本文档提供了论文中常用数学公式的LaTeX模板，可以直接复制到Word中（使用MathType或Word自带公式编辑器）。

---

## 1. 注意力机制相关公式

### 1.1 自注意力（Self-Attention）

给定输入特征 $X \in \mathbb{R}^{n \times d}$，首先通过线性变换得到查询（Query）、键（Key）和值（Value）：

$$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$$

其中，$W_Q, W_K, W_V \in \mathbb{R}^{d \times d_k}$ 是可学习的权重矩阵。

自注意力的计算公式为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

其中，$\sqrt{d_k}$ 是缩放因子，用于稳定梯度。

### 1.2 跨模态注意力（Cross-Modal Attention）

对于模态 $i$ 和模态 $j$ 的特征 $F_i \in \mathbb{R}^{d_i}$ 和 $F_j \in \mathbb{R}^{d_j}$，跨模态注意力权重计算为：

$$\alpha_{i \rightarrow j} = \text{softmax}\left(\frac{(F_i W_Q^i)(F_j W_K^j)^T}{\sqrt{d_k}}\right)$$

注意力加权后的特征为：

$$\tilde{F}_i = F_i + \alpha_{i \rightarrow j}(F_j W_V^j)$$

这里使用了残差连接（Residual Connection）来保留原始特征。

### 1.3 多头注意力（Multi-Head Attention）

多头注意力机制将特征投影到 $h$ 个不同的子空间：

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W_O$$

其中，第 $i$ 个头的计算为：

$$\text{head}_i = \text{Attention}(QW_Q^i, KW_K^i, VW_V^i)$$

$W_O \in \mathbb{R}^{hd_v \times d}$ 是输出投影矩阵。

---

## 2. 特征融合相关公式

### 2.1 特征拼接（Concatenation）

最简单的融合方式是将多个模态的特征直接拼接：

$$F_{\text{concat}} = [F_v; F_a; F_t] \in \mathbb{R}^{d_v + d_a + d_t}$$

其中，$[;]$ 表示拼接操作。

### 2.2 加权融合（Weighted Fusion）

为每个模态分配一个可学习的权重：

$$F_{\text{fused}} = w_v F_v + w_a F_a + w_t F_t$$

权重通过softmax归一化：

$$w_i = \frac{\exp(\alpha_i)}{\sum_{j \in \{v,a,t\}} \exp(\alpha_j)}$$

其中，$\alpha_i$ 是可学习参数。

### 2.3 双线性融合（Bilinear Fusion）

双线性融合可以建模两个模态之间的二阶交互：

$$F_{\text{bilinear}} = F_v^T W F_a$$

其中，$W \in \mathbb{R}^{d_v \times d_a}$ 是可学习的双线性权重矩阵。

### 2.4 Tucker分解（Tucker Decomposition）

为了降低双线性融合的参数量，可以使用Tucker分解：

$$F_{\text{tucker}} = (F_v^T U_v)^T \mathcal{G} (F_a^T U_a)$$

其中，$U_v \in \mathbb{R}^{d_v \times r}, U_a \in \mathbb{R}^{d_a \times r}$ 是降维矩阵，$\mathcal{G} \in \mathbb{R}^{r \times r}$ 是核心张量。

---

## 3. 时空图卷积网络（ST-GCN）

### 3.1 图卷积操作

对于骨骼图 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$，其中 $\mathcal{V}$ 是关节点集合，$\mathcal{E}$ 是连接边。

图卷积操作定义为：

$$f_{\text{out}}(v_i) = \sum_{v_j \in \mathcal{N}(v_i)} \frac{1}{Z_i(v_j)} f_{\text{in}}(v_j) \cdot w(l_i(v_j))$$

其中：
- $\mathcal{N}(v_i)$ 是节点 $v_i$ 的邻域
- $Z_i(v_j) = |\{v_k | l_i(v_k) = l_i(v_j)\}|$ 是归一化因子
- $l_i(v_j)$ 是节点 $v_j$ 在节点 $v_i$ 邻域中的标签（例如：自身、相邻、远端）
- $w(\cdot)$ 是可学习的权重函数

### 3.2 时空图卷积

将时间维度也纳入图结构，时空图卷积定义为：

$$f_{\text{out}}(v_{ti}) = \sum_{v_{t'j} \in B(v_{ti})} \frac{1}{Z_{ti}(v_{t'j})} f_{\text{in}}(v_{t'j}) \cdot W(l_{ti}(v_{t'j}))$$

其中，$B(v_{ti})$ 是节点 $v_{ti}$ 的时空邻域：

$$B(v_{ti}) = \{v_{t'j} | d(v_j, v_i) \leq K, |t'-t| \leq \lfloor \tau/2 \rfloor\}$$

- $K$ 是空间距离阈值
- $\tau$ 是时间窗口大小

---

## 4. 损失函数

### 4.1 交叉熵损失（Cross-Entropy Loss）

对于 $K$ 分类任务，交叉熵损失定义为：

$$\mathcal{L}_{\text{CE}} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})$$

其中：
- $N$ 是样本数量
- $y_{i,k} \in \{0, 1\}$ 是真实标签（one-hot编码）
- $\hat{y}_{i,k} = \text{softmax}(z_{i,k}) = \frac{\exp(z_{i,k})}{\sum_{j=1}^{K}\exp(z_{i,j})}$ 是预测概率
- $z_{i,k}$ 是模型输出的logits

### 4.2 Focal Loss

为了解决类别不平衡问题，可以使用Focal Loss：

$$\mathcal{L}_{\text{focal}} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{k=1}^{K} \alpha_k (1-\hat{y}_{i,k})^\gamma y_{i,k} \log(\hat{y}_{i,k})$$

其中：
- $\alpha_k$ 是类别权重
- $\gamma$ 是聚焦参数，通常取2

### 4.3 标签平滑（Label Smoothing）

标签平滑可以防止过拟合：

$$y'_{i,k} = (1-\epsilon)y_{i,k} + \frac{\epsilon}{K}$$

其中，$\epsilon$ 是平滑参数，通常取0.1。

### 4.4 对比学习损失（Contrastive Loss）

如果使用对比学习预训练特征提取器：

$$\mathcal{L}_{\text{contrast}} = -\log\frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}$$

其中：
- $z_i, z_j$ 是正样本对的特征
- $\text{sim}(u, v) = u^T v / (\|u\|\|v\|)$ 是余弦相似度
- $\tau$ 是温度参数

---

## 5. 评估指标

### 5.1 分类指标

**准确率（Accuracy）：**

$$\text{Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathbb{1}(\hat{y}_i = y_i)$$

**精确率（Precision）：**

$$\text{Precision}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FP}_k}$$

**召回率（Recall）：**

$$\text{Recall}_k = \frac{\text{TP}_k}{\text{TP}_k + \text{FN}_k}$$

**F1分数：**

$$F1_k = 2 \times \frac{\text{Precision}_k \times \text{Recall}_k}{\text{Precision}_k + \text{Recall}_k}$$

**宏平均F1（Macro-F1）：**

$$\text{Macro-F1} = \frac{1}{K}\sum_{k=1}^{K} F1_k$$

**加权平均F1（Weighted-F1）：**

$$\text{Weighted-F1} = \sum_{k=1}^{K} \frac{n_k}{N} F1_k$$

其中，$n_k$ 是类别 $k$ 的样本数量。

### 5.2 混淆矩阵（Confusion Matrix）

混淆矩阵 $C \in \mathbb{R}^{K \times K}$，其中元素 $C_{ij}$ 表示真实类别为 $i$、预测类别为 $j$ 的样本数量。

**类别准确率（Class Accuracy）：**

$$\text{Accuracy}_i = \frac{C_{ii}}{\sum_{j=1}^{K} C_{ij}}$$

### 5.3 Top-K准确率

$$\text{Top-K-Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathbb{1}(y_i \in \text{Top-K}(\hat{y}_i))$$

其中，$\text{Top-K}(\hat{y}_i)$ 表示模型预测概率最高的 $K$ 个类别。

---

## 6. 优化算法

### 6.1 Adam优化器

$$m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}$$
$$\theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

其中：
- $g_t$ 是第 $t$ 步的梯度
- $m_t, v_t$ 是一阶和二阶矩估计
- $\beta_1, \beta_2$ 是衰减率，通常取0.9和0.999
- $\eta$ 是学习率
- $\epsilon$ 是数值稳定项，通常取1e-8

### 6.2 余弦退火学习率（Cosine Annealing）

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{T_{\text{cur}}}{T_{\max}}\pi\right)\right)$$

其中：
- $\eta_{\max}$ 是初始学习率
- $\eta_{\min}$ 是最小学习率
- $T_{\text{cur}}$ 是当前epoch
- $T_{\max}$ 是总epoch数

---

## 7. 可解释性分析

### 7.1 梯度加权类激活映射（Grad-CAM）

对于卷积层 $k$ 的特征图 $A^k \in \mathbb{R}^{h \times w \times c}$，类别 $c$ 的Grad-CAM为：

$$\alpha_k^c = \frac{1}{Z}\sum_{i}\sum_{j} \frac{\partial y^c}{\partial A_{ij}^k}$$

$$L_{\text{Grad-CAM}}^c = \text{ReLU}\left(\sum_{k} \alpha_k^c A^k\right)$$

其中，$y^c$ 是类别 $c$ 的预测分数，$Z$ 是归一化因子。

### 7.2 注意力可视化

对于注意力权重矩阵 $\alpha \in \mathbb{R}^{n \times n}$，可以通过热图可视化：

$$\text{Heatmap}(i, j) = \alpha_{ij}$$

这可以展示模型在做决策时关注了哪些输入部分。

---

## 8. 归一化技术

### 8.1 Batch Normalization

$$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
$$y_i = \gamma \hat{x}_i + \beta$$

其中：
- $\mu_B = \frac{1}{m}\sum_{i=1}^{m}x_i$ 是batch均值
- $\sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m}(x_i - \mu_B)^2$ 是batch方差
- $\gamma, \beta$ 是可学习的缩放和偏移参数

### 8.2 Layer Normalization

$$\hat{x}_i = \frac{x_i - \mu_L}{\sqrt{\sigma_L^2 + \epsilon}}$$

其中，$\mu_L, \sigma_L$ 是在特征维度上计算的均值和方差（而不是batch维度）。

---

## 使用建议

1. **Word中插入公式：**
   - 使用Word内置公式编辑器（插入 → 公式）
   - 或安装MathType插件

2. **LaTeX文档：**
   - 直接复制到`.tex`文件的equation环境中
   - 例如：`\begin{equation} ... \end{equation}`

3. **编号：**
   - Word自动编号：右键公式 → 插入题注
   - LaTeX自动编号：使用`\begin{equation}`而不是`\begin{equation*}`

4. **引用：**
   - 在正文中写："如公式(3.1)所示，注意力机制..."
   - 确保所有公式都被引用至少一次

---

## 检查清单

- [ ] 所有变量都有定义（例如：$d_k$ 是什么）
- [ ] 公式编号连续（3.1, 3.2, 3.3...）
- [ ] 所有公式在正文中至少被引用一次
- [ ] 符号使用一致（不要一会儿用 $W$，一会儿用 $w$）
- [ ] 复杂公式有文字解释
- [ ] 矩阵/向量维度标注清楚（例如：$W \in \mathbb{R}^{d \times k}$）
