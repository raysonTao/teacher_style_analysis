# 教师风格数据集从零构建方案

> **状态：当前无可用数据集，本文档为从零构建的完整执行方案。**
>
> 目标：在 4–6 周内构建一个规模约 350–500 个会话级样本、覆盖 7 类教学风格的标注数据集，具备可发表级可信度。

---

## 一、整体思路

教学风格分类是**会话级别**的任务，需要连续 5–20 分钟的教学片段才能做出可靠判断。数据来源分两条并行路线：

| 路线 | 数据源 | 优势 | 局限 |
|------|--------|------|------|
| A | **MM-TBA**（Nature Scientific Data, 2025） | 已标注动作标签 + 讲课评估报告，可引用权威期刊 | 单片段仅 3–10 秒，需聚合重建 |
| B | **公开网络课程视频**（B站/中国大学MOOC/学堂在线） | 完整长视频，风格特征明显，风格多样 | 需人工筛选，版权须说明（学术研究用途） |

两条路线并行，最终合并，预计总规模 **350–500 个样本**。

---

## 二、路线 A：基于 MM-TBA 的会话级重建

### 2.1 下载数据集

MM-TBA 已在 Figshare 公开，可直接下载：

- **论文**：[A Multi-Modal Dataset for Teacher Behavior Analysis in Offline Classrooms](https://www.nature.com/articles/s41597-025-05426-6)（Nature Scientific Data, 2025）
- **数据下载地址**：https://figshare.com/articles/dataset/MM-TBA/28942505
- **内容**：4,839 个视频片段 + 标注 JSON（动作类别、时间戳、边界框、姿态关键点、讲课评估报告）

```bash
# 安装 figshare 下载工具或直接 wget
pip install requests tqdm
# 或访问页面手动下载 zip 压缩包（约 XX GB）
```

### 2.2 数据集结构分析

下载后检查 JSON 标注文件结构，关键字段：

```json
{
  "teacher_id": "T001",
  "session_id": "S003",
  "clip_id": "C0045",
  "start_time": 125.3,
  "end_time": 131.7,
  "action_label": "gesturing",
  "evaluation_report": "该教师注重引导学生思考，常用追问方式...",
  "keypoints": [...]
}
```

### 2.3 会话级聚合脚本

```python
# aggregate_mmmtba.py
import json
from collections import defaultdict

def aggregate_by_session(annotations):
    """按 teacher_id + session_id 聚合片段，构建会话级样本"""
    sessions = defaultdict(list)

    for clip in annotations:
        key = (clip['teacher_id'], clip['session_id'])
        sessions[key].append(clip)

    valid_sessions = []
    for (tid, sid), clips in sessions.items():
        clips_sorted = sorted(clips, key=lambda x: x['start_time'])
        total_duration = sum(c['end_time'] - c['start_time'] for c in clips_sorted)

        # 筛选条件：总时长 ≥ 5 分钟 且 有讲课评估报告
        if total_duration >= 300 and clips_sorted[0].get('evaluation_report'):
            valid_sessions.append({
                'teacher_id': tid,
                'session_id': sid,
                'clips': clips_sorted,
                'total_duration_sec': total_duration,
                'evaluation_report': clips_sorted[0]['evaluation_report'],
                'action_distribution': compute_action_dist(clips_sorted)
            })

    return valid_sessions

def compute_action_dist(clips):
    """统计该会话中各动作的占比"""
    from collections import Counter
    total = sum(c['end_time'] - c['start_time'] for c in clips)
    action_time = Counter()
    for c in clips:
        action_time[c['action_label']] += c['end_time'] - c['start_time']
    return {k: round(v/total, 3) for k, v in action_time.items()}

# 运行
with open('mmmtba_annotations.json') as f:
    annots = json.load(f)

sessions = aggregate_by_session(annots)
print(f"有效会话数：{len(sessions)}")  # 预期 150–250 个
```

**预期产出**：150–250 个有效会话级样本，每个样本包含动作分布 + 讲课评估报告文字。

---

## 三、路线 B：公开网课视频收集

### 3.1 目标平台与收集策略

| 平台 | 网址 | 推荐学科 | 特点 |
|------|------|----------|------|
| B站（bilibili） | bilibili.com/channel/education | 全科 | 量最大，风格多样 |
| 中国大学MOOC | icourse163.org | 大学各科 | 高质量，讲授型为主 |
| 学堂在线 | xuetangx.com | 大学各科 | 清华/知名校课程 |
| 人教版课堂录像 | 各省名师工程网站 | 中小学 | 初高中，含多种风格 |

### 3.2 各风格代表性搜索词

为确保 7 类风格均有足够样本，按风格定向搜索：

| 目标风格 | B站搜索关键词 | 期望找到的课型 |
|----------|--------------|--------------|
| 理论讲授型 | "大学物理 课堂录像" / "高中化学 讲课" | 知识点密集的理论课 |
| 启发引导型 | "苏格拉底式教学" / "数学 启发式教学" / "探究性学习" | 多设问、引导发现 |
| 互动导向型 | "英语课堂互动" / "活动型课堂" / "讨论式教学" | 学生参与度高 |
| 逻辑推导型 | "数学证明 教学" / "物理推导 高中" / "逻辑思维 课堂" | 板书密集，严格推导 |
| 题目驱动型 | "高考数学刷题" / "题型讲解" / "例题讲解 物理" | 题型分析为主 |
| 情感表达型 | "语文情感朗读" / "历史故事性教学" / "艺术鉴赏课" | 情感丰富，富感染力 |
| 耐心细致型 | "小学数学课堂" / "慢节奏讲课" / "概念细讲" | 节奏慢，反复强调 |

### 3.3 视频下载工具

```bash
# 安装 yt-dlp（支持 B 站、MOOC 等平台）
pip install yt-dlp

# B站视频下载（需设置 cookies 绕过登录限制）
yt-dlp -f "bv*+ba/best" --cookies-from-browser chrome \
  -o "data/raw/%(uploader)s_%(id)s.%(ext)s" \
  "https://www.bilibili.com/video/BV1xxxx"

# 批量下载（创建 url_list.txt，每行一个URL）
yt-dlp -f "bv*+ba/best" -a url_list.txt \
  -o "data/raw/%(uploader)s_%(id)s.%(ext)s"
```

### 3.4 筛选标准

下载后，人工快速筛选（每个视频看 5–10 分钟），保留满足以下条件的视频：

- 单人教师出镜，全程授课（非讲座/演讲）
- 视频时长 15 分钟以上
- 音频清晰，无严重噪声
- 可以感知到明显的风格特征

**目标**：每类风格 8–12 个完整视频，共约 60–80 个原始视频。

---

## 四、数据处理流程

### 4.1 视频分段

将长视频切割成**10–15 分钟**的会话级片段（一个"教学片段"是最小分析单元）：

```python
# segment_video.py
import subprocess
import json

def segment_video(video_path, segment_length=720, output_dir="segments"):
    """将视频切割为 segment_length 秒（默认 12 分钟）的片段"""
    import os
    os.makedirs(output_dir, exist_ok=True)

    # 获取视频总时长
    probe = subprocess.run([
        'ffprobe', '-v', 'quiet', '-print_format', 'json',
        '-show_streams', video_path
    ], capture_output=True, text=True)
    duration = float(json.loads(probe.stdout)['streams'][0]['duration'])

    segments = []
    start = 0
    seg_idx = 0

    while start + segment_length * 0.5 < duration:  # 至少半段才保留
        end = min(start + segment_length, duration)
        out_path = f"{output_dir}/{os.path.splitext(os.path.basename(video_path))[0]}_seg{seg_idx:03d}.mp4"

        subprocess.run([
            'ffmpeg', '-i', video_path,
            '-ss', str(start), '-t', str(end - start),
            '-c', 'copy', '-y', out_path
        ], capture_output=True)

        segments.append({'path': out_path, 'start': start, 'end': end})
        start += segment_length
        seg_idx += 1

    return segments
```

### 4.2 ASR 转写

使用 Whisper Large-v3（与论文 SHAPE 系统一致）批量转写：

```python
# transcribe.py
import whisper
import os

model = whisper.load_model("large-v3")

def transcribe_batch(segment_paths, output_dir="transcripts"):
    os.makedirs(output_dir, exist_ok=True)
    results = {}

    for video_path in segment_paths:
        seg_id = os.path.splitext(os.path.basename(video_path))[0]
        transcript_path = f"{output_dir}/{seg_id}.txt"

        if os.path.exists(transcript_path):
            continue  # 跳过已转写

        result = model.transcribe(video_path, language="zh", task="transcribe")

        # 保存全文转写
        with open(transcript_path, 'w', encoding='utf-8') as f:
            f.write(result['text'])

        # 也保存带时间戳的分段文本
        with open(f"{output_dir}/{seg_id}_timestamped.json", 'w', encoding='utf-8') as f:
            json.dump(result['segments'], f, ensure_ascii=False, indent=2)

        results[seg_id] = result['text']
        print(f"转写完成：{seg_id}")

    return results
```

### 4.3 代表性帧提取

```python
# extract_frames.py
import cv2
import os

def extract_keyframes(video_path, fps_sample=0.1, output_dir="frames"):
    """每 10 秒抽取 1 帧（fps_sample=0.1），对 12 分钟视频约抽取 72 帧"""
    os.makedirs(output_dir, exist_ok=True)
    seg_id = os.path.splitext(os.path.basename(video_path))[0]

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps / fps_sample)  # 每多少帧取一帧

    frame_paths = []
    frame_count = 0
    saved_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % frame_interval == 0:
            frame_path = f"{output_dir}/{seg_id}_f{saved_count:04d}.jpg"
            cv2.imwrite(frame_path, frame)
            frame_paths.append(frame_path)
            saved_count += 1
        frame_count += 1

    cap.release()
    return frame_paths  # 约 70–80 帧/片段
```

---

## 五、MLLM 标注方案

### 5.1 推荐模型

| 模型 | 获取方式 | 显存需求 | 中文能力 | 推荐场景 |
|------|---------|---------|---------|---------|
| **Qwen2-VL-7B-Instruct** | 本地（HuggingFace） | ~18 GB | 极强 | 有 GPU 时优先 |
| **InternVL2-8B** | 本地（HuggingFace） | ~20 GB | 强 | 备选 |
| **Qwen2-VL-72B** | 阿里云 API（按 token 收费） | — | 极强 | 无 GPU 时 |
| **GPT-4o** | OpenAI API | — | 强 | 人工验证子集用 |

**最低配置推荐**：Qwen2-VL-7B-Instruct，RTX 3090（24GB）可运行。

### 5.2 完整标注 Prompt 模板

这是整个方案最关键的部分，必须精心设计以区分 7 种风格：

```python
STYLE_DEFINITIONS = """
【7类教学风格操作性定义】

1. **理论讲授型（Theory-Lecture）**
   核心特征：以讲解概念、原理、定义为主；板书或PPT内容密集；教师主导课堂节奏；
   学生主要为听讲；提问少且多为验证性问题（"对不对？"）。
   典型话语：「这个定理的证明是这样的...」「概念上我们需要理解...」

2. **启发引导型（Heuristic-Guide）**
   核心特征：多追问式提问（"为什么？"/"你怎么想的？"）；
   引导学生自主发现规律；留有等待思考时间；
   教师是"脚手架"而非主角。
   典型话语：「大家思考一下，如果是这种情况会怎样？」「谁来说说你的推导过程？」

3. **互动导向型（Interaction-Orient）**
   核心特征：频繁师生对话（互动次数多）；点名/随机提问学生；
   小组讨论环节；学生参与度高；课堂氛围活跃。
   典型话语：「来，这位同学回答一下」「大家讨论一分钟，然后分享」

4. **逻辑推导型（Logic-Deduction）**
   核心特征：严格的步骤式推导（如数学证明、物理推导）；
   板书内容多为公式推演；语言精准，逻辑严密；
   极少跳步，每一步都有明确依据。
   典型话语：「由（1）式代入（2）式，得...」「证明如下，首先假设...」

5. **题目驱动型（Problem-Driven）**
   核心特征：以例题、习题为核心组织教学；
   大量时间分析题型和解题步骤；
   教学内容以"题目讲解"为单位推进；考点导向。
   典型话语：「我们来看这道题」「这类题的做法是...」「同学们做一下，我来讲」

6. **情感表达型（Emotional-Express）**
   核心特征：语调抑扬顿挫，情感投入；
   大量使用故事、比喻、生活类比；
   有时展现激情或幽默；与学生建立情感连接。
   典型话语：「这段历史太让人震撼了！」（语调变化明显）「想象一下，你就是那个时代的人...」

7. **耐心细致型（Patient-Detail）**
   核心特征：语速缓慢，反复确认学生是否理解；
   多次重复关键知识点（换不同方式说）；
   主动询问"听懂了吗？"；对基础概念细致铺垫。
   典型话语：「我再说一遍，这个地方特别重要」「还有没有同学没明白的？」「我换个方式解释...」
"""

FEW_SHOT_EXAMPLES = """
【参考样例】（每类风格 1 例）

样例A - 理论讲授型：
  视频描述：教师站在讲台，PPT密集呈现数学公式，全程讲解，学生安静记笔记，教师未提问。
  转写片段："...牛顿第二定律F=ma，这里m是物体质量，a是加速度，方向与合外力一致。
           下面我们来看第三定律，作用力与反作用力大小相等方向相反..."
  → 判断：理论讲授型，置信度：0.95

样例B - 启发引导型：
  视频描述：教师提问后停顿等待，学生举手回答，教师追问"你为什么这样想？"。
  转写片段："...这道题我们先不急着做，大家想想为什么会有这个规律？（停顿5秒）
           好，这位同学说一下...嗯，很好，那为什么这样可以推出...？"
  → 判断：启发引导型，置信度：0.92

（其余5类略，实际 prompt 中需补全）
"""

ANNOTATION_PROMPT_TEMPLATE = """
你是一名有10年以上课堂观察经验的教育学专家，专门研究教师教学风格。

{style_definitions}

{few_shot_examples}

---

现在请分析以下教学片段，判断该教师的**主导教学风格**：

【教学场景描述】（视频帧观察）
{visual_description}

【课堂话语转写】（前500字）
{transcript_excerpt}

{evaluation_report_section}

---

请按以下步骤分析：

**第一步：观察行为特征**
- 教师的肢体行为和板书情况是什么？
- 教师与学生的互动模式是什么？
- 提问的频率和类型是什么？

**第二步：分析话语特征**
- 语言的主要功能是讲解、提问还是互动？
- 是否有明显的风格标志词句？

**第三步：综合判断**
给出最终判断，格式如下：
```
风格类别：[从7类中选一]
置信度：[0.0–1.0]
判断依据：[2–3句话，说明最关键的证据]
排除原因：[说明为何排除最相近的1–2个风格]
```
"""
```

### 5.3 标注执行脚本

```python
# annotate_styles.py
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import json, os, re
from PIL import Image

# 加载模型（一次性）
model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-7B-Instruct",
    torch_dtype="auto", device_map="auto"
)
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct")

def annotate_segment(segment_id, frame_paths, transcript, evaluation_report=None):
    """对单个视频片段进行风格标注，独立运行 3 次取多数投票"""

    # 从帧中采样最多 12 张（均匀采样，避免 token 超限）
    sampled_frames = sample_frames(frame_paths, n=12)

    # 构建 visual_description（如果模型支持直接输入图片就用图片；否则生成文字描述）
    transcript_excerpt = transcript[:500] if len(transcript) > 500 else transcript

    eval_section = ""
    if evaluation_report:
        eval_section = f"【讲课评估报告（来源：MM-TBA标注）】\n{evaluation_report}"

    prompt = ANNOTATION_PROMPT_TEMPLATE.format(
        style_definitions=STYLE_DEFINITIONS,
        few_shot_examples=FEW_SHOT_EXAMPLES,
        visual_description="[见输入图片]",
        transcript_excerpt=transcript_excerpt,
        evaluation_report_section=eval_section
    )

    votes = []
    confidences = []

    for run in range(3):  # 独立运行 3 次
        messages = [{"role": "user", "content": []}]

        # 添加帧图片
        for frame_path in sampled_frames:
            messages[0]["content"].append({"type": "image", "image": frame_path})

        # 添加文本 prompt
        messages[0]["content"].append({"type": "text", "text": prompt})

        text_input = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text_input], images=image_inputs,
            padding=True, return_tensors="pt"
        ).to(model.device)

        outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.3, do_sample=True)
        response = processor.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)

        style, confidence = parse_annotation_response(response)
        if style:
            votes.append(style)
            confidences.append(confidence)

    # 多数投票
    if not votes:
        return None

    from collections import Counter
    final_style = Counter(votes).most_common(1)[0][0]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0

    return {
        'segment_id': segment_id,
        'style': final_style,
        'confidence': avg_confidence,
        'votes': votes,
        'needs_review': avg_confidence < 0.7 or len(set(votes)) > 1
    }

def parse_annotation_response(response):
    """从模型输出中提取风格类别和置信度"""
    style_map = {
        '理论讲授': '理论讲授型', '启发引导': '启发引导型',
        '互动导向': '互动导向型', '逻辑推导': '逻辑推导型',
        '题目驱动': '题目驱动型', '情感表达': '情感表达型',
        '耐心细致': '耐心细致型'
    }
    style = None
    confidence = 0.5

    for key, val in style_map.items():
        if key in response:
            style = val
            break

    # 提取置信度
    match = re.search(r'置信度[：:]\s*([0-9.]+)', response)
    if match:
        confidence = float(match.group(1))

    return style, confidence

def sample_frames(frame_paths, n=12):
    """均匀采样 n 帧"""
    if len(frame_paths) <= n:
        return frame_paths
    step = len(frame_paths) / n
    return [frame_paths[int(i * step)] for i in range(n)]
```

---

## 六、人工验证方案

### 6.1 验证规模

- 从 MLLM 标注结果中随机抽取 **20%**（约 70–100 个样本）
- 由 **3 名具有教育学背景的研究者**独立标注（可以是导师、同门、教育学系同学）
- 标注工具：使用 Excel 或 Label Studio（本地部署，无需服务器）

### 6.2 Label Studio 快速部署

```bash
pip install label-studio
label-studio start --port 8080
# 访问 http://localhost:8080，创建项目，导入视频片段
```

项目配置（XML 格式）：
```xml
<View>
  <Video name="video" value="$video_url"/>
  <TextArea name="transcript" value="$transcript" readonly="true"/>
  <Choices name="teaching_style" toName="video" choice="single">
    <Choice value="理论讲授型"/>
    <Choice value="启发引导型"/>
    <Choice value="互动导向型"/>
    <Choice value="逻辑推导型"/>
    <Choice value="题目驱动型"/>
    <Choice value="情感表达型"/>
    <Choice value="耐心细致型"/>
  </Choices>
</View>
```

### 6.3 一致性计算

```python
# compute_agreement.py
from sklearn.metrics import cohen_kappa_score
import pandas as pd

def compute_kappa(human_labels, llm_labels):
    """计算 MLLM 标注与人工标注的 Cohen's Kappa"""
    kappa = cohen_kappa_score(human_labels, llm_labels)
    print(f"Cohen's Kappa: {kappa:.3f}")

    if kappa >= 0.80:
        print("✓ 高度一致（可直接使用）")
    elif kappa >= 0.70:
        print("✓ 实质性一致（可使用，论文中注明）")
    elif kappa >= 0.60:
        print("△ 中等一致（需优化 prompt 后对低分类重新标注）")
    else:
        print("✗ 一致性不足（需重新设计 prompt）")

    return kappa

# 使用示例
df = pd.read_csv('validation_results.csv')
kappa = compute_kappa(df['human_label'], df['llm_label'])

# 计算各类别混淆矩阵，找出最容易混淆的风格对
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(df['human_label'], df['llm_label'])
```

### 6.4 验证通过标准

| 指标 | 最低要求 | 理想目标 |
|------|---------|---------|
| MLLM vs 人工 Kappa | ≥ 0.70 | ≥ 0.80 |
| 3 名标注者间 Kappa | ≥ 0.65 | ≥ 0.75 |
| 置信度 ≥ 0.7 的样本比例 | ≥ 60% | ≥ 75% |

若 Kappa < 0.70，找出混淆最多的风格对，补充 few-shot 示例，重新标注该类别。

---

## 七、数据集质量控制与最终构成

### 7.1 过滤规则

```python
def filter_dataset(annotations):
    """最终数据集质量过滤"""
    filtered = []
    for item in annotations:
        # 规则1：保留置信度 >= 0.7 的样本
        if item['confidence'] < 0.7:
            continue
        # 规则2：3次投票中至少2次一致
        from collections import Counter
        if Counter(item['votes']).most_common(1)[0][1] < 2:
            continue
        filtered.append(item)
    return filtered
```

### 7.2 目标类别分布（最终）

| 风格类别 | 目标样本数 | 来源 |
|---------|-----------|------|
| 理论讲授型 | 60–80 | MOOC（最易找到）|
| 题目驱动型 | 60–80 | MOOC（高中刷题课）|
| 逻辑推导型 | 50–70 | MOOC + MM-TBA |
| 启发引导型 | 50–70 | 精选名师课 + MM-TBA |
| 互动导向型 | 50–70 | 英语课、讨论课 |
| 耐心细致型 | 40–60 | 初中/小学课堂 |
| 情感表达型 | 30–50 | 语文/历史/艺术（最稀少）|
| **总计** | **~350–500** | |

**注意**：情感表达型样本天然稀少，可接受略低于其他类别，论文中说明类别不平衡处理方法（加权损失函数）。

### 7.3 数据集划分

```python
from sklearn.model_selection import StratifiedKFold, train_test_split

# 方案A：5折交叉验证（推荐，充分利用所有样本）
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 方案B：固定划分（若样本 > 400）
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.15/0.8, stratify=y_temp, random_state=42
)
# 最终比例：65% 训练 / 15% 验证 / 20% 测试
```

---

## 八、时间安排

```
第 1 周
├── Day 1–2：下载 MM-TBA，运行 aggregate_mmmtba.py，得到会话列表
├── Day 3–5：从 B站/MOOC 收集 60–80 个原始视频（每天 15–20 个，人工筛选）
└── Day 6–7：运行分段脚本（segment_video.py），处理所有视频

第 2 周
├── Day 1–3：批量 ASR 转写（transcribe.py），GPU 可并行处理
├── Day 4–5：帧提取（extract_frames.py）
└── Day 6–7：调试 MLLM 标注 pipeline，先在 30 个样本上验证效果

第 3 周
├── Day 1–3：全量 MLLM 标注（约 400–600 个片段）
├── Day 4–5：过滤低置信度样本，整理标注结果
└── Day 6–7：随机抽取 20% 样本，发给人工标注者

第 4 周
├── Day 1–5：人工标注进行（标注者独立完成）
└── Day 6–7：计算 Kappa，若 < 0.70 则针对性优化 prompt 重标

第 5 周（如需优化）
├── Day 1–3：重新标注问题类别（通常是情感表达型/耐心细致型混淆）
├── Day 4–5：最终数据集组装，统计各类别分布
└── Day 6–7：运行 SHAPE 系统实验，记录结果
```

---

## 九、论文中的数据集描述（可直接使用的文字）

将以下文字整合入 chapter-3.tex 的数据集章节：

---

### 3.X 数据集构建

#### 3.X.1 数据来源

本研究采用**双源融合**策略构建教师风格标注数据集。

**来源一（MM-TBA派生）**：从 MM-TBA 数据集（发表于 Nature Scientific Data, 2025年）\cite{ref_mmtba2025} 中提取会话级样本。MM-TBA 包含超过 300 位教师的 4,839 个教学视频片段，并附有由专业观察员撰写的讲课评估报告。通过按教师ID和课程ID对连续片段进行聚合，筛选总时长不低于 5 分钟且含有评估报告的会话，共获得 × 个会话级样本。

**来源二（公开课程录像）**：从学堂在线、中国大学MOOC平台及哔哩哔哩教育频道收集覆盖数学、语文、英语、物理等 × 个学科的公开课录像共 × 段，切割为 10–12 分钟的教学片段，共产生 × 个候选样本。

#### 3.X.2 标注流程

采用**多模态大语言模型辅助标注（MLLM-Assisted Annotation）**方法：以 Qwen2-VL-7B-Instruct\cite{ref_qwen2vl} 作为标注模型，输入包括视频代表性帧（每分钟 1 帧，共 10–12 帧）和 ASR 转写文本前 500 字，配合包含 7 类风格操作性定义和各类 1 个 Few-shot 示例的 Chain-of-Thought 提示词，对每个样本独立运行 3 次并以多数投票确定最终标签。

过滤条件：保留平均置信度 ≥ 0.7 且 3 次投票中至少 2 次一致的样本（过滤率约 X%）。

#### 3.X.3 标注质量验证

从标注结果中随机抽取 20%（共 × 个样本）由 3 名具有教育学背景的研究者独立复核（3 名标注者间 Cohen's Kappa = X.XX）。将 MLLM 标注结果与人工标注结果对比，Cohen's Kappa = **X.XX**，达到"实质性一致"（0.70–0.80）标准\cite{ref_landis1977}，表明 MLLM 标注具有可接受的可靠性。

#### 3.X.4 最终数据集统计

过滤后获得最终数据集共 × 个样本，7 类风格分布如下：[表格]。由于各类风格在真实课堂中的自然频率不均衡，采用加权交叉熵损失函数处理类别不平衡（见式 X.X）。

---

## 十、关键参考文献（需加入 paper-manual.bib）

```bibtex
@article{ref_mmtba2025,
  author = {[MM-TBA 作者]},
  title = {A Multi-Modal Dataset for Teacher Behavior Analysis in Offline Classrooms},
  journal = {Scientific Data},
  year = {2025},
  publisher = {Nature Publishing Group},
  doi = {10.1038/s41597-025-05426-6}
}

@article{ref_qwen2vl,
  author = {Wang, Peng and Bai, Shuai and others},
  title = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  journal = {arXiv preprint arXiv:2409.12191},
  year = {2024}
}

@article{ref_landis1977,
  author = {Landis, J. Richard and Koch, Gary G.},
  title = {The Measurement of Observer Agreement for Categorical Data},
  journal = {Biometrics},
  volume = {33},
  number = {1},
  pages = {159--174},
  year = {1977}
}
```

---

## 附录：快速启动检查清单

- [ ] 下载 MM-TBA 数据集（Figshare：https://figshare.com/articles/dataset/MM-TBA/28942505）
- [ ] 安装依赖：`pip install yt-dlp whisper transformers qwen-vl-utils opencv-python label-studio`
- [ ] 确认 GPU 可用：`nvidia-smi`（推荐 ≥ 20GB 显存用于 Qwen2-VL-7B）
- [ ] 运行 `aggregate_mmmtba.py`，确认输出会话数 ≥ 100
- [ ] 下载 30 个试验性 MOOC 视频，运行完整 pipeline 验证
- [ ] 调试 MLLM prompt：在已知风格的 10 个样本上测试准确率 ≥ 70% 再全量标注
- [ ] 招募 3 名标注者，完成 20% 人工验证
- [ ] 计算 Kappa，达标后运行 SHAPE 实验
