\newpage
\vspace{-1cm}
\chapter*{\zihao{-3}\heiti{ABSTRACT}}
\addcontentsline{toc}{chapter}{Abstract}
\vspace{-0.5cm}

In the wave of educational digitalization, massive volumes of classroom video data urgently need to be effectively leveraged to empower teaching. Teaching style is a key determinant of classroom quality; however, traditional evaluation methods are highly subjective and suffer from delayed feedback, making them inadequate for the demand for objective, real-time, and quantifiable classroom feedback in smart education environments. To address this, the present study designs and implements a teacher teaching-style profiling system based on multimodal deep learning, aimed at delivering objective, fine-grained, and interpretable intelligent style analysis.

Existing classroom analysis techniques are constrained by two core limitations: single-modality video or audio is insufficient to comprehensively characterize teaching styles, and style recognition models fail to provide decision rationales or feature contribution insights. To address these challenges, this study proposes \textbf{SHAPE (Semantic Hierarchical Attention Profiling Engine)}, which achieves adaptive feature fusion and accurate style profiling through semantic-driven segmentation, hierarchical teaching intent recognition, and a cross-modal attention mechanism. The system incorporates the following specific contributions: (1) \textit{Data segmentation}: a semantic-driven utterance segmentation strategy is proposed, which preserves the semantic integrity of teaching utterances through dependency parsing and discourse boundary detection, substantially improving teaching intent recognition accuracy; (2) \textit{Audio modality}: audio is employed not only for speech emotion recognition fine-tuned to classroom scenarios, but also converted into text via Automatic Speech Recognition (ASR), laying the foundation for intent recognition; (3) \textit{Text modality}: BERT-based Hierarchical Dialogue Act Recognition (H-DAR) is introduced with a two-level classification architecture (4 coarse classes + 10 fine classes), extending single-level classification to fine-grained dual-level recognition and more effectively capturing the characteristic linguistic patterns of different teaching styles; (4) \textit{Visual modality}: identity recognition algorithms are employed to achieve stable teacher tracking, and Spatial-Temporal Graph Convolutional Networks (ST-GCN) perform temporal modeling of skeleton sequences, substantially outperforming single-frame action recognition; (5) \textit{Fusion and interpretability}: SHAPE adaptively fuses visual, audio, and text features via a cross-modal attention mechanism, and combines attention weights with SHAP interpretability analysis to enhance the traceability of model decision rationale.

On a self-constructed teacher style dataset (209 samples, 7 style categories), SHAPE achieves an accuracy of \textbf{92.5\%} on the style recognition task, significantly outperforming single-modality methods and simple fusion approaches. Ablation experiments further confirm that the semantic-driven segmentation strategy improves style recognition accuracy by \textbf{2.1 percentage points}, validating the effectiveness of these improvements.

A teacher classroom profiling system is also constructed to visualize the outputs of the above algorithms. The system generates intuitive and traceable teacher style profiles---including style radar charts, modality contribution analysis, and representative segment playback---providing scientific, objective, and fine-grained data support for teacher style awareness and pedagogical research.

\vspace{0.5cm}
\hspace{-1cm}
{\bf{\sihao{Keywords:}}} \textit{Teacher Teaching Style; Multimodal Learning Analytics; Cross-modal Attention; Deep Learning}
