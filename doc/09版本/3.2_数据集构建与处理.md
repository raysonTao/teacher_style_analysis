## 3.2 数据集构建与处理

### 3.2.1 数据采集流程

**硬件要求：** - 视频：1280×720分辨率，25fps，H.264编码 -
音频：16kHz采样率，单声道，PCM编码 -
存储：每节课（40分钟）约占用500MB空间

**采集策略：**

1.  固定机位拍摄，确保教师活动区域完整入画

2.  使用定向麦克风采集教师语音，降低学生噪声干扰

3.  同步记录时间戳，精度达到毫秒级

### 3.2.2 视频预处理

### （1）视频解码与抽帧

使用FFmpeg库解码视频流，按25fps提取RGB帧：

$$V = \{ v_{1},v_{2},...,v_{T}\},\quad v_{i} \in \mathbb{R}^{720 \times 1280 \times 3}$$

其中，$v_{i}$ 表示第 $i$ 帧的RGB像素矩阵。

#### （2）视频增强

为提升模型鲁棒性，对训练数据应用以下增强策略： -
**随机裁剪**：以0.8-1.0的缩放比例裁剪 -
**颜色抖动**：亮度、对比度、饱和度随机扰动（±20%） -
**时间抖动**：随机丢帧以模拟帧率不稳定

$$v_{i}\prime = \text{ColorJitter}\left( \text{RandomCrop}\left( v_{i},\text{scale} = 0.8 \right) \right)$$

#### （3）教师检测、追踪��姿态估计

视频处理采用YOLOv8[16]进行人体检测，DeepSORT[30]算法进行教师身份追踪（ID稳定性提升25.5%），MediaPipe Pose提取33个骨骼关键点。DeepSORT通过结合外观特征（ReID）和运动模型（卡尔曼滤波）实现稳定追踪，基本消除了身份漂移问题（详见4.3.1节消融实验）。

姿态估计后保留置信度>0.5的关键点，缺失点通过线性插值补全。最终输出骨骼序列$P \in \mathbb{R}^{T \times 33 \times 4}$（T帧，33关节点，每点包含x/y/z坐标和置信度），用于后续ST-GCN时序建模。

### 3.2.3 音频预处理

#### （1）音频重采样与降噪

将原始音频统一重采样到16kHz单声道，并应用谱减法（Spectral
Subtraction）降噪：

$$S_{\text{clean}}(f) = max\left( \left| S_{\text{noisy}}(f) \right| - \alpha \cdot \left| N(f) \right|,\beta \cdot \left| S_{\text{noisy}}(f) \right| \right)$$

其中： - $S_{\text{noisy}}(f)$ 是带噪语音的频谱 - $N(f)$
是噪声频谱估计（从静音段提取） - $\alpha = 2.0$ 是过减因子 -
$\beta = 0.01$ 是谱下限

#### （2）语音活动检测（VAD）

采用基于能量的VAD算法检测有效语音段。计算短时能量：

$$E(n) = \sum_{m = n - N + 1}^{n}\left| x(m) \right|^{2}$$

其中，$N$ 是窗口长度（通常取400个采样点，对应25ms）。

当 $E(n) > \theta_{\text{energy}}$ 时判定为语音帧，其中阈值
$\theta_{\text{energy}}$ 设为静音段能量均值的3倍：

$$\theta_{\text{energy}} = 3 \times \text{mean}\left( E_{\text{silence}} \right)$$

**统计特征提取**： -
**语音活动比**：$\text{VAR} = \frac{N_{\text{voice}}}{N_{\text{total}}}$ -
**静音比**：$\text{SR} = 1 - \text{VAR}$ -
**平均语速**：$\text{Speed} = \frac{N_{\text{words}}}{T_{\text{total}}}$（字/秒）

#### （3）情感特征提取

使用Wav2Vec
2.0模型提取768维深度声学嵌入，然后通过情感分类头输出6维情感分布：

$$p_{\text{emotion}} = \text{softmax}\left( W_{e}h_{\text{wav2vec}} + b_{e} \right)$$

其中： - $h_{\text{wav2vec}} \in \mathbb{R}^{768}$ 是Wav2Vec 2.0的输出 -
$W_{e} \in \mathbb{R}^{6 \times 768}$ 是情感分类权重 -
$p_{\text{emotion}} = \left\lbrack p_{\text{neutral}},p_{\text{happy}},p_{\text{sad}},p_{\text{angry}},p_{\text{surprise}},p_{\text{fear}} \right\rbrack$

**情感极性分数**：

$$\text{EmotionPolarity} = p_{\text{happy}} + p_{\text{surprise}} - p_{\text{sad}} - p_{\text{angry}} - p_{\text{fear}}$$

值域为 $\lbrack - 3,2\rbrack$，正值表示积极情感，负值表示消极情感。

### 3.2.4 文本预处理

#### （1）语音转文本（ASR）

采用Whisper-medium模型进行语音识别，该模型支持中英混合识别：

$$T = \text{Whisper}(A)$$

其中，$A$ 是音频波形，$T$ 是转写文本。

**转写质量评估**：在测试集上字错率（CER）为8.7%：

$$\text{CER} = \frac{S + D + I}{N} \times 100\%$$

其中，$S,D,I$ 分别是替换、删除、插入错误数，$N$ 是总字符数。

#### （2）文本清洗

对转写文本进行以下处理：

1.  **去除语气词**：移除"嗯"、"啊"、"那个"等填充词

2.  **句子分割**：按标点符号和停顿分割为句子

    3\. **错别字纠正**：使用拼音纠错模型（Pycorrector）

#### （3）对话行为识别

使用BERT模型将每个句子分类为4类对话行为：

$$p_{\text{act}} = \text{softmax}\left( \text{MLP}\left( \text{BERT}(T) \right) \right)$$

其中： - $\text{BERT}(T) \in \mathbb{R}^{768}$ 是句子的BERT嵌入 -
$\text{MLP}$ 是两层全连接网络 -
$p_{\text{act}} = \left\lbrack p_{Q},p_{I},p_{E},p_{F} \right\rbrack$
对应Question, Instruction, Explanation, Feedback

**对话行为分布统计**：

$$\text{ActDistribution} = \frac{1}{N_{s}}\sum_{i = 1}^{N_{s}}p_{\text{act}}^{(i)}$$

其中，$N_{s}$ 是句子数量。

