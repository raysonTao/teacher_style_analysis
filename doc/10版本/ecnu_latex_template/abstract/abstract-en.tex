\newpage
\vspace{-1cm}
\chapter*{\zihao{-3}\heiti{ABSTRACT}}
\addcontentsline{toc}{chapter}{Abstract}
\vspace{-0.5cm}

In the wave of educational digitalization, massive classroom video data urgently needs to be effectively utilized to empower teaching. Teacher teaching style is a key factor influencing classroom quality, but traditional evaluation methods are highly subjective and have delayed feedback, making it difficult to meet the demand for objective, real-time, and quantifiable classroom feedback in smart education environments. To this end, this study designs and implements a teacher teaching style profiling analysis system based on multimodal deep learning, aiming to provide objective, fine-grained, and interpretable intelligent style portrait analysis.

Existing classroom analysis techniques suffer from limitations such as the inability of single-modal video or audio to comprehensively characterize teaching styles, and the lack of decision basis and feature contribution analysis in style recognition. To address these challenges, this study proposes \textbf{SHAPE (Semantic Hierarchical Attention Profiling Engine)}, which achieves adaptive feature fusion and accurate style profiling through semantic-driven segmentation, hierarchical teaching intent recognition, and cross-modal attention mechanisms. Specifically: (1) A semantic-driven utterance segmentation strategy is proposed, which preserves the semantic integrity of teaching utterances through dependency parsing and discourse boundary detection, substantially improving teaching intent recognition accuracy; (2) The audio modality employs not only speech emotion recognition fine-tuned for classroom scenarios, but also Automatic Speech Recognition (ASR) to convert audio into text, laying the foundation for intent recognition; (3) The text modality introduces BERT-based Hierarchical Dialogue Act Recognition (H-DAR) with a two-level classification architecture (4 coarse classes + 10 fine classes), extending single-level classification to fine-grained dual-level recognition to more effectively capture the characteristic linguistic patterns of different teaching styles; (4) The visual modality uses identity recognition algorithms for stable teacher tracking and applies Spatial-Temporal Graph Convolutional Networks (ST-GCN) for temporal modeling of skeleton sequences, achieving substantially higher accuracy than single-frame action recognition; (5) SHAPE adaptively fuses visual, audio, and text features through a cross-modal attention mechanism, and combines attention weights with SHAP interpretability analysis to enhance the traceability of model decision rationale.

On a self-built teacher style dataset (209 samples, 7 style categories), SHAPE achieves an accuracy of \textbf{93.5\%} in style recognition, significantly outperforming single-modal methods and simple fusion methods. Ablation experiments further confirm that the semantic-driven segmentation strategy improves style recognition accuracy by \textbf{2.1 percentage points}, validating the effectiveness of these improvements.

A teacher classroom portrait system is also constructed to visualize the algorithm results. The system generates intuitive and traceable teacher style portraits---including style radar charts, modal contribution analysis, and representative clip playback---providing scientific, objective, and fine-grained data support for teacher style awareness and pedagogical research.

\vspace{0.5cm}
\hspace{-1cm}
{\bf{\sihao{Keywords:}}} \textit{Teacher Teaching Style; Multimodal Learning Analytics; Cross-modal Attention; Deep Learning}
