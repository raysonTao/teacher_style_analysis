\chapter{研究方法与总体设计}


\section{系统总体思路与研究框架}

本研究以"基于课堂录像的教师风格画像分析系统"为核心目标，构建一个集多模态特征提取、风格映射建模、画像生成与可视化反馈
于一体的分析体系。研究总体思路遵循"数据采集---特征建模---风格映射---结果反馈"的主线，旨在实现从课堂视频到教学风格画像的全流程量化分析与智能反馈。

\subsubsection{3.1.1 总体研究思路}

在教育信息化与人工智能技术的背景下，教师课堂行为与教学风格的客观识别与分析是推动教学质量评价科学化的重要方向。传统的教师评价多依赖主观观察和问卷调查，难以反映教学过程中的动态变化与多维特征。本研究借助\textbf{多模态学习分析（MMLA）}框架，综合运用计算机视觉、语音识别与自然语言处理等技术，对教师在课堂中的非言语行为与语言特征进行量化建模，从而构建教师风格画像，实现教学风格的客观、可解释识别。

系统总体思路遵循**"数据采集 → 特征提取 → 模态融合 → 风格映射 →
画像生成"**的技术路线，核心在于： 1.
\textbf{多模态协同}：视频、音频、文本三种模态互补增强 2.
\textbf{端到端建模}：从原始数据直接学习到风格标签的映射 3.
\textbf{可解释性}：通过注意力机制和SHAP分析提供决策依据

\subsubsection{3.1.2 四层系统架构}

系统由四个层次构成，如图3.1所示：

\textbf{【建议插入图3.1：系统四层架构图】}

（图应包含：数据层 → 特征提取层 → 融合分类层 →
应用层，每层标注关键技术）

\subsubsection{\textbf{第一层：数据采集与预处理层}}

通过录播系统采集课堂视频与音频数据，并利用以下技术完成数据清洗与时序同步：

\textbf{数据同步机制}：采用音频波形匹配算法（Cross-Correlation）实现视频与音频的精确对齐。设视频音轨为
$a_{v}(t)$，独立音频为 $a_{s}(t)$，时间偏移量 $\tau$
通过最大化互相关函数获得：

$$\tau^{\ast} = arg\max_{\tau}\int_{- \infty}^{\infty}a_{v}(t) \cdot a_{s}(t + \tau)\, dt$$

$$\text{或在离散时间域：}\quad\tau^{\ast} = arg\max_{\tau}\sum_{t}^{}a_{v}\lbrack t\rbrack \cdot a_{s}\lbrack t + \tau\rbrack$$

其中，$\tau^{\ast}$ 是最佳对齐偏移量，通常在±500ms范围内。

\textbf{数据分段策略：从基线到改进}

\textbf{（1）基线方法：固定时间窗口分段}

在初步实验中，我们采用固定时间窗口分段作为基线方法。将课堂视频按固定时间窗口 $T = 10s$ 分段，设完整课堂时长为 $L$，则生成 $N = \lfloor L/T \rfloor$ 个片段：

$$\mathcal{S}_{\text{baseline}} = \{S_1, S_2, ..., S_N\}$$

每个片段 $S_i$ 包含：
\begin{itemize}
    \item \textbf{视频帧序列}：$V_i = \{v_1, v_2, ..., v_{250}\}$（25fps × 10s = 250帧）
    \item \textbf{音频片段}：$A_i \in \mathbb{R}^{160000}$（16kHz × 10s = 160,000采样点）
    \item \textbf{转写文本}：$T_i$（经Whisper ASR生成）
\end{enumerate}

\textbf{基线方法的优势}：
\begin{itemize}
    \item 实现简单，易于工程化部署
    \item 计算开销固定，便于批量处理（45分钟课堂生成270个片段）
    \item 时序对齐容易（音视频按10秒固定对齐）
\end{enumerate}

\textbf{基线方法的局限}：

通过对209个样本的定性分析，我们发现固定分段在约\textbf{23.4%}的样本中出现了语义割裂现象。典型案例包括：
\begin{itemize}
    \item \textbf{逻辑推导被割裂}（占比35%）：完整的"因为...所以...因此"逻辑链被分割到不同片段
    \item \textbf{概念定义不完整}（占比28%）："所谓X，就是..."的定义句被截断
    \item \textbf{案例讲解跨段}（占比37%）：多句案例描述被人为分割
\end{enumerate}

定量分析显示，固定分段导致教学意图识别F1值下降约\textbf{5.2%}，风格识别准确率下降约\textbf{2.1%}（详见4.6节消融实验）。

\textbf{（2）改进方法：语义驱动的话语分段}

基于上述实验发现，我们提出\textbf{语义驱动的话语分段策略}，以保证每个分析单元是一个\textbf{语义完整的教学话语单元（Semantic Unit）}。具体流程如下：

① \textbf{ASR全文转写}：使用Whisper Large-v3模型对完整课堂音频进行转写，获得带时间戳的文本序列 $\mathcal{T} = \{(w_1, t_1), (w_2, t_2), ..., (w_M, t_M)\}$，其中 $w_i$ 是词语，$t_i$ 是时间戳；

② \textbf{句子边界检测}：结合标点符号（句号、问号、感叹号）与停顿时长（$\Delta t > 300$ms）识别句子边界，将文本序列切分为句子序列 $\mathcal{S} = \{s_1, s_2, ..., s_K\}$；

③ \textbf{依存句法分析}：使用预训练的中文句法分析模型（HanLP）识别句子间的逻辑连接关系，提取逻辑连接词（"因为""所以""但是"等）及其作用域；

④ \textbf{话语边界检测}：基于以下规则判断话语单元结束：
  - 逻辑链完整（如"因为...所以..."结构完成）
  - 出现话题转换标记（"那么""接下来""现在"）
  - 单元时长超过上限（$\Delta t > 30$s）

⑤ \textbf{形成语义单元}：将一个或多个连续句子合并为一个语义单元 $U_i$，设完整课堂时长为 $L$，则生成 $N$ 个语义单元（通常 $N \approx 150 \sim 200$ 个/45分钟课）：

$$\mathcal{U} = \{U_1, U_2, ..., U_N\}$$

每个语义单元 $U_i$ 包含：
\begin{itemize}
    \item \textbf{文本内容}：$T_i = \{s_j, s_{j+1}, ..., s_k\}$（一个或多个句子）
    \item \textbf{音频片段}：$A_i \in \mathbb{R}^{N_s}$（$N_s$ 为采样点数，通常 $5s \leq \Delta t_i \leq 30s$）
    \item \textbf{视频帧序列}：$V_i = \{v_1, v_2, ..., v_{T_i}\}$（帧数 $T_i = \text{fps} \times \Delta t_i$，通常125-750帧）
    \item \textbf{时间范围}：$(t_{\text{start}}^i, t_{\text{end}}^i)$
\end{enumerate}

\textbf{改进方法的优势}：

相比固定时间窗口，语义驱动分段具有以下优势：
\begin{itemize}
    \item \textbf{语义完整性提升}：从76.6%提升至\textbf{95.3%}（提升18.7个百分点）
    \item \textbf{适应教学节奏}：单元时长灵活（5-30秒），自动适应不同教学风格
    \item \textbf{后续任务性能提升}：教学意图识别F1值提升\textbf{5.2%}，风格识别准确率提升\textbf{2.1%}
    \item \textbf{单元数量更合理}：平均175个单元/课（vs 固定270个），减少35%，降低冗余
\end{enumerate}

例如，一个完整的逻辑推导单元（"因为速度等于位移除以时间，所以我们可以得到v=s/t，因此当时间固定时，速度与位移成正比"）会被完整保留，而不会被人为切断。这使得后续的教学意图识别模型能够捕捉完整的逻辑链，识别准确率显著提升（见4.6节消融实验）。

\subsubsection{\textbf{后续系统架构概述}}

基于语义单元分段后，系统采用\textbf{四层架构}设计（见图3.1）：

\textbf{第二层：特征提取层}
三模态并行处理（Pipeline并行，总耗时0.82s/片段）：
\begin{itemize}
    \item 视觉：YOLOv8 → DeepSORT → MediaPipe → ST-GCN → 20维特征
    \item 音频：Wav2Vec 2.0 → 情感分类 → 15维特征
    \item 文本：BERT → H-DAR（10类细粒度意图） → 35维特征
\end{enumerate}

\textbf{第三层：融合分类层}
SHAPE跨模态注意力融合模型（详见3.3节）进行7类风格分类：理论讲授型、耐心细致型、启发引导型、题目驱动型、互动导向型、逻辑推导型、情感表达型。

\textbf{第四层：应用服务层}
画像生成、可视化图表、SHAP可解释性分析（详见3.4节）。

\textbf{关键设计}：
\begin{enumerate}
    \item 异步任务队列（Celery + RabbitMQ）支持批量处理
    \item 三级缓存策略（Redis特征缓存 + MySQL元数据 + MinIO视频存储）降低重复计算开销
    \item 水平扩展支持，特征提取与模型推理服务可独立扩容
\end{enumerate}

\section{多模态数据采集与预处理方法}

\subsubsection{3.2.1 数据采集流程}

\textbf{硬件要求：} - 视频：1280×720分辨率，25fps，H.264编码 -
音频：16kHz采样率，单声道，PCM编码 -
存储：每节课（40分钟）约占用500MB空间

\textbf{采集策略：}

\begin{enumerate}
    \item 固定机位拍摄，确保教师活动区域完整入画

    \item 使用定向麦克风采集教师语音，降低学生噪声干扰

    \item 同步记录时间戳，精度达到毫秒级
\end{enumerate}

\subsubsection{3.2.2 视频预处理}

\subsection{（1）视频解码与抽帧}

使用FFmpeg库解码视频流，按25fps提取RGB帧：

$$V = \{ v_{1},v_{2},...,v_{T}\},\quad v_{i} \in \mathbb{R}^{720 \times 1280 \times 3}$$

其中，$v_{i}$ 表示第 $i$ 帧的RGB像素矩阵。

\subsubsection{（2）视频增强}

为提升模型鲁棒性，对训练数据应用以下增强策略： -
\textbf{随机裁剪}：以0.8-1.0的缩放比例裁剪 -
\textbf{颜色抖动}：亮度、对比度、饱和度随机扰动（±20%） -
\textbf{时间抖动}：随机丢帧以模拟帧率不稳定

$$v_{i}\prime = \text{ColorJitter}\left( \text{RandomCrop}\left( v_{i},\text{scale} = 0.8 \right) \right)$$

\subsubsection{（3）教师检测、追踪��姿态估计}

视频处理采用YOLOv8\cite{ref16}进行人体检测，DeepSORT\cite{ref30}算法进行教师身份追踪（ID稳定性提升25.5%），MediaPipe Pose提取33个骨骼关键点。DeepSORT通过结合外观特征（ReID）和运动模型（卡尔曼滤波）实现稳定追踪，基本消除了身份漂移问题（详见4.3.1节消融实验）。

姿态估计后保留置信度>0.5的关键点，缺失点通过线性插值补全。最终输出骨骼序列$P \in \mathbb{R}^{T \times 33 \times 4}$（T帧，33关节点，每点包含x/y/z坐标和置信度），用于后续ST-GCN时序建模。

\subsubsection{3.2.3 音频预处理}

\subsubsection{（1）音频重采样与降噪}

将原始音频统一重采样到16kHz单声道，并应用谱减法（Spectral
Subtraction）降噪：

$$S_{\text{clean}}(f) = max\left( \left| S_{\text{noisy}}(f) \right| - \alpha \cdot \left| N(f) \right|,\beta \cdot \left| S_{\text{noisy}}(f) \right| \right)$$

其中： - $S_{\text{noisy}}(f)$ 是带噪语音的频谱 - $N(f)$
是噪声频谱估计（从静音段提取） - $\alpha = 2.0$ 是过减因子 -
$\beta = 0.01$ 是谱下限

\subsubsection{（2）语音活动检测（VAD）}

采用基于能量的VAD算法检测有效语音段。计算短时能量：

$$E(n) = \sum_{m = n - N + 1}^{n}\left| x(m) \right|^{2}$$

其中，$N$ 是窗口长度（通常取400个采样点，对应25ms）。

当 $E(n) > \theta_{\text{energy}}$ 时判定为语音帧，其中阈值
$\theta_{\text{energy}}$ 设为静音段能量均值的3倍：

$$\theta_{\text{energy}} = 3 \times \text{mean}\left( E_{\text{silence}} \right)$$

\textbf{统计特征提取}： -
\textbf{语音活动比}：$\text{VAR} = \frac{N_{\text{voice}}}{N_{\text{total}}}$ -
\textbf{静音比}：$\text{SR} = 1 - \text{VAR}$ -
\textbf{平均语速}：$\text{Speed} = \frac{N_{\text{words}}}{T_{\text{total}}}$（字/秒）

\subsubsection{（3）情感特征提取}

使用Wav2Vec
2.0模型提取768维深度声学嵌入，然后通过情感分类头输出6维情感分布：

$$p_{\text{emotion}} = \text{softmax}\left( W_{e}h_{\text{wav2vec}} + b_{e} \right)$$

其中： - $h_{\text{wav2vec}} \in \mathbb{R}^{768}$ 是Wav2Vec 2.0的输出 -
$W_{e} \in \mathbb{R}^{6 \times 768}$ 是情感分类权重 -
$p_{\text{emotion}} = \left\lbrack p_{\text{neutral}},p_{\text{happy}},p_{\text{sad}},p_{\text{angry}},p_{\text{surprise}},p_{\text{fear}} \right\rbrack$

\textbf{情感极性分数}：

$$\text{EmotionPolarity} = p_{\text{happy}} + p_{\text{surprise}} - p_{\text{sad}} - p_{\text{angry}} - p_{\text{fear}}$$

值域为 $\lbrack - 3,2\rbrack$，正值表示积极情感，负值表示消极情感。

\subsubsection{3.2.4 文本预处理}

\subsubsection{（1）语音转文本（ASR）}

采用Whisper-medium模型进行语音识别，该模型支持中英混合识别：

$$T = \text{Whisper}(A)$$

其中，$A$ 是音频波形，$T$ 是转写文本。

\textbf{转写质量评估}：在测试集上字错率（CER）为8.7%：

$$\text{CER} = \frac{S + D + I}{N} \times 100\%$$

其中，$S,D,I$ 分别是替换、删除、插入错误数，$N$ 是总字符数。

\subsubsection{（2）文本清洗}

对转写文本进行以下处理：

\begin{enumerate}
    \item \textbf{去除语气词}：移除"嗯"、"啊"、"那个"等填充词

    \item \textbf{句子分割}：按标点符号和停顿分割为句子
\end{enumerate}

    3\. \textbf{错别字纠正}：使用拼音纠错模型（Pycorrector）

\subsubsection{（3）对话行为识别}

使用BERT模型将每个句子分类为4类对话行为：

$$p_{\text{act}} = \text{softmax}\left( \text{MLP}\left( \text{BERT}(T) \right) \right)$$

其中： - $\text{BERT}(T) \in \mathbb{R}^{768}$ 是句子的BERT嵌入 -
$\text{MLP}$ 是两层全连接网络 -
$p_{\text{act}} = \left\lbrack p_{Q},p_{I},p_{E},p_{F} \right\rbrack$
对应Question, Instruction, Explanation, Feedback

\textbf{对话行为分布统计}：

$$\text{ActDistribution} = \frac{1}{N_{s}}\sum_{i = 1}^{N_{s}}p_{\text{act}}^{(i)}$$

其中，$N_{s}$ 是句子数量。

\section{SHAPE：教师风格画像引擎设计}

这是本研究的核心创新，我们设计了\textbf{SHAPE (Semantic Hierarchical Attention Profiling Engine，语义层次化注意力画像引擎)}来实现特征的自适应融合与风格画像。SHAPE通过语义驱动分段、层次化教学意图识别和跨模态注意力机制，构建了从课堂录像到教师风格画像的完整流程。

\subsubsection{3.3.1 设计动机}

传统的多模态融合方法主要有三类：

\textbf{(1) 早期融合（Early Fusion）}：直接拼接原始特征

$$F_{\text{concat}} = \left\lbrack F_{v};F_{a};F_{t} \right\rbrack \in \mathbb{R}^{20 + 15 + 35} = \mathbb{R}^{70}$$

\textbf{局限性}： - 不同模态的维度和尺度差异大，高维模态会主导融合结果 -
无法建模模态间的交互关系 - 缺乏对不同模态重要性的自适应调整

\textbf{(2) 晚期融合（Late Fusion）}：分别训练单模态分类器，结果加权平均

$$P_{\text{final}} = w_{v}P_{v} + w_{a}P_{a} + w_{t}P_{t}$$

\textbf{局限性}： - 权重 $w_{v},w_{a},w_{t}$
固定，无法根据样本内容自适应调整 - 忽略了模态间的互补信息

\textbf{(3) 中间融合（Middle Fusion）}：在特征层进行加权融合

$$F_{\text{weighted}} = w_{v}F_{v} + w_{a}F_{a} + w_{t}F_{t}$$

\textbf{局限性}： - 仍然是固定权重 - 不同模态的特征空间不一致，直接相加不合理

采用\textbf{跨模态注意力机制}：

1\. 不同模态在不同样本上的重要性（样本自适应）

2\. 模态之间的交互关系（跨模态增强）

3\. 决策依据的可解释性（注意力权重可视化）

### 

\subsubsection{3.3.2 SHAPE网络架构}

SHAPE由五个核心模块组成：

\textbf{【建议插入图3.2：SHAPE详细架构图】}

（图应包含：特征投影 → 跨模态注意力 → 时序建模 → 特征融合 → 分类器）

\subsubsection{\textbf{模块1：特征投影层（Feature Projection Layer）}}

由于三个模态的原始特征维度不同（$F_{v} \in \mathbb{R}^{20},F_{a} \in \mathbb{R}^{15},F_{t} \in \mathbb{R}^{35}$），首先通过全连接层投影到统一维度
$d = 512$：

$$F_{v}\prime = \text{ReLU}\left( W_{v}F_{v} + b_{v} \right),\quad F_{v}\prime \in \mathbb{R}^{512}$$

$$F_{a}\prime = \text{ReLU}\left( W_{a}F_{a} + b_{a} \right),\quad F_{a}\prime \in \mathbb{R}^{512}$$

$$F_{t}\prime = \text{ReLU}\left( W_{t}F_{t} + b_{t} \right),\quad F_{t}\prime \in \mathbb{R}^{512}$$

其中，$W_{v} \in \mathbb{R}^{512 \times 20},W_{a} \in \mathbb{R}^{512 \times 15},W_{t} \in \mathbb{R}^{512 \times 35}$
是可学习的投影��阵。

\textbf{设计考量}： - ReLU激活函数引入非线性，提升特征表达能力 -
统一维度便于后续的注意力计算

#### 

\subsubsection{\textbf{模块2：跨模态注意力层（Cross-Modal Attention Layer）}}

这是SHAPE的核心创新。对于每对模态 $(i,j)$，计算从模态 $i$ 到模态 $j$
的注意力：

\textbf{步骤1：计算Query, Key, Value}

$$Q_{i} = F_{i}\prime W_{Q}^{i},\quad K_{j} = F_{j}\prime W_{K}^{j},\quad V_{j} = F_{j}\prime W_{V}^{j}$$

其中，$W_{Q}^{i},W_{K}^{j},W_{V}^{j} \in \mathbb{R}^{512 \times 64}$
是可学习参数，注意力维度 $d_{k} = 64$。

\textbf{步骤2：计算注意力权重}

$$\alpha_{i \rightarrow j} = \text{softmax}\left( \frac{Q_{i}K_{j}^{T}}{\sqrt{d_{k}}} \right)$$

这里，$\alpha_{i \rightarrow j}$ 是一个标量（因为 $Q_{i},K_{j}$
都是向量），表示模态 $j$ 对模态 $i$ 的重要性。

\textbf{步骤3：加权融合}

$${\widetilde{F}}_{i}^{(j)} = \alpha_{i \rightarrow j}V_{j}$$

${\widetilde{F}}_{i}^{(j)}$ 表示从模态 $j$ 中提取的、与模态 $i$
相关的信息。

\textbf{全局跨模态交互}：

每个模态需要与其他两个模态进行交互：

$${\widetilde{F}}_{v} = F_{v}\prime + {\widetilde{F}}_{v}^{(a)} + {\widetilde{F}}_{v}^{(t)}$$

$${\widetilde{F}}_{a} = F_{a}\prime + {\widetilde{F}}_{a}^{(v)} + {\widetilde{F}}_{a}^{(t)}$$

$${\widetilde{F}}_{t} = F_{t}\prime + {\widetilde{F}}_{t}^{(v)} + {\widetilde{F}}_{t}^{(a)}$$

这里使用了\textbf{残差连接}（Residual Connection），保留原始特征信息。

\textbf{设计考量}： - 缩放因子 $\sqrt{d_{k}}$
防止内积过大导致softmax梯度消失 - 残差连接缓解深层网络的梯度消失问题 -
即使跨模态信息不相关，原始特征也不会被破坏

\textbf{跨模态注意力的有效性}：
\begin{itemize}
    \item 跨模态注意力使模型能自适应学习模态重要性，例如"情感表达型"教师模型会自动增大音频权重（$\alpha_{a \to v} = 0.62$）
    \item 残差连接保留原始特征，即使跨模态信息不相关，原始特征也不会被破坏
    \item 相比简单拼接（Early Fusion），跨模态注意力使准确率提升\textbf{8.3个百分点}（见4.4节对比实验）
\end{enumerate}

####

\subsubsection{\textbf{模块3：时序建模层（Temporal Modeling Layer）}}

课堂是一个时序过程，教师风格在时间维度上展现。我们使用\textbf{双向LSTM（BiLSTM）}建模时序依赖：

对于一个完整课堂的 $N$ 个片段
$\{ S_{1},S_{2},...,S_{N}\}$，每个片段的特征为
$\{{\widetilde{F}}_{1},{\widetilde{F}}_{2},...,{\widetilde{F}}_{N}\}$（这里省略模态下标，表示融合后的特征）。

\textbf{前向LSTM}：

$${\overrightarrow{h}}_{n} = \text{LSTM}_{\text{forward}}\left( {\widetilde{F}}_{n},{\overrightarrow{h}}_{n - 1} \right)$$

\textbf{后向LSTM}：

$${\overleftarrow{h}}_{n} = \text{LSTM}_{\text{backward}}\left( {\widetilde{F}}_{n},{\overleftarrow{h}}_{n + 1} \right)$$

\textbf{双向拼接}：

$$h_{n} = \left\lbrack {\overrightarrow{h}}_{n};{\overleftarrow{h}}_{n} \right\rbrack \in \mathbb{R}^{1024}$$

（每个方向的隐状态维度为512）

\textbf{设计考量}： - BiLSTM能够捕捉片段之间的前后依赖关系 -
例如，教师在讲授后通常会进行提问互动，这种模式可以被LSTM学习

#### 

\subsubsection{\textbf{模块4：注意力池化层（Attention Pooling Layer）}}

将所有片段的特征聚合为一个固定长度的向量：

$$\beta_{n} = \frac{\exp\left( v^{T}\tanh\left( W_{p}h_{n} \right) \right)}{\sum_{m = 1}^{N}\exp\left( v^{T}\tanh\left( W_{p}h_{m} \right) \right)}$$

$$F_{\text{pooled}} = \sum_{n = 1}^{N}\beta_{n}h_{n}$$

其中： - $W_{p} \in \mathbb{R}^{256 \times 1024}$ 是注意力权重矩阵 -
$v \in \mathbb{R}^{256}$ 是注意力向量 - $\beta_{n}$ 是第 $n$
个片段的重要性权重

\textbf{设计考量}： -
不同片段对风格识别的贡献不同（例如，提问片段对"启发引导型"更重要） -
注意力池化能够自适应地关注关键片段

#### 

\subsubsection{\textbf{模块5：风格分类器（Style Classifier）}}

最终通过两层全连接网络进行分类：

$$h_{1} = \text{ReLU}\left( W_{1}F_{\text{pooled}} + b_{1} \right),\quad h_{1} \in \mathbb{R}^{256}$$

$$h_{2} = \text{Dropout}\left( h_{1},p = 0.3 \right)$$

$$z = W_{2}h_{2} + b_{2},\quad z \in \mathbb{R}^{7}$$

$$P\left( y|X \right) = \text{softmax}(z)$$

其中，$z$ 是logits，$P\left( y|X \right)$ 是7类教学风格的概率分布。

\textbf{设计考量}： - Dropout（$p = 0.3$）防止过拟合 -
两层网络（而不是单层）增强非线性拟合能力

### 

### 

\subsubsection{3.3.3 损失函数与优化}

\subsubsection{\textbf{损失函数}}

采用\textbf{交叉熵损失}加\textbf{标签平滑}：

$$\mathcal{L}_{\text{CE}} = - \frac{1}{N}\sum_{i = 1}^{N}{\sum_{k = 1}^{7}y_{i,k}}\prime log\left( {\widehat{y}}_{i,k} \right)$$

其中，标签平滑后的标签为：

$$y_{i,k}\prime = (1 - \epsilon)y_{i,k} + \frac{\epsilon}{7}$$

本研究中，平滑参数 $\epsilon = 0.1$。

\textbf{设计考量}： - 标签平滑防止模型对某个类别过于自信 - 提高模型的泛化能力

#### 

\subsubsection{\textbf{优化算法}}

使用\textbf{Adam优化器}：

$$m_{t} = \beta_{1}m_{t - 1} + \left( 1 - \beta_{1} \right)g_{t}$$

$$v_{t} = \beta_{2}v_{t - 1} + \left( 1 - \beta_{2} \right)g_{t}^{2}$$

$${\widehat{m}}_{t} = \frac{m_{t}}{1 - \beta_{1}^{t}},\quad{\widehat{v}}_{t} = \frac{v_{t}}{1 - \beta_{2}^{t}}$$

$$\theta_{t} = \theta_{t - 1} - \eta\frac{{\widehat{m}}_{t}}{\sqrt{{\widehat{v}}_{t}} + \epsilon}$$

其中，$\beta_{1} = 0.9,\beta_{2} = 0.999,\epsilon = 10^{- 8}$。

#### 

\subsubsection{\textbf{学习率调度}}

采用\textbf{余弦退火}策略：

$$\eta_{t} = \eta_{\min} + \frac{1}{2}\left( \eta_{\max} - \eta_{\min} \right)\left( 1 + cos\left( \frac{t}{T_{\max}}\pi \right) \right)$$

其中，$\eta_{\max} = 10^{- 4}$，$\eta_{\min} = 10^{- 6}$，$T_{\max} = 100$。

## 

\section{教师风格画像与反馈机制设计}

教师风格画像（Teacher Style
Profiling）是将多模态特征分析与风格识别结果进行结构化呈现的过程，其目的在于以可视化、可解释、可反馈的方式展示教师的课堂行为特征与教学风格特征。

本节在前述风格映射模型的基础上，提出了一个集
数据可视化---风格建模---可解释分析
于一体的教师风格画像系统设计方案，旨在实现教师风格的量化描述与特征可视化输出。

\subsection{风格画像生成}

对于一节完整的课堂，系统输出：

\subsubsection{(1) 风格分类结果}

$$\text{PrimaryStyle} = arg\max_{k}P\left( y = k|X \right)$$

例如："该教师的主导风格为\textbf{启发引导型}（置信度89.3%）"

\subsubsection{(2) 风格雷达图}

将7类风格的概率分布可视化为雷达图：

$$\text{RadarPlot}\left( P(y = 1),P(y = 2),...,P(y = 7) \right)$$

\textbf{设计考量}：大多数教师不是单一风格，雷达图能展示混合风格特征。

\subsubsection{(3) 模态贡献度分析}

通过跨模态注意力权重
$\alpha_{i \rightarrow j}$，计算每个模态的总贡献度：

$$\text{ModalityContribution}_{i} = \frac{\sum_{j \neq i}^{}\alpha_{i \rightarrow j}}{\sum_{i,j}^{}\alpha_{i \rightarrow j}}$$

例如："该课堂中，\textbf{视觉模态}贡献45%，\textbf{音频模态}贡献32%，\textbf{文本模态}贡献23%"

\subsubsection{(4) 典型片段回放}

选择注意力池化权重 $\beta_{n}$ 最高的前3个片段，作为该风格的典型代表：

$$\text{TopSegments} = \text{TopK}\left( \{\beta_{1},\beta_{2},...,\beta_{N}\},K = 3 \right)$$

用户可以点击查看这些片段，直观理解系统的判断依据。

\subsubsection{3.4.2 可解释性分析}

\subsubsection{(1) SHAP值分析}

使用SHAP（SHapley Additive
exPlanations）分析每个特征对预测结果的边际贡献：

$$\phi_{i} = \sum_{S \subseteq F\backslash\{ i\}}^{}\frac{|S|!\left( |F| - |S| - 1 \right)!}{|F|!}\left\lbrack f_{S \cup \{ i\}}(x) - f_{S}(x) \right\rbrack$$

其中： - $\phi_{i}$ 是特征 $i$ 的SHAP值 - $S$ 是特征子集 - $f_{S}(x)$
是仅使用特征子集 $S$ 时的模型预测

\textbf{可视化}：生成特征贡献度条形图，例如： - "提问频率" →
+0.25（正向贡献） - "静音比" → -0.12（负向贡献）

\subsubsection{(2) 注意力热图}

将跨模态注意力权重矩阵
$\left\lbrack \alpha_{i \rightarrow j} \right\rbrack$ 可视化为3×3热图：

$$\begin{bmatrix}
 - & \alpha_{v \rightarrow a} & \alpha_{v \rightarrow t} \\
\alpha_{a \rightarrow v} & - & \alpha_{a \rightarrow t} \\
\alpha_{t \rightarrow v} & \alpha_{t \rightarrow a} & - 
\end{bmatrix}$$

\textbf{解释示例}： - 如果
$\alpha_{v \rightarrow a} = 0.78$，说明"视觉模态高度依赖音频信息" -
这在"情感表达型"教师中很常见（肢体语言与语调同步）

\subsubsection{(3) 模态重要性分析}

通过跨模态注意力权重$\alpha_{i \to j}$，我们可以计算每种教学风格对各模态的依赖程度：

$$\text{ModalityWeight}_{k,m} = \frac{1}{N_k} \sum_{i \in \mathcal{C}_k} \alpha_{i \to m}$$

其中$\mathcal{C}_k$是风格类别$k$的所有样本，$N_k$是样本数，$m \in \{v, a, t\}$是模态。

\textbf{表3-X：七类教学风格的模态依赖模式（注意力权重分析）}

  ---------------------------------------------------------------------------------
  风格类别        视觉权重    音频权重    文本权重    主导模态    特征解释
  --------------- ----------- ----------- ----------- ----------- -----------------
  理论讲授型      0.25        0.32        \textbf{0.43}    文本        高频使用"概念定义"
                                                                  和"理论讲授"话语

  耐心细致型      0.28        \textbf{0.45}    0.27        音频        语速慢、停顿多、
                                                                  重复强调

  启发引导型      0.35        0.32        \textbf{0.33}    均衡        视觉互动+音频情感
                                                                  +文本提问三者协同

  题目驱动型      \textbf{0.42}    0.28        0.30        视觉        板书频繁、指向
                                                                  黑板动作多

  互动导向型      \textbf{0.50}    0.28        0.22        视觉        走动频繁、手势丰富、
                                                                  空间覆盖广

  逻辑推导型      0.22        0.25        \textbf{0.53}    文本        高频使用"因为...
                                                                  所以...因此"逻辑链

  情感表达型      0.26        \textbf{0.62}    0.12        音频        语调丰富、情感
                                                                  极性分数高
  ---------------------------------------------------------------------------------

\textbf{关键发现}：
\begin{enumerate}
    \item \textbf{模态依赖的风格差异显著}（方差分析F=42.3, p<0.001）
    \item \textbf{音频主导型}：情感表达型(0.62)、耐心细致型(0.45)
    \item \textbf{视觉主导型}：互动导向型(0.50)、题目驱动型(0.42)
    \item \textbf{文本主导型}：逻辑推导型(0.53)、理论讲授型(0.43)
    \item \textbf{均衡型}：启发引导型三模态权重相近（标准差0.015）
\end{enumerate}

这些模态依赖模式揭示了不同教学风格的行为特征。例如，互动导向型教师的视觉模态权重达到0.50，主要体现为高频走动和丰富手势；而逻辑推导型教师的文本模态权重达到0.53，主要体现为密集的逻辑连接词使用。详细的实验结果见第5章5.2.3节。

\section{本章小结}

本章详细阐述了基于课堂录像的教师风格画像分析系统的总体设计思路与技术框架，主要工作包括：

\begin{enumerate}
    \item \textbf{系统架构设计}：构建了包含数据采集、特征提取、模态融合、风格映射四层的系统架构，明确了各层的功能与技术路线。

    \item \textbf{多模态数据预处理}：设计了视频、音频、文本三个模态的预处理流程，包括数据同步（互相关算法）、教师追踪（DeepSORT）、语音转写（Whisper）、对话行为识别（BERT）等关键技术，并通过数学建模明确了每个步骤的输入输出。

    \item \textbf{SHAPE网络设计}：提出了多模态注意力网络（SHAPE）这一核心创新，通过跨模态注意力机制实现特征的自适应融合。详细阐述了五个子模块的数学建模：特征投影、跨模态注意力、时序建模、注意力池化、风格分类器。相比传统拼接或加权方法，SHAPE能够：
\end{enumerate}

    -   \textbf{样本自适应}地调整模态权重
    -   \textbf{跨模态增强}建模模态交互
    -   \textbf{时序建模}捕捉片段依赖
    -   \textbf{可解释性}提供注意力权重可视化

\begin{enumerate}
    \item \textbf{风格画像与反馈机制}：设计了包含风格雷达图、模态贡献度分析、典型片段回放、SHAP值分析、个性化反馈在内的完整画像生成与解释系统。
\end{enumerate}

\textbf{与现有工作的对比}： -
相比\textbf{简单拼接}，SHAPE通过注意力机制提升3.8个百分点 -
相比\textbf{固定权重融合}，SHAPE的权重是样本自适应的 -
相比\textbf{单模态方法}，SHAPE利用了模态间的互补信息

\textbf{局限性与未来工作}： -
当前模型假设所有模态都可用，未来可研究缺失模态的鲁棒融合 -
时序建模仅使用BiLSTM，未来可探索Transformer的长程依赖能力

本章设计的方法框架为第四章的实验验证提供了理论基础，为第五章的系统实现提供了技术蓝图。下一章将通过详细的对比实验和消融实验，验证每个技术模块的有效性，并评估系统的整体性能。

\textbf{本章插图清单：} - 图3.1：系统四层架构图（数据层 → 特征提取层 →
融合分类层 → 应用层） - 图3.2：SHAPE详细架构图（特征投影 → 跨模态注意力 →
BiLSTM → 注意力池化 → 分类器） -
图3.3：跨模态注意力机制示意图（三个模态之间的双向注意力连接） -
图3.4：DeepSORT追踪流程图（检测 → ReID特征提取 → 卡尔曼预测 →
匈牙利匹配）

\textbf{本章公式清单：} - 公式3.1：音频视频时间同步（互相关函数） -
公式3.2：教师选择策略（位置+大小加权） - 公式3.3：DeepSORT卡尔曼滤波 -
公式3.4-3.5：谱减法降噪 - 公式3.6-3.7：语音活动检测（短时能量） -
公式3.8：情感极性分数 - 公式3.9-3.12：SHAPE特征投影 -
公式3.13-3.18：跨模态注意力计算 - 公式3.19-3.21：BiLSTM时序建模 -
公式3.22-3.23：注意力池化 - 公式3.24-3.26：分类器 -
公式3.27-3.28：损失函数（交叉熵+标签平滑） - 公式3.29-3.32：Adam优化器 -
公式3.33：余弦退火学习率
