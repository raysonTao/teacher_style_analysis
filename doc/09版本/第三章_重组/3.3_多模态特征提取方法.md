## 3.3 多模态特征提取方法

本节详细阐述视频、音频、文本三个模态的特征提取技术路线。三个模态在Pipeline中并行处理，总耗时0.82s/片段，为后续的融合分类提供高质量的特征表示。

### 3.3.1 音频模态特征提取

音频模态是教师课堂风格分析中最核心的维度之一。语音不仅承载了教学内容的信息，还反映了教师的表达方式、情绪状态与课堂节奏。音频模态承载"韵律节奏—情感表达—教学意图"三层语义信息。本研究提出**Wav2Vec 2.0自监督表征 + 情感分类**的端到端音频分析链路。

#### （1）深度学习自监督声学表征

本研究采用Wav2Vec 2.0进行音频特征提取。Wav2Vec 2.0通过自监督对比学习从无标注音频中学习通用表征，在课堂噪声环境下相比传统MFCC特征准确率提升6.4个百分点（SNR=10dB时提升11.3个百分点）。

对于音频片段 $\mathbf{x} \in \mathbb{R}^{N_s}$（$N_s$ 为采样点数，通常对应5-30秒），特征提取流程为：

**步骤1：Wav2Vec 2.0编码**

$$\mathbf{h}_{\text{wav2vec}} = \text{Wav2Vec2}(\mathbf{x}), \quad \mathbf{h}_{\text{wav2vec}} \in \mathbb{R}^{T \times 768}$$

其中，$T$ 是时间帧数（采样率16kHz下，$T \approx N_s / 320$）。

**步骤2：时序平均池化**

$$\mathbf{h}_{\text{audio}} = \frac{1}{T}\sum_{t=1}^{T} \mathbf{h}_{\text{wav2vec}}[t] \in \mathbb{R}^{768}$$

**步骤3：情感分类**

使用情感分类头输出6维情感分布：

$$\mathbf{p}_{\text{emotion}} = \text{softmax}(W_e \mathbf{h}_{\text{audio}} + b_e) \in \mathbb{R}^{6}$$

其中：
- $W_e \in \mathbb{R}^{6 \times 768}$ 是情感分类权重
- $\mathbf{p}_{\text{emotion}} = \left\lbrack p_{\text{neutral}},p_{\text{happy}},p_{\text{sad}},p_{\text{angry}},p_{\text{surprise}},p_{\text{fear}} \right\rbrack$

**情感极性分数**：

$$\text{EmotionPolarity} = p_{\text{happy}} + p_{\text{surprise}} - p_{\text{sad}} - p_{\text{angry}} - p_{\text{fear}}$$

值域为 $\lbrack - 3,2\rbrack$，正值表示积极情感，负值表示消极情感。

#### （2）音频特征编码汇总

最终，音频模态生成**15维编码向量** $F_{a} \in \mathbb{R}^{15}$：

$$F_{a} = \left\lbrack \underset{\text{6维情感}}{\underbrace{p_{\text{neutral}},...,p_{\text{fear}}}},\underset{\text{语速}}{\underbrace{v_{\text{speed}}}},\underset{\text{活动比}}{\underbrace{\text{VAR},\text{SR}}},\underset{\text{韵律}}{\underbrace{\mu_{\text{vol}},\sigma_{\text{pitch}}}},\underset{\text{极性}}{\underbrace{e_{\text{polar}}}},\underset{\text{压缩嵌入}}{\underbrace{z_{1},z_{2},z_{3}}} \right\rbrack$$

其中：
- 前6维：Wav2Vec 2.0情感分布
- 第7维：语速 $v_{\text{speed}} = N_{\text{words}}/T$（归一化到[0,1]）
- 第8-9维：语音活动比（VAR）、静音比（SR）
- 第10-11维：音量均值、音高变化系数
- 第12维：情感极性分数
- 第13-15维：Wav2Vec 2.0嵌入的分段均值（768维→3维，通过PCA降维）

### 3.3.2 视频模态特征提取

视频模态捕捉教师的非言语行为（肢体动作、空间移动、板书互动等）。本研究提出**DeepSORT稳定追踪 + ST-GCN时序建模**的视频分析链路。

#### （1）DeepSORT稳定追踪算法

课堂场景存在多人干扰（学生走动、举手），单纯依赖YOLO检测会导致教师ID在遮挡后跳变为学生ID。本研究采用DeepSORT算法，通过结合外观特征（ReID）和运动模型（卡尔曼滤波）实现稳定追踪。

**核心思想**：
1. **外观特征（ReID）**：提取检测框的深度特征，用于区分不同人物
2. **运动模型（卡尔曼滤波）**：预测目标下一帧位置，平滑轨迹
3. **匈牙利匹配**：基于外观相似度和位置距离进行目标关联

**卡尔曼滤波状态更新**：

状态向量 $\mathbf{x}_t = [c_x, c_y, w, h, \dot{c}_x, \dot{c}_y, \dot{w}, \dot{h}]^T$（中心坐标、宽高及其速度）：

$$\mathbf{x}_{t|t-1} = F \mathbf{x}_{t-1|t-1}$$

$$\mathbf{x}_{t|t} = \mathbf{x}_{t|t-1} + K_t (\mathbf{z}_t - H \mathbf{x}_{t|t-1})$$

其中，$F$ 是状态转移矩阵，$H$ 是观测矩阵，$K_t$ 是卡尔曼增益，$\mathbf{z}_t$ 是检测观测。

**追踪效果**：

DeepSORT使教师ID稳定性从68.3%提升至93.8%（提升25.5个百分点），平均ID切换次数从8.7次/视频降至0.8次/视频（降低90.8%），基本消除了身份漂移问题。

#### （2）ST-GCN时序动作识别

本研究采用ST-GCN（Spatial-Temporal Graph Convolutional Network）进行骨骼序列时序建模。ST-GCN将骨骼序列建模为时空图结构，通过图卷积捕捉关节间的依赖关系。

**输入表示**：

骨骼序列 $X \in \mathbb{R}^{C \times T \times V}$：
- $C=3$：坐标维度（x, y, z）
- $T=32$：时间帧数
- $V=25$：关节点数（从MediaPipe的33个关键点中选择25个身体主要关节）

**网络结构**：

$$\begin{aligned}
X_1 &= \text{ST-GCN-Block}(X_0, C_{\text{out}}=64) \\
X_2 &= \text{ST-GCN-Block}(X_1, C_{\text{out}}=128) \\
X_3 &= \text{ST-GCN-Block}(X_2, C_{\text{out}}=256) \\
\mathbf{h}_{\text{video}} &= \text{GAP}(X_3) \in \mathbb{R}^{256}
\end{aligned}$$

其中，GAP表示全局平均池化（Global Average Pooling）。

**ST-GCN-Block结构**：

每个ST-GCN-Block包含：
1. **空间图卷积**：捕捉关节间的空间依赖（如肩-肘-腕链）
2. **时序卷积**：捕捉时序依赖（如挥手动作的连续帧）
3. **残差连接**：缓解梯度消失

**图卷积公式**：

$$f_{\text{out}} = \Lambda^{-\frac{1}{2}}(A + I)\Lambda^{-\frac{1}{2}} f_{\text{in}} W$$

其中：
- $A$ 是骨骼图的邻接矩阵（关节连接关系）
- $I$ 是单位矩阵（自连接）
- $\Lambda$ 是度矩阵（归一化）
- $W$ 是可学习权重

#### （3）视频特征编码汇总

ST-GCN的256维输出经过MLP降维到**20维视频特征** $F_v \in \mathbb{R}^{20}$：

$$F_v = \text{ReLU}(W_v \mathbf{h}_{\text{video}} + b_v), \quad W_v \in \mathbb{R}^{20 \times 256}$$

这20维特征捕捉了教师的典型动作模式：
- 维度1-5：上肢动作（挥手、指向、手势）
- 维度6-10：下肢动作（走动、站立、空间移动）
- 维度11-15：躯干姿态（前倾、直立、转身）
- 维度16-20：综合行为（板书、互动、静止）

**相比单帧规则识别的优势**：
- 准确率提升17.7个百分点（从71.2%提升至88.9%）
- 推理速度快2.5倍（32帧批处理 vs 逐帧分类）
- 骨骼表征具有隐私保护优势（无需原始RGB像素）

### 3.3.3 文本模态特征提取

文本模态反映教师的教学语言特征和教学意图。本研究提出**BERT语义编码 + 层次化细粒度对话行为识别（H-DAR）**的文本分析链路。

#### （1）BERT语义编码

采用BERT模型将教师话语（语义单元）编码为固定维度的语义向量。

对于教师话语（语义单元） $s = [w_1, w_2, ..., w_n]$（$w_i$ 是词）：

$$\mathbf{h}_{\text{BERT}} = \text{BERT}([CLS], w_1, ..., w_n, [SEP])$$

取[CLS]位置的输出作为语义单元表征：

$$\mathbf{h}_s = \mathbf{h}_{\text{BERT}}[0] \in \mathbb{R}^{768}$$

#### （2）层次化细粒度对话行为识别（H-DAR）

传统对话行为识别多采用粗粒度四分类（提问、指令、讲解、反馈），但这无法有效区分不同教学风格的特征性语言模式。H-DAR将教学意图扩展为**10类细粒度分类**。

**细粒度对话行为分类体系**：

将教师话语分为**4个粗类、10个细类**：

| 粗类 | 细类 | 定义 | 示例 | 典型风格 |
|-----|------|-----|------|---------|
| **Question** | Heuristic-Q（启发性提问） | 引导学生深度思考的开放性问题 | "为什么会出现这种现象？" | 启发引导型 |
|  | Factual-Q（事实性提问） | 检查知识掌握的封闭性问题 | "这个概念是什么？" | 传统讲授型 |
| **Explanation** | Definition（概念定义） | 明确、精准地解释核心概念 | "所谓牛顿第一定律，就是..." | 理论讲授型 |
|  | Reasoning（逻辑推导） | 展示推理过程和因果关系 | "因为A，所以B，因此C" | 逻辑推导型 |
|  | Theory（理论讲授） | 系统性地讲解理论框架 | "根据信息论，我们可以..." | 理论讲授型 |
|  | Case-Study（案例分析） | 通过具体例子说明抽象概念 | "比如说，在实际生产中..." | 案例讲授型 |
| **Instruction** | Organization（组织指令） | 组织课堂活动、调整教学流程 | "请大家打开课本第50页" | 组织导向型 |
|  | Task（任务指令） | 布置学习任务和练习 | "请完成课后习题1-5题" | 任务导向型 |
| **Feedback** | Positive-FB（正向反馈） | 肯定、鼓励学生回答 | "很好！这个回答非常准确" | 情感表达型 |
|  | Corrective-FB（纠正反馈） | 指出错误并给予纠正 | "这里有个小错误，应该是..." | 纠正导向型 |

**设计原则**：
- **教育学导向**：细类划分基于教育学理论中的教学行为分类（如Bloom认知层次、CLASS维度）
- **风格区分度**：每个细类能够有效区分不同教学风格的特征性语言模式
- **标注可行性**：细类定义明确，人工标注一致性高（Kappa > 0.80）

**层次化分类架构**：

采用**两层分类器**：第1层进行粗分类（4类），第2层根据粗分类结果选择对应的细分类器（2-4个子类）。

$$\text{BERT} \rightarrow \begin{cases}
\text{Coarse Classifier} \rightarrow \{Q, E, I, F\} \\
\text{Fine Classifier}_Q \rightarrow \{\text{Heuristic-Q}, \text{Factual-Q}\} \\
\text{Fine Classifier}_E \rightarrow \{\text{Definition}, \text{Reasoning}, \text{Theory}, \text{Case}\} \\
\text{Fine Classifier}_I \rightarrow \{\text{Organization}, \text{Task}\} \\
\text{Fine Classifier}_F \rightarrow \{\text{Positive-FB}, \text{Corrective-FB}\}
\end{cases}$$

**步骤1：粗分类**

$$\mathbf{p}_{\text{coarse}} = \text{softmax}(W_c \mathbf{h}_s + b_c) \in \mathbb{R}^4$$

其中，$W_c \in \mathbb{R}^{4 \times 768}$。预测粗类别：$c = \arg\max(\mathbf{p}_{\text{coarse}})$

**步骤2：细分类**

根据粗类别 $c$ 选择对应的细分类器：

$$\mathbf{p}_{\text{fine}} = \text{softmax}(W_c^{\text{fine}} \mathbf{h}_s + b_c^{\text{fine}}) \in \mathbb{R}^{K_c}$$

其中，$K_c$ 是粗类 $c$ 的子类数量（2或4）。

**步骤3：联合训练**

损失函数结合粗分类和细分类：

$$\mathcal{L} = \alpha \cdot \mathcal{L}_{\text{coarse}} + (1-\alpha) \cdot \mathcal{L}_{\text{fine}}$$

其中，$\alpha = 0.3$ 是权重系数，$\mathcal{L}_{\text{coarse}}$ 和 $\mathcal{L}_{\text{fine}}$ 均为交叉熵损失。

**步骤4：对话行为分布统计**

对一节课的所有语义单元 $\{U_1, U_2, ..., U_N\}$，计算细粒度对话行为分布：

$$\mathbf{d}_{\text{act}} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{1}_{\text{act}}^{(i)} \in \mathbb{R}^{10}$$

其中，$\mathbf{1}_{\text{act}}^{(i)}$ 是one-hot编码（10维）。该分布向量作为教师的"教学意图画像"，能够有效区分不同教学风格。

#### （3）文本特征编码汇总

文本模态生成**35维编码向量** $F_{t} \in \mathbb{R}^{35}$，包含：

- **10维细粒度对话行为编码**：$\mathbf{d}_{\text{act}}$（10类对话行为的频率分布）
- **4维粗分类编码**：$\mathbf{p}_{\text{coarse}}$（Question/Explanation/Instruction/Feedback分布）
- **1维意图置信度**：$\max(\mathbf{p}_{\text{fine}})$（模型对预测的置信度）
- **20维NLP统计特征**：
  - 词数、句数、平均句长
  - 逻辑连接词频率（"因为""所以""但是"）
  - 专业术语数量
  - 疑问词频率（"为什么""如何""是否"）
  - 评价词频率（"很好""正确""错误"）
  - 重复率（相同词语重复次数）
  - 等等

### 3.3.4 多模态特征汇总

**表3.2：多模态特征编码汇总**

| 模态 | 特征维度 | 关键技术 | 特征内容 |
|-----|---------|---------|---------|
| **音频** | 15维 | Wav2Vec 2.0 + 情感分类 | 6维情感分布 + 语速 + VAR/SR + 韵律 + 极性 + 3维嵌入 |
| **视频** | 20维 | DeepSORT + ST-GCN | 上肢动作(5) + 下肢动作(5) + 躯干姿态(5) + 综合行为(5) |
| **文本** | 35维 | BERT + H-DAR | 10维细粒度意图 + 4维粗分类 + 1维置信度 + 20维NLP统计 |
| **总计** | 70维 | - | 多模态特征向量 $F = [F_v; F_a; F_t]$ |

这70维特征向量 $F \in \mathbb{R}^{70}$ 将作为后续SHAPE融合模型的输入，进行跨模态注意力融合与风格分类（详见第四章）。

**特征提取性能**：

- **处理速度**：0.82s/片段（三模态Pipeline并行）
  - 视频：0.35s（YOLOv8+DeepSORT+ST-GCN）
  - 音频：0.28s（Wav2Vec 2.0）
  - 文本：0.19s（BERT+H-DAR）
- **准确率**：
  - 音频情感分类：86.7%（6分类）
  - 视频动作识别：88.9%（6类动作）
  - 文本意图识别：89.0%（10类H-DAR）
