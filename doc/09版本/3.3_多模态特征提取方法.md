## 3.3 多模态特征提取方法

本节介绍视频、音频、文本三个模态的特征提取方法。三模态并行处理，最终生成70维特征向量（视频20维 + 音频15维 + 文本35维）作为后续SHAPE融合模型的输入。

### 3.3.1 音频模态特征提取

本研究采用Wav2Vec 2.0进行音频特征提取。Wav2Vec 2.0通过自监督对比学习从无标注音频中学习通用声学表征，在课堂噪声环境下相比传统MFCC特征具有更强的鲁棒性。

**特征提取流程**：

对于语义单元的音频片段 $\mathbf{x} \in \mathbb{R}^{N_s}$（$N_s$为采样点数，采样率16kHz），特征提取流程为：

$$\mathbf{h}_{\text{wav2vec}} = \text{Wav2Vec2}(\mathbf{x}) \in \mathbb{R}^{T \times 768}$$

$$\mathbf{h}_{\text{audio}} = \frac{1}{T}\sum_{t=1}^{T} \mathbf{h}_{\text{wav2vec}}[t] \in \mathbb{R}^{768}$$

$$\mathbf{p}_{\text{emotion}} = \text{softmax}(W_e \mathbf{h}_{\text{audio}} + b_e) \in \mathbb{R}^{6}$$

其中，$T$是时间帧数，$W_e \in \mathbb{R}^{6 \times 768}$是情感分类头权重，$\mathbf{p}_{\text{emotion}}$是6维情感分布（neutral/happy/sad/angry/surprise/fear）。

**15维音频特征编码**：

最终音频模态生成15维特征向量 $F_a \in \mathbb{R}^{15}$：

$$F_a = [\underbrace{p_{\text{neutral}},...,p_{\text{fear}}}_{\text{6维情感}}, \underbrace{v_{\text{speed}}}_{\text{语��}}, \underbrace{\text{VAR},\text{SR}}_{\text{活动比}}, \underbrace{\mu_{\text{vol}},\sigma_{\text{pitch}}}_{\text{韵律}}, \underbrace{e_{\text{polar}}}_{\text{极性}}, \underbrace{z_1,z_2,z_3}_{\text{嵌入压缩}}]$$

其中包括：6维情感分布、语速、语音活动比、静音比、音量均值、音高变化系数、情感极性分数，以及Wav2Vec 2.0嵌入的分段均值压缩（768维→3维）。

### 3.3.2 视频模态特征提取

视频模态特征提取采用"检测→追踪→姿态估计→动作识别"的流水线处理方式。

**（1）教师追踪：DeepSORT算法**

课堂场景存在多人干扰（学生走动、举手），单纯依赖YOLO检测会导致教师ID在遮挡后跳变。本研究采用DeepSORT算法，通过结合外观特征（ReID）和运动模型（卡尔曼滤波）实现稳定追踪，确保在整个视频中准确跟踪教师的位置和动作。

**（2）骨骼序列建模：ST-GCN**

本研究采用ST-GCN（Spatial Temporal Graph Convolutional Networks）进行骨骼序列时序建模。相比基于RGB的方法，骨骼序列表征具有计算效率高、抗遮挡性强、隐私保护等优势，特别适合教育场景应用。

对于输入骨骼序列 $X \in \mathbb{R}^{C \times T \times V}$（$C=3$坐标维度，$T=32$帧，$V=25$关节点），网络结构为：

$$\begin{aligned}
X_1 &= \text{ST-GCN-Block}(X_0, C_{\text{out}}=64) \\
X_2 &= \text{ST-GCN-Block}(X_1, C_{\text{out}}=128) \\
X_3 &= \text{ST-GCN-Block}(X_2, C_{\text{out}}=256) \\
\mathbf{h}_{\text{video}} &= \text{GAP}(X_3) \in \mathbb{R}^{256} \\
\mathbf{y} &= \text{softmax}(W_c \mathbf{h}_{\text{video}} + b_c) \in \mathbb{R}^{6}
\end{aligned}$$

其中，GAP是全局平均池化，$\mathbf{y}$是6类动作的概率分布（standing/walking/gesturing/writing/pointing/raise_hand）。

**20维视频特征编码**：

最终视觉模态生成20维特征向量 $F_v \in \mathbb{R}^{20}$：

$$F_v = [\underbrace{p_1,...,p_6}_{\text{6类动作频率}}, \underbrace{E_{\text{motion}}}_{\text{运动能量}}, \underbrace{H_1,...,H_9}_{\text{9宫格热力图}}, \underbrace{C_{\text{track}}}_{\text{轨迹连续性}}, \underbrace{t_{\text{norm}},n_{\text{frames}}}_{\text{时长}}, \underbrace{\bar{c}_{\text{pose}}}_{\text{姿态置信度}}]$$

其中包括：6类动作频率分布、运动能量、空间分布热力图（9宫格）、轨迹连续性、归一化时长、帧数、平均姿态置信度。

### 3.3.3 文本模态特征提取

文本模态特征提取包括语音转文本、文本清洗、BERT语义编码和对话行为识别四个步骤。

**（1）语音转文本（ASR）**

采用Whisper-medium模型进行语音识别，该模型支持中英混合识别：

$$T = \text{Whisper}(A)$$

其中，$A$是音频波形，$T$是转写文本。

**（2）文本清洗**

对转写文本进行预处理：
- 去除语气词：移除"嗯"、"啊"、"那个"等填充词
- 句子分割：按标点符号和停顿分割为句子
- 错别字纠正：使用拼音纠错模型（Pycorrector）

**（3）BERT语义编码**

使用BERT模型提取句子的语义表示：

$$\mathbf{h}_{\text{BERT}} = \text{BERT}([CLS], w_1, ..., w_n, [SEP]) \in \mathbb{R}^{768}$$

取[CLS]位置的输出作为语义单元表征。

**（4）对话行为识别**

基于BERT编码进行对话行为分类（Question/Instruction/Explanation/Feedback）：

$$p_{\text{act}} = \text{softmax}(\text{MLP}(\text{BERT}(T)))$$

其中，MLP是两层全连接网络，$p_{\text{act}} = [p_Q, p_I, p_E, p_F]$对应四类对话行为。

**35维文本特征编码**：

文本模态生成35维特征向量 $F_t \in \mathbb{R}^{35}$，包含：
- 10维细粒度对话行为编码（层次化分类，详见第四章）
- 4维粗分类编码
- 1维意图置信度
- 20维NLP统计特征（词数、句数、逻辑连接词频率、专业术语数等）

