## 3.4 创新3：SHAPE跨模态注意力融合模型

### 3.4.1 设计动机与网络架构

传统的多模态融合方法主要有三类，但均存在局限：

**(1) 早期融合（Early Fusion）**：直接拼接原始特征

$$F_{\text{concat}} = [F_v; F_a; F_t] \in \mathbb{R}^{70}$$

**局限性**：不同模态的维度和尺度差异大，高维模态会主导融合结果；无法建模模态间的交互关系；缺乏对不同模态重要性的自适应调整。

**(2) 晚期融合（Late Fusion）**：分别训练单模态分类器，结果加权平均

$$P_{\text{final}} = w_vP_v + w_aP_a + w_tP_t$$

**局限性**：权重 $w_v, w_a, w_t$ 固定，无法根据样本内容自适应调整；忽略了模态间的互补信息。

**(3) 中间融合（Middle Fusion）**：在特征层进行加权融合

$$F_{\text{weighted}} = w_vF_v + w_aF_a + w_tF_t$$

**局限性**：仍然是固定权重；不同模态的特征空间不一致，直接相加不合理。

**本研究采用跨模态注意力机制**，解决以下问题：
1. 不同模态在不同样本上的重要性（样本自适应）
2. 模态之间的交互关系（跨模态增强）
3. 决策依据的可解释性（注意力权重可视化）

#### SHAPE网络架构

SHAPE由五个核心模块组成（详见3.1.3节四层架构图）：

**模块1：特征投影层**

由于三个模态的原始特征维度不同（$F_v \in \mathbb{R}^{20}, F_a \in \mathbb{R}^{15}, F_t \in \mathbb{R}^{35}$），首先通过全连接层投影到统一维度 $d=512$：

$$F'_v = \text{ReLU}(W_vF_v + b_v), \quad F'_v \in \mathbb{R}^{512}$$
$$F'_a = \text{ReLU}(W_aF_a + b_a), \quad F'_a \in \mathbb{R}^{512}$$
$$F'_t = \text{ReLU}(W_tF_t + b_t), \quad F'_t \in \mathbb{R}^{512}$$

**模块2：跨模态注意力层**（核心创新）

对于每对模态 $(i,j)$，计算从模态 $i$ 到模态 $j$ 的注意力：

**步骤1：计算Query, Key, Value**

$$Q_i = F'_iW_Q^i, \quad K_j = F'_jW_K^j, \quad V_j = F'_jW_V^j$$

其中，$W_Q^i, W_K^j, W_V^j \in \mathbb{R}^{512 \times 64}$，注意力维度 $d_k=64$。

**步骤2：计算注意力权重**

$$\alpha_{i \to j} = \text{softmax}\left(\frac{Q_iK_j^T}{\sqrt{d_k}}\right)$$

**步骤3：跨模态增强**

$$\tilde{F}_i^{(j)} = \alpha_{i \to j}V_j$$

**步骤4：残差连接**

$$\tilde{F}_v = F'_v + \tilde{F}_v^{(a)} + \tilde{F}_v^{(t)}$$
$$\tilde{F}_a = F'_a + \tilde{F}_a^{(v)} + \tilde{F}_a^{(t)}$$
$$\tilde{F}_t = F'_t + \tilde{F}_t^{(v)} + \tilde{F}_t^{(a)}$$

**模块3：BiLSTM时序建模**

将课堂中的所有语义单元序列 $\{\tilde{F}_1, \tilde{F}_2, ..., \tilde{F}_N\}$ 输入BiLSTM：

$$\mathbf{h}_i = \text{BiLSTM}(\tilde{F}_i, \mathbf{h}_{i-1})$$

**模块4：注意力池化**

计算时序注意力权重 $\beta_i$，自适应聚合关键片段：

$$\beta_i = \frac{\exp(\mathbf{w}^T\mathbf{h}_i)}{\sum_{j=1}^N\exp(\mathbf{w}^T\mathbf{h}_j)}$$

$$\mathbf{h}_{\text{pool}} = \sum_{i=1}^N\beta_i\mathbf{h}_i$$

**模块5：风格分类器**

$$\mathbf{p} = \text{softmax}(W_{\text{cls}}\mathbf{h}_{\text{pool}} + b_{\text{cls}}) \in \mathbb{R}^7$$

**损失函数**：交叉熵 + 标签平滑（$\epsilon=0.1$）

$$\mathcal{L} = -\sum_{k=1}^7 y_k'\log(p_k), \quad y_k' = (1-\epsilon)y_k + \frac{\epsilon}{7}$$

### 3.4.2 实验验证与消融分析

#### （1）实验设置

**对比方法**：
1. **Single-Video**: 仅视觉特征
2. **Single-Audio**: 仅音频特征
3. **Single-Text**: 仅文本特征（H-DAR）
4. **Early Fusion**: 直接拼接 $[F_v; F_a; F_t]$
5. **Late Fusion**: 加权平均单模态预测
6. **SHAPE（Proposed）**: 跨模态注意力融合

**训练策略**：
- 优化器：Adam，学习率 $\eta=10^{-4}$
- Batch Size：32
- Epochs：50（早停策略）
- 余弦退火学习率调度

#### （2）SHAPE vs 简单融合

**表3.12：SHAPE vs 基线方法（测试集准确率）**

| 方法 | 准确率 | Precision | Recall | F1 | 相比最佳单模态 |
|-----|--------|-----------|--------|----|--------------| 
| Single-Video | 75.5% | 0.73 | 0.72 | 0.72 | baseline-V |
| Single-Audio | 72.6% | 0.70 | 0.69 | 0.70 | baseline-A |
| **Single-Text (H-DAR)** | **78.3%** | **0.77** | **0.76** | **0.76** | **baseline-T** |
| Early Fusion | 85.2% | 0.83 | 0.82 | 0.83 | +6.9% |
| Late Fusion | 87.6% | 0.86 | 0.85 | 0.85 | +9.3% |
| **SHAPE (Proposed)** | **91.4%** | **0.90** | **0.89** | **0.89** | **+13.1%** |

**关键发现**：
1. **SHAPE显著优于简单融合**：
   - vs Early Fusion: +6.2%（85.2% → 91.4%）
   - vs Late Fusion: +3.8%（87.6% → 91.4%）
   - vs最佳单模态（Text）: +13.1%（78.3% → 91.4%）

2. **文本模态最强**：H-DAR细粒度意图识别提供了强判别力（78.3%）

3. **跨模态注意力的价值**：相比Late Fusion提升3.8%，证明模态交互的重要性

#### （3）消融实验

**表3.13：SHAPE模块消融实验**

| 配置 | 准确率 | ΔAcc | 说明 |
|-----|--------|------|------|
| SHAPE (Complete) | 91.4% | baseline | 完整模型 |
| w/o Cross-Modal Attention | 88.7% | **-2.7%** | 改用简单拼接 |
| w/o BiLSTM | 89.8% | -1.6% | 去除时序建模 |
| w/o Attention Pooling | 90.3% | -1.1% | 改用平均池化 |
| w/o Residual Connection | 89.5% | -1.9% | 去除残差连接 |

**关键发现**：
1. **跨模态注意力最关键**：移除后性能下降最大（-2.7%，p<0.01）
2. **时序建模重要**：BiLSTM贡献1.6%
3. **注意力池化有效**：相比平均池化提升1.1%

**统计显著性检验**（SHAPE vs Late Fusion）：
- 配对t检验：$t=4.12, p<0.01$
- McNemar检验：$\chi^2=6.8, p<0.01$

#### （4）模态重要性分析

通过统计跨模态注意力权重 $\alpha$，分析不同风格对模态的依赖：

**表3.14：七类风格的跨模态注意力权重统计**

| 风格 | 视觉 $\alpha_v$ | 音频 $\alpha_a$ | 文本 $\alpha_t$ | 主导模态 |
|-----|----------------|----------------|----------------|---------|
| 情感表达型 | 0.26 | **0.62** | 0.12 | 音频主导 |
| 互动导向型 | **0.50** | 0.28 | 0.22 | 视觉主导 |
| 逻辑推导型 | 0.22 | 0.25 | **0.53** | 文本主导 |
| 理论讲授型 | 0.20 | 0.27 | **0.53** | 文本主导 |
| 启发引导型 | 0.33 | 0.34 | 0.33 | 均衡 |
| 耐心细致型 | 0.28 | **0.42** | 0.30 | 音频主导 |
| 题目驱动型 | **0.45** | 0.28 | 0.27 | 视觉主导 |

**关键发现**：
1. **不同风格对模态的依赖显著不同**（F=42.3, p<0.001）
2. **音频主导**：情感表达型（0.62）、耐心细致型（0.42）
3. **视觉主导**：互动导向型（0.50）、题目驱动型（0.45）
4. **文本主导**：逻辑推导型（0.53）、理论讲授型（0.53）
5. **均衡型**：启发引导型（标准差0.015）

### 本节小结

本节提出并验证了**SHAPE跨模态注意力融合模型**，这是SHAPE引擎的第三项核心创新。

**核心贡献**：
- **五模块架构**：投影→注意力→BiLSTM→池化→分类器
- **跨模态注意力**：样本自适应的模态重要性调整（6个注意力权重）
- **端到端可解释**：注意力权重 $\alpha, \beta$ 可视化，追溯决策依据

**定量结果**：
- **vs Early Fusion**: +6.2%（85.2% → 91.4%）
- **vs Late Fusion**: +3.8%（87.6% → 91.4%）
- **vs最佳单模态**: +13.1%（78.3% → 91.4%）
- **跨模态注意力贡献**: -2.7%（p<0.01，消融后88.7%）

**可解释性**：
- 揭示了不同风格对模态的依赖模式
- 为教育学研究提供了定量证据

这些结果表明，**SHAPE是一项有效的创新**，通过跨模态注意力机制实现了样本自适应的模态融合，显著提升了教师风格识别性能。
