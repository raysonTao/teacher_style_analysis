# 教师教学风格画像分析系统V1.0 设计说明书

**软件名称**：教师教学风格画像分析系统V1.0
**版本号**：V1.0
**编制日期**：2026年01月
**编制单位**：[申请人姓名]

---

## 目录

1. [引言](#1-引言)
2. [系统总体架构设计](#2-系统总体架构设计)
3. [数据库设计](#3-数据库设计)
4. [功能模块设计](#4-功能模块设计)
5. [接口设计](#5-接口设计)
6. [关键算法设计](#6-关键算法设计)
7. [性能设计](#7-性能设计)
8. [安全设计](#8-安全设计)

---

## 1. 引言

### 1.1 编写目的

本设计说明书的编写目的是详细描述"教师教学风格画像分析系统V1.0"的系统架构、数据库设计、功能模块、接口规范和关键算法实现，为系统开发、测试、维护提供技术依据。

本文档预期读者包括：
- 系统开发人员
- 系统测试人员
- 系统维护人员
- 技术评审人员

### 1.2 项目背景

传统课堂评价依赖人工观察和主观判断，存在效率低、成本高、覆盖面窄等问题。本系统利用计算机视觉、语音识别、自然语言处理和深度学习技术，实现课堂录像的自动化分析，识别教师教学风格，生成可视化画像报告，为教师专业发展和教学质量评估提供客观数据支撑。

### 1.3 定义与缩略语

| 术语/缩略语 | 全称/解释 |
|------------|---------|
| MMAN | Multi-Modal Attention Network，跨模态注意力网络 |
| ST-GCN | Spatial-Temporal Graph Convolutional Network，时空图卷积网络 |
| DeepSORT | Deep Simple Online and Realtime Tracking，深度实时目标跟踪 |
| Wav2Vec2 | Wave-to-Vector 2，自监督语音表征学习模型 |
| BERT | Bidirectional Encoder Representations from Transformers |
| SHAP | SHapley Additive exPlanations，模型可解释性方法 |
| ASR | Automatic Speech Recognition，自动语音识别 |
| DAR | Dialogue Act Recognition，对话行为识别 |
| VAD | Voice Activity Detection，语音活动检测 |
| SMI | Style Matching Index，风格匹配指数 |
| REST | Representational State Transfer，表述性状态转移 |
| ORM | Object-Relational Mapping，对象关系映射 |
| JWT | JSON Web Token，JSON网络令牌 |

### 1.4 参考资料

1. GB/T 8567-2006《计算机软件文档编制规范》
2. GB/T 11457-2006《软件工程术语》
3. 《深度学习》（Ian Goodfellow等著）
4. PyTorch官方文档：https://pytorch.org/docs/
5. Flask官方文档：https://flask.palletsprojects.com/

---

## 2. 系统总体架构设计

### 2.1 系统架构概述

系统采用**五层架构设计**，从下至上分别为：

```
┌─────────────────────────────────────────────────────┐
│              表现层 (Presentation Layer)             │
│   Web UI (Vue.js) │ RESTful API │ WebSocket        │
└─────────────────────────────────────────────────────┘
                          ↓↑
┌─────────────────────────────────────────────────────┐
│               业务逻辑层 (Business Layer)             │
│  课程管理 │ 分析调度 │ 画像生成 │ 反馈引擎           │
└─────────────────────────────────────────────────────┘
                          ↓↑
┌─────────────────────────────────────────────────────┐
│          AI推理层 (AI Inference Layer)              │
│  MMAN模型 │ 特征提取 │ SHAP解释器 │ 推荐引擎        │
└─────────────────────────────────────────────────────┘
                          ↓↑
┌─────────────────────────────────────────────────────┐
│            数据访问层 (Data Access Layer)             │
│  PostgreSQL (ORM) │ Redis (Cache) │ File Storage   │
└─────────────────────────────────────────────────────┘
                          ↓↑
┌─────────────────────────────────────────────────────┐
│           基础设施层 (Infrastructure Layer)           │
│  Linux/Win Server │ NVIDIA GPU │ FFmpeg │ Nginx    │
└─────────────────────────────────────────────────────┘
```

### 2.2 技术栈选型

#### 后端技术栈

| 技术组件 | 版本 | 用途 |
|---------|------|-----|
| Python | 3.9+ | 主要开发语言 |
| Flask | 2.3.0 | Web应用框架 |
| PyTorch | 2.0.1 | 深度学习框架 |
| SQLAlchemy | 2.0 | ORM数据库操作 |
| Celery | 5.3 | 异步任务队列 |
| Redis | 7.0 | 缓存与消息队列 |
| PostgreSQL | 14 | 关系型数据库 |
| Gunicorn | 20.1 | WSGI服务器 |
| Nginx | 1.24 | 反向代理与静态文件服务 |

#### 前端技术栈

| 技术组件 | 版本 | 用途 |
|---------|------|-----|
| Vue.js | 3.2 | 前端框架 |
| Vue Router | 4.1 | 路由管理 |
| Vuex | 4.0 | 状态管理 |
| Element Plus | 2.3 | UI组件库 |
| ECharts | 5.4 | 数据可视化 |
| Video.js | 8.0 | 视频播放器 |
| Axios | 1.4 | HTTP客户端 |

#### AI技术栈

| 技术组件 | 版本 | 用途 |
|---------|------|-----|
| YOLOv8 | ultralytics 8.0 | 目标检测 |
| MediaPipe | 0.10 | 姿态估计 |
| Whisper | openai-whisper 20230314 | 语音识别 |
| Transformers | 4.30 | 预训练模型库（BERT, Wav2Vec2） |
| OpenCV | 4.8 | 计算机视觉 |
| Librosa | 0.10 | 音频处理 |
| SHAP | 0.42 | 模型解释 |

### 2.3 部署架构

#### 2.3.1 单机部署架构

```
┌────────────────────────────────────────┐
│          Nginx (Port 80/443)           │
│  - 反向代理                             │
│  - 静态文件服务                          │
│  - SSL终结                              │
└────────────────┬───────────────────────┘
                 │
        ┌────────┴────────┐
        │                 │
┌───────▼─────┐  ┌───────▼─────┐
│   Flask     │  │   Celery    │
│  (Gunicorn) │  │   Worker    │
│  Port 5000  │  │             │
└──────┬──────┘  └──────┬──────┘
       │                │
       └────────┬───────┘
                │
    ┌───────────┼───────────┐
    │           │           │
┌───▼──┐  ┌────▼───┐  ┌───▼────┐
│Redis │  │Postgres│  │ Files  │
│ 6379 │  │  5432  │  │/uploads│
└──────┘  └────────┘  └────────┘
```

#### 2.3.2 分布式部署架构（可选）

```
┌─────────────────────────────────────────────┐
│        Load Balancer (Nginx/HAProxy)        │
└──────┬──────────────┬──────────────┬────────┘
       │              │              │
┌──────▼──────┐ ┌────▼──────┐ ┌────▼──────┐
│  Web Server │ │ Web Server│ │ Web Server│
│   Node 1    │ │   Node 2  │ │   Node 3  │
└──────┬──────┘ └────┬──────┘ └────┬──────┘
       │             │              │
       └─────────────┼──────────────┘
                     │
    ┌────────────────┼────────────────┐
    │                │                │
┌───▼──────┐  ┌─────▼─────┐  ┌──────▼──────┐
│  Redis   │  │ PostgreSQL│  │  AI Cluster │
│ Cluster  │  │ Master-   │  │  (GPU Nodes)│
│(Sentinel)│  │  Slave    │  │             │
└──────────┘  └───────────┘  └─────────────┘
```

### 2.4 系统工作流程

#### 2.4.1 视频上传与分析流程

```
[用户] → [上传视频] → [Web Server]
                          ↓
                     [保存文件]
                          ↓
                     [创建任务]
                          ↓
                    [Celery Queue]
                          ↓
                   [Celery Worker]
                          ↓
        ┌─────────────────┼─────────────────┐
        │                 │                 │
  [视频预处理]       [音频预处理]       [文本预处理]
        │                 │                 │
  [YOLOv8+DeepSORT]  [Wav2Vec2]        [Whisper ASR]
        │                 │                 │
  [MediaPipe+STGCN]  [情感提取]        [BERT DAR]
        │                 │                 │
        └─────────────────┼─────────────────┘
                          ↓
                    [MMAN融合模型]
                          ↓
                    [风格分类输出]
                          ↓
              ┌───────────┼───────────┐
              │           │           │
        [SHAP解释]   [典型片段]   [改进建议]
              │           │           │
              └───────────┼───────────┘
                          ↓
                    [生成画像报告]
                          ↓
                    [保存到数据库]
                          ↓
                   [通知用户完成]
```

---

## 3. 数据库设计

### 3.1 数据库ER图

```
┌──────────────┐          ┌──────────────┐
│    User      │1        n│   Course     │
│──────────────│──────────│──────────────│
│ id (PK)      │          │ id (PK)      │
│ username     │          │ user_id (FK) │
│ email        │          │ name         │
│ password     │          │ teacher_name │
│ created_at   │          │ video_path   │
└──────────────┘          │ status       │
                          │ created_at   │
                          └──────┬───────┘
                                 │1
                                 │
                                 │n
                          ┌──────▼───────┐
                          │   Analysis   │
                          │──────────────│
                          │ id (PK)      │
                          │ course_id(FK)│
                          │ style_scores │
                          │ features     │
                          │ finished_at  │
                          └──────┬───────┘
                                 │1
                    ┌────────────┼────────────┐
                    │n           │n           │n
            ┌───────▼──────┐ ┌──▼────────┐ ┌─▼──────────┐
            │   Segment    │ │   SHAP    │ │ Feedback   │
            │──────────────│ │───────────│ │────────────│
            │ id (PK)      │ │ id (PK)   │ │ id (PK)    │
            │analysis_id(FK│ │analysis_id│ │analysis_id │
            │ start_time   │ │ feature   │ │ suggestions│
            │ end_time     │ │ contrib   │ │ smi_score  │
            │ label        │ │ values    │ │created_at  │
            └──────────────┘ └───────────┘ └────────────┘
```

### 3.2 数据表设计

#### 3.2.1 用户表 (users)

存储系统用户信息。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 用户ID |
| username | VARCHAR(50) | UNIQUE, NOT NULL | 用户名 |
| email | VARCHAR(100) | UNIQUE, NOT NULL | 邮箱 |
| password_hash | VARCHAR(255) | NOT NULL | 密码哈希值 |
| avatar | VARCHAR(255) | NULL | 头像路径 |
| role | VARCHAR(20) | DEFAULT 'user' | 角色：user/admin |
| status | VARCHAR(20) | DEFAULT 'active' | 状态：active/inactive |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |
| updated_at | TIMESTAMP | DEFAULT NOW() | 更新时间 |

索引：
- `idx_username` ON username
- `idx_email` ON email

#### 3.2.2 课程表 (courses)

存储上传的课程视频信息。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 课程ID |
| user_id | INTEGER | FOREIGN KEY → users(id) | 上传用户ID |
| name | VARCHAR(200) | NOT NULL | 课程名称 |
| teacher_name | VARCHAR(50) | NOT NULL | 教师姓名 |
| subject | VARCHAR(50) | NOT NULL | 学科 |
| grade | VARCHAR(50) | NOT NULL | 学段 |
| course_type | VARCHAR(50) | NOT NULL | 课程类型 |
| record_date | DATE | NULL | 录制日期 |
| video_path | VARCHAR(500) | NOT NULL | 视频文件路径 |
| video_duration | INTEGER | NULL | 视频时长（秒） |
| thumbnail | VARCHAR(255) | NULL | 缩略图路径 |
| status | VARCHAR(20) | DEFAULT 'pending' | 状态：pending/processing/completed/failed |
| error_message | TEXT | NULL | 错误信息 |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |
| updated_at | TIMESTAMP | DEFAULT NOW() | 更新时间 |

索引：
- `idx_user_id` ON user_id
- `idx_status` ON status
- `idx_created_at` ON created_at

#### 3.2.3 分析结果表 (analyses)

存储课程的风格分析结果。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 分析ID |
| course_id | INTEGER | FOREIGN KEY → courses(id) | 课程ID |
| style_scores | JSONB | NOT NULL | 七类风格得分 |
| modality_contrib | JSONB | NULL | 模态贡献度 |
| video_features | JSONB | NULL | 视频特征向量 |
| audio_features | JSONB | NULL | 音频特征向量 |
| text_features | JSONB | NULL | 文本特征向量 |
| rule_features | JSONB | NULL | 规则特征向量 |
| attention_weights | JSONB | NULL | 注意力权重 |
| processing_time | FLOAT | NULL | 处理耗时（秒） |
| started_at | TIMESTAMP | NULL | 开始时间 |
| finished_at | TIMESTAMP | NULL | 完成时间 |

索引：
- `idx_course_id` ON course_id
- `idx_finished_at` ON finished_at

**style_scores JSONB 结构示例**：
```json
{
  "理论讲授型": 0.78,
  "启发引导型": 0.45,
  "互动导向型": 0.32,
  "逻辑推导型": 0.65,
  "题目驱动型": 0.56,
  "情感表达型": 0.23,
  "耐心细致型": 0.51
}
```

#### 3.2.4 典型片段表 (segments)

存储自动提取的典型教学片段。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 片段ID |
| analysis_id | INTEGER | FOREIGN KEY → analyses(id) | 分析ID |
| start_time | FLOAT | NOT NULL | 开始时间（秒） |
| end_time | FLOAT | NOT NULL | 结束时间（秒） |
| label | VARCHAR(100) | NOT NULL | 片段标签 |
| confidence | FLOAT | NULL | 置信度 |
| shap_contrib | FLOAT | NULL | SHAP贡献度 |
| thumbnail | VARCHAR(255) | NULL | 片段缩略图 |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |

索引：
- `idx_analysis_id` ON analysis_id

#### 3.2.5 SHAP解释表 (shap_explanations)

存储SHAP特征解释结果。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 解释ID |
| analysis_id | INTEGER | FOREIGN KEY → analyses(id) | 分析ID |
| feature_name | VARCHAR(100) | NOT NULL | 特征名称 |
| feature_value | FLOAT | NULL | 特征值 |
| shap_value | FLOAT | NOT NULL | SHAP贡献值 |
| modality | VARCHAR(20) | NULL | 所属模态 |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |

索引：
- `idx_analysis_id` ON analysis_id
- `idx_shap_value` ON shap_value DESC

#### 3.2.6 反馈建议表 (feedbacks)

存储生成的个性化反馈建议。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | SERIAL | PRIMARY KEY | 反馈ID |
| analysis_id | INTEGER | FOREIGN KEY → analyses(id) | 分析ID |
| smi_score | FLOAT | NULL | SMI风格匹配指数 |
| suggestions | JSONB | NOT NULL | 改进建议列表 |
| target_style | VARCHAR(50) | NULL | 目标风格 |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |

索引：
- `idx_analysis_id` ON analysis_id

**suggestions JSONB 结构示例**：
```json
[
  {
    "title": "增加提问频次",
    "current": "平均每10分钟提问2.3次",
    "target": "每10分钟提问4-6次",
    "strategy": "在知识点讲解后设计2-3个渐进式问题",
    "priority": "high"
  },
  {
    "title": "延长等待时长",
    "current": "提问后平均等待1.8秒",
    "target": "等待3-5秒",
    "strategy": "提问后有意识地数3秒再补充说明",
    "priority": "medium"
  }
]
```

#### 3.2.7 任务队列表 (tasks)

存储Celery异步任务状态。

| 字段名 | 数据类型 | 约束 | 说明 |
|-------|---------|------|------|
| id | VARCHAR(155) | PRIMARY KEY | Celery任务ID |
| course_id | INTEGER | FOREIGN KEY → courses(id) | 关联课程ID |
| task_name | VARCHAR(255) | NOT NULL | 任务名称 |
| status | VARCHAR(50) | NOT NULL | 状态：PENDING/STARTED/SUCCESS/FAILURE/RETRY |
| result | TEXT | NULL | 任务结果 |
| traceback | TEXT | NULL | 错误堆栈 |
| created_at | TIMESTAMP | DEFAULT NOW() | 创建时间 |
| updated_at | TIMESTAMP | DEFAULT NOW() | 更新时间 |

索引：
- `idx_course_id` ON course_id
- `idx_status` ON status

### 3.3 Redis缓存设计

#### 3.3.1 缓存键命名规范

```
模块:功能:标识符[:子标识符]
```

示例：
- `user:session:12345` - 用户会话
- `course:info:67890` - 课程基本信息
- `analysis:result:123` - 分析结果
- `model:cache:mman:video_features` - 模型特征缓存

#### 3.3.2 主要缓存内容

| 缓存键模式 | 数据类型 | 过期时间 | 说明 |
|-----------|---------|---------|------|
| `user:session:{user_id}` | String | 7天 | 用户会话信息 |
| `course:info:{course_id}` | Hash | 1小时 | 课程基本信息 |
| `analysis:result:{analysis_id}` | Hash | 24小时 | 分析结果缓存 |
| `task:status:{task_id}` | String | 24小时 | 任务状态 |
| `rate:limit:{user_id}:{action}` | String | 1分钟 | 频率限制 |
| `model:features:{course_id}:{modality}` | String | 1小时 | 特征向量缓存 |

---

## 4. 功能模块设计

### 4.1 模块划分

系统分为10个核心功能模块：

```
┌─────────────────────────────────────────────────────┐
│                    系统总体                          │
└──────────────────┬──────────────────────────────────┘
                   │
    ┌──────────────┼──────────────┐
    │              │              │
┌───▼───┐    ┌────▼────┐    ┌───▼────┐
│用户管理│    │课程管理 │    │分析调度 │
└───────┘    └────┬────┘    └───┬────┘
                  │              │
    ┌─────────────┼──────────────┼─────────────┐
    │             │              │             │
┌───▼────┐  ┌────▼────┐  ┌─────▼─────┐  ┌────▼────┐
│视频特征 │  │音频特征 │  │文本特征   │  │规则特征 │
│提取模块 │  │提取模块 │  │提取模块   │  │提取模块 │
└────┬───┘  └────┬────┘  └─────┬─────┘  └────┬────┘
     │           │              │             │
     └───────────┼──────────────┼─────────────┘
                 │              │
            ┌────▼──────────────▼────┐
            │   MMAN融合模型模块     │
            └────┬───────────────────┘
                 │
    ┌────────────┼────────────┐
    │            │            │
┌───▼──┐   ┌────▼────┐  ┌───▼───┐
│画像生成│   │可解释性 │  │反馈引擎│
│模块    │   │分析模块 │  │模块    │
└────────┘   └─────────┘  └────────┘
```

### 4.2 用户管理模块

#### 4.2.1 功能描述

负责用户注册、登录、权限管理、会话管理等功能。

#### 4.2.2 主要类设计

```python
class User(db.Model):
    """用户模型"""
    __tablename__ = 'users'

    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(50), unique=True, nullable=False)
    email = db.Column(db.String(100), unique=True, nullable=False)
    password_hash = db.Column(db.String(255), nullable=False)
    role = db.Column(db.String(20), default='user')
    status = db.Column(db.String(20), default='active')
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    # 关系
    courses = db.relationship('Course', backref='owner', lazy='dynamic')

    def set_password(self, password: str):
        """设置密码"""
        self.password_hash = generate_password_hash(password)

    def check_password(self, password: str) -> bool:
        """验证密码"""
        return check_password_hash(self.password_hash, password)

    def generate_token(self) -> str:
        """生成JWT令牌"""
        payload = {
            'user_id': self.id,
            'exp': datetime.utcnow() + timedelta(days=7)
        }
        return jwt.encode(payload, current_app.config['SECRET_KEY'])
```

#### 4.2.3 关键函数

```python
def register(username: str, email: str, password: str) -> dict:
    """
    用户注册

    Args:
        username: 用户名
        email: 邮箱
        password: 密码

    Returns:
        {'success': bool, 'message': str, 'user_id': int}
    """
    # 1. 验证输入
    if not is_valid_email(email):
        return {'success': False, 'message': '邮箱格式不正确'}

    if len(password) < 8:
        return {'success': False, 'message': '密码至少8位'}

    # 2. 检查用户名/邮箱是否存在
    if User.query.filter_by(username=username).first():
        return {'success': False, 'message': '用户名已存在'}

    if User.query.filter_by(email=email).first():
        return {'success': False, 'message': '邮箱已被注册'}

    # 3. 创建用户
    user = User(username=username, email=email)
    user.set_password(password)
    db.session.add(user)
    db.session.commit()

    # 4. 发送验证邮件
    send_verification_email(user.email)

    return {'success': True, 'message': '注册成功', 'user_id': user.id}


def login(username_or_email: str, password: str) -> dict:
    """
    用户登录

    Returns:
        {'success': bool, 'message': str, 'token': str, 'user': dict}
    """
    # 1. 查找用户
    user = User.query.filter(
        (User.username == username_or_email) |
        (User.email == username_or_email)
    ).first()

    if not user:
        return {'success': False, 'message': '用户不存在'}

    # 2. 验证密码
    if not user.check_password(password):
        return {'success': False, 'message': '密码错误'}

    # 3. 检查用户状态
    if user.status != 'active':
        return {'success': False, 'message': '账号已被禁用'}

    # 4. 生成Token
    token = user.generate_token()

    # 5. 缓存会话信息
    redis_client.setex(
        f'user:session:{user.id}',
        7 * 24 * 3600,
        json.dumps({'user_id': user.id, 'username': user.username})
    )

    return {
        'success': True,
        'message': '登录成功',
        'token': token,
        'user': user.to_dict()
    }
```

### 4.3 课程管理模块

#### 4.3.1 功能描述

负责课程视频上传、存储、管理、查询等功能。

#### 4.3.2 主要类设计

```python
class Course(db.Model):
    """课程模型"""
    __tablename__ = 'courses'

    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, db.ForeignKey('users.id'))
    name = db.Column(db.String(200), nullable=False)
    teacher_name = db.Column(db.String(50), nullable=False)
    subject = db.Column(db.String(50), nullable=False)
    grade = db.Column(db.String(50), nullable=False)
    video_path = db.Column(db.String(500), nullable=False)
    video_duration = db.Column(db.Integer)
    status = db.Column(db.String(20), default='pending')
    created_at = db.Column(db.DateTime, default=datetime.utcnow)

    # 关系
    analyses = db.relationship('Analysis', backref='course', lazy='dynamic')

    def get_video_url(self) -> str:
        """获取视频访问URL"""
        return f"/api/videos/{self.id}/stream"

    def start_analysis(self):
        """启动分析任务"""
        from tasks import analyze_course
        task = analyze_course.delay(self.id)
        return task.id
```

#### 4.3.3 关键函数

```python
def upload_course(user_id: int, file, metadata: dict) -> dict:
    """
    上传课程视频

    Args:
        user_id: 用户ID
        file: 视频文件对象
        metadata: 课程元数据（名称、教师、学科等）

    Returns:
        {'success': bool, 'course_id': int, 'task_id': str}
    """
    # 1. 验证文件
    if not allowed_file(file.filename):
        return {'success': False, 'message': '不支持的文件格式'}

    file_size = get_file_size(file)
    if file_size > MAX_FILE_SIZE:
        return {'success': False, 'message': '文件超过2GB限制'}

    # 2. 生成文件路径
    filename = secure_filename(file.filename)
    timestamp = int(time.time())
    unique_filename = f"{timestamp}_{filename}"
    file_path = os.path.join(UPLOAD_FOLDER, unique_filename)

    # 3. 保存文件
    file.save(file_path)

    # 4. 获取视频时长
    duration = get_video_duration(file_path)

    # 5. 生成缩略图
    thumbnail_path = generate_thumbnail(file_path)

    # 6. 创建课程记录
    course = Course(
        user_id=user_id,
        name=metadata['name'],
        teacher_name=metadata['teacher_name'],
        subject=metadata['subject'],
        grade=metadata['grade'],
        video_path=file_path,
        video_duration=duration,
        thumbnail=thumbnail_path,
        status='pending'
    )
    db.session.add(course)
    db.session.commit()

    # 7. 启动分析任务
    task_id = course.start_analysis()

    return {
        'success': True,
        'course_id': course.id,
        'task_id': task_id
    }
```

### 4.4 特征提取模块

#### 4.4.1 视频特征提取模块

```python
class VideoFeatureExtractor:
    """视频特征提取器"""

    def __init__(self, device='cuda'):
        self.device = device
        self.yolo = YOLO('checkpoints/yolov8n.pt')
        self.deepsort = DeepSort()
        self.mediapipe_pose = mp.solutions.pose.Pose()
        self.stgcn = STGCNModel().to(device)
        self.stgcn.load_state_dict(torch.load('checkpoints/stgcn.pth'))
        self.stgcn.eval()

    def extract(self, video_path: str) -> np.ndarray:
        """
        提取视频特征

        Args:
            video_path: 视频文件路径

        Returns:
            features: (T, 20) 视频特征向量序列
        """
        # 1. 视频读取与预处理
        segments = self._preprocess_video(video_path)

        features_list = []
        for segment in segments:
            # 2. 目标检测（YOLOv8）
            detections = self._detect_persons(segment['frames'])

            # 3. 多目标跟踪（DeepSORT）
            tracks = self._track_teacher(detections, segment['frames'])

            # 4. 姿态估计（MediaPipe）
            skeletons = self._extract_skeletons(tracks, segment['frames'])

            # 5. 动作识别（ST-GCN）
            action_features = self._recognize_actions(skeletons)

            features_list.append(action_features)

        return np.array(features_list)  # (T, 20)

    def _detect_persons(self, frames: List[np.ndarray]) -> List[dict]:
        """使用YOLOv8检测人物"""
        detections = []
        for frame in frames:
            results = self.yolo(frame, classes=[0])  # 0=person
            boxes = results[0].boxes.xyxy.cpu().numpy()
            confs = results[0].boxes.conf.cpu().numpy()
            detections.append({'boxes': boxes, 'scores': confs})
        return detections

    def _track_teacher(self, detections: List[dict], frames: List[np.ndarray]) -> dict:
        """使用DeepSORT追踪教师"""
        teacher_id = None
        teacher_tracks = []

        for frame_idx, (detection, frame) in enumerate(zip(detections, frames)):
            # DeepSORT更新
            tracks = self.deepsort.update(
                detection['boxes'],
                detection['scores'],
                frame
            )

            # 识别教师ID（基于空间先验和持续时长）
            if teacher_id is None and len(tracks) > 0:
                teacher_id = self._identify_teacher(tracks)

            # 提取教师轨迹
            teacher_track = [t for t in tracks if t['id'] == teacher_id]
            if teacher_track:
                teacher_tracks.append(teacher_track[0])

        return {'id': teacher_id, 'tracks': teacher_tracks}

    def _extract_skeletons(self, tracks: dict, frames: List[np.ndarray]) -> np.ndarray:
        """使用MediaPipe提取骨架序列"""
        skeletons = []
        for track, frame in zip(tracks['tracks'], frames):
            x1, y1, x2, y2 = track['bbox']
            person_crop = frame[int(y1):int(y2), int(x1):int(x2)]

            # MediaPipe姿态估计
            results = self.mediapipe_pose.process(person_crop)
            if results.pose_landmarks:
                landmarks = np.array([
                    [lm.x, lm.y] for lm in results.pose_landmarks.landmark
                ])  # (33, 2)
                skeletons.append(landmarks)
            else:
                skeletons.append(np.zeros((33, 2)))

        return np.array(skeletons)  # (T, 33, 2)

    def _recognize_actions(self, skeletons: np.ndarray) -> np.ndarray:
        """使用ST-GCN识别动作"""
        # 转换为PyTorch张量
        x = torch.tensor(skeletons, dtype=torch.float32).unsqueeze(0).to(self.device)

        # ST-GCN推理
        with torch.no_grad():
            features = self.stgcn(x)  # (1, 20)

        return features.cpu().numpy().squeeze()  # (20,)
```

#### 4.4.2 音频特征提取模块

```python
class AudioFeatureExtractor:
    """音频特征提取器"""

    def __init__(self, device='cuda'):
        self.device = device
        self.wav2vec2 = Wav2Vec2Model.from_pretrained(
            'checkpoints/wav2vec2-base-960h'
        ).to(device)
        self.wav2vec2.eval()
        self.emotion_model = EmotionClassifier().to(device)
        self.emotion_model.load_state_dict(torch.load('checkpoints/emotion.pth'))
        self.emotion_model.eval()

    def extract(self, video_path: str) -> np.ndarray:
        """
        提取音频特征

        Args:
            video_path: 视频文件路径

        Returns:
            features: (T, 15) 音频特征向量序列
        """
        # 1. 提取音频
        audio_path = self._extract_audio(video_path)

        # 2. 语音活动检测
        speech_segments = self._voice_activity_detection(audio_path)

        # 3. 说话人分离
        teacher_segments = self._speaker_diarization(speech_segments)

        features_list = []
        for segment in teacher_segments:
            # 4. Wav2Vec2特征提取
            wav2vec_features = self._extract_wav2vec_features(segment['audio'])

            # 5. 情感识别
            emotion_features = self._extract_emotion_features(segment['audio'])

            # 6. 韵律特征
            prosody_features = self._extract_prosody_features(segment['audio'])

            # 拼接特征
            combined = np.concatenate([
                wav2vec_features,    # (6,)
                emotion_features,    # (3,)
                prosody_features     # (6,)
            ])  # (15,)

            features_list.append(combined)

        return np.array(features_list)  # (T, 15)

    def _extract_wav2vec_features(self, audio: np.ndarray) -> np.ndarray:
        """使用Wav2Vec2提取声学表征"""
        # 转换为PyTorch张量
        x = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(self.device)

        # Wav2Vec2推理
        with torch.no_grad():
            outputs = self.wav2vec2(x)
            hidden_states = outputs.last_hidden_state  # (1, T, 768)

        # 时间维度池化
        pooled = hidden_states.mean(dim=1)  # (1, 768)

        # 降维到6维
        features = self._project_features(pooled, target_dim=6)

        return features.cpu().numpy().squeeze()  # (6,)

    def _extract_emotion_features(self, audio: np.ndarray) -> np.ndarray:
        """提取情感特征"""
        x = torch.tensor(audio, dtype=torch.float32).unsqueeze(0).to(self.device)

        with torch.no_grad():
            emotion_probs = self.emotion_model(x)  # (1, 3) [positive, neutral, negative]

        return emotion_probs.cpu().numpy().squeeze()  # (3,)

    def _extract_prosody_features(self, audio: np.ndarray) -> np.ndarray:
        """提取韵律特征（语速、音高、能量等）"""
        # 语速
        speech_rate = self._calculate_speech_rate(audio)

        # 音高统计
        pitch = librosa.yin(audio, fmin=50, fmax=500)
        pitch_mean = np.mean(pitch)
        pitch_std = np.std(pitch)

        # 能量统计
        energy = librosa.feature.rms(y=audio)[0]
        energy_mean = np.mean(energy)
        energy_std = np.std(energy)

        # 零交叉率
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        zcr_mean = np.mean(zcr)

        return np.array([
            speech_rate, pitch_mean, pitch_std,
            energy_mean, energy_std, zcr_mean
        ])  # (6,)
```

#### 4.4.3 文本特征提取模块

```python
class TextFeatureExtractor:
    """文本特征提取器"""

    def __init__(self, device='cuda'):
        self.device = device
        self.whisper = whisper.load_model("base")
        self.bert = BertModel.from_pretrained('checkpoints/bert-base-chinese').to(device)
        self.dar_classifier = DialogueActClassifier().to(device)
        self.dar_classifier.load_state_dict(torch.load('checkpoints/dar.pth'))
        self.bert.eval()
        self.dar_classifier.eval()

    def extract(self, video_path: str) -> np.ndarray:
        """
        提取文本特征

        Args:
            video_path: 视频文件路径

        Returns:
            features: (T, 25) 文本特征向量序列
        """
        # 1. 音频提取
        audio_path = self._extract_audio(video_path)

        # 2. 语音识别（Whisper）
        transcripts = self._speech_to_text(audio_path)

        features_list = []
        for transcript in transcripts:
            # 3. 文本清洗
            cleaned_text = self._clean_text(transcript['text'])

            # 4. BERT语义编码
            bert_features = self._extract_bert_features(cleaned_text)

            # 5. 对话行为识别
            dar_features = self._extract_dialogue_act(cleaned_text)

            # 6. 统计特征
            stat_features = self._extract_statistical_features(cleaned_text)

            # 拼接特征
            combined = np.concatenate([
                bert_features,    # (12,)
                dar_features,     # (7,)
                stat_features     # (6,)
            ])  # (25,)

            features_list.append(combined)

        return np.array(features_list)  # (T, 25)

    def _speech_to_text(self, audio_path: str) -> List[dict]:
        """使用Whisper进行语音识别"""
        result = self.whisper.transcribe(
            audio_path,
            language='zh',
            task='transcribe'
        )

        # 按10秒切分
        segments = []
        for seg in result['segments']:
            segments.append({
                'start': seg['start'],
                'end': seg['end'],
                'text': seg['text']
            })

        return segments

    def _extract_bert_features(self, text: str) -> np.ndarray:
        """使用BERT提取语义特征"""
        # Tokenization
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)

        # BERT编码
        with torch.no_grad():
            outputs = self.bert(**inputs)
            cls_embedding = outputs.last_hidden_state[:, 0, :]  # (1, 768)

        # 降维到12维
        features = self._project_features(cls_embedding, target_dim=12)

        return features.cpu().numpy().squeeze()  # (12,)

    def _extract_dialogue_act(self, text: str) -> np.ndarray:
        """对话行为识别（陈述、提问、指令等）"""
        inputs = self.tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)

        with torch.no_grad():
            dar_probs = self.dar_classifier(inputs)  # (1, 7)

        return dar_probs.cpu().numpy().squeeze()  # (7,)

    def _extract_statistical_features(self, text: str) -> np.ndarray:
        """提取统计特征"""
        # 1. 关键词密度
        keyword_density = self._calculate_keyword_density(text)

        # 2. 逻辑连接词频率
        logic_word_freq = self._count_logic_words(text)

        # 3. 疑问句比例
        question_ratio = text.count('？') / max(len(text), 1)

        # 4. 平均句长
        sentences = text.split('。')
        avg_sentence_len = np.mean([len(s) for s in sentences if s])

        # 5. 实体词密度
        entity_density = self._calculate_entity_density(text)

        # 6. 情感极性
        sentiment = self._calculate_sentiment(text)

        return np.array([
            keyword_density, logic_word_freq, question_ratio,
            avg_sentence_len, entity_density, sentiment
        ])  # (6,)
```

### 4.5 MMAN融合模型模块

#### 4.5.1 模型结构

```python
class MMANModel(nn.Module):
    """Multi-Modal Attention Network"""

    def __init__(self, config):
        super().__init__()
        self.config = config

        # 模态编码器
        self.modality_encoders = nn.ModuleDict({
            'video': ModalityEncoder(20, 128, dropout=0.1),
            'audio': ModalityEncoder(15, 128, dropout=0.1),
            'text': ModalityEncoder(25, 128, dropout=0.1)
        })

        # 跨模态Transformer
        self.transformer = TransformerEncoder(
            embed_dim=128,
            num_layers=2,
            num_heads=4,
            ff_dim=512,
            dropout=0.1
        )

        # BiLSTM
        self.lstm = nn.LSTM(
            input_size=128,
            hidden_size=128,
            num_layers=2,
            bidirectional=True,
            dropout=0.1,
            batch_first=True
        )

        # 注意力池化
        self.attention_pooling = AttentionPooling(
            input_dim=256,  # BiLSTM双向
            hidden_dim=128
        )

        # 分类头
        self.classifier = nn.Sequential(
            nn.Linear(256 + 7, 256),  # 256 + 7维规则特征
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 7)  # 7类教学风格
        )

    def forward(self, features, rule_features=None, return_attention=False):
        """
        Args:
            features: dict
                - 'video': (B, 20)
                - 'audio': (B, 15)
                - 'text': (B, 25)
            rule_features: (B, 7)
            return_attention: bool

        Returns:
            output_dict: dict
                - 'logits': (B, 7)
                - 'probabilities': (B, 7)
                - 'predictions': (B,)
                - 'transformer_attention': (B, 3, 3)
                - 'pooling_attention': (B, 3)
        """
        batch_size = features['video'].size(0)

        # 1. 模态编码
        encoded = []
        for modality in ['video', 'audio', 'text']:
            enc = self.modality_encoders[modality](features[modality])
            encoded.append(enc)

        # 堆叠为序列 (B, 3, 128)
        multimodal_seq = torch.stack(encoded, dim=1)

        # 2. Transformer跨模态融合
        trans_out, trans_attn = self.transformer(multimodal_seq)  # (B, 3, 128)

        # 3. BiLSTM时序建模
        lstm_out, _ = self.lstm(trans_out)  # (B, 3, 256)

        # 4. 注意力池化
        pooled, pool_attn = self.attention_pooling(lstm_out)  # (B, 256)

        # 5. 融合规则特征
        if rule_features is not None:
            pooled = torch.cat([pooled, rule_features], dim=-1)  # (B, 263)

        # 6. 分类
        logits = self.classifier(pooled)  # (B, 7)
        probs = torch.softmax(logits, dim=-1)
        preds = torch.argmax(probs, dim=-1)

        output_dict = {
            'logits': logits,
            'probabilities': probs,
            'predictions': preds
        }

        if return_attention:
            output_dict['transformer_attention'] = trans_attn
            output_dict['pooling_attention'] = pool_attn

        return output_dict
```

#### 4.5.2 推理流程

```python
def inference_pipeline(course_id: int) -> dict:
    """
    完整推理pipeline

    Args:
        course_id: 课程ID

    Returns:
        {
            'style_scores': dict,
            'modality_contrib': dict,
            'attention_weights': dict,
            'features': dict
        }
    """
    # 1. 加载课程
    course = Course.query.get(course_id)
    video_path = course.video_path

    # 2. 特征提取
    video_extractor = VideoFeatureExtractor(device='cuda')
    audio_extractor = AudioFeatureExtractor(device='cuda')
    text_extractor = TextFeatureExtractor(device='cuda')
    rule_extractor = RuleFeatureExtractor()

    video_features = video_extractor.extract(video_path)  # (T, 20)
    audio_features = audio_extractor.extract(video_path)  # (T, 15)
    text_features = text_extractor.extract(video_path)    # (T, 25)
    rule_features = rule_extractor.extract(
        video_features, audio_features, text_features
    )  # (7,)

    # 3. 时间对齐（取平均）
    video_feat = torch.tensor(video_features.mean(axis=0), dtype=torch.float32).unsqueeze(0)
    audio_feat = torch.tensor(audio_features.mean(axis=0), dtype=torch.float32).unsqueeze(0)
    text_feat = torch.tensor(text_features.mean(axis=0), dtype=torch.float32).unsqueeze(0)
    rule_feat = torch.tensor(rule_features, dtype=torch.float32).unsqueeze(0)

    # 4. MMAN推理
    model = MMANModel(config).to('cuda')
    model.load_state_dict(torch.load('checkpoints/best_model.pth'))
    model.eval()

    with torch.no_grad():
        output = model(
            features={
                'video': video_feat.cuda(),
                'audio': audio_feat.cuda(),
                'text': text_feat.cuda()
            },
            rule_features=rule_feat.cuda(),
            return_attention=True
        )

    # 5. 解析结果
    style_names = [
        '理论讲授型', '启发引导型', '互动导向型', '逻辑推导型',
        '题目驱动型', '情感表达型', '耐心细致型'
    ]

    style_scores = {
        name: float(score)
        for name, score in zip(style_names, output['probabilities'].cpu().numpy()[0])
    }

    # 6. 计算模态贡献度
    pool_attn = output['pooling_attention'].cpu().numpy()[0]  # (3,)
    modality_contrib = {
        '视频': float(pool_attn[0]),
        '音频': float(pool_attn[1]),
        '文本': float(pool_attn[2])
    }

    return {
        'style_scores': style_scores,
        'modality_contrib': modality_contrib,
        'attention_weights': output['transformer_attention'].cpu().numpy(),
        'features': {
            'video': video_features,
            'audio': audio_features,
            'text': text_features,
            'rule': rule_features
        }
    }
```

### 4.6 可解释性分析模块

```python
class SHAPExplainer:
    """SHAP可解释性分析器"""

    def __init__(self, model, background_data):
        self.model = model
        self.explainer = shap.DeepExplainer(model, background_data)

    def explain(self, features) -> dict:
        """
        生成SHAP解释

        Returns:
            {
                'global_importance': dict,
                'local_explanation': dict
            }
        """
        # 1. 计算SHAP值
        shap_values = self.explainer.shap_values(features)

        # 2. 全局特征重要性
        global_importance = self._calculate_global_importance(shap_values)

        # 3. 局部样本解释
        local_explanation = self._generate_local_explanation(
            shap_values, features
        )

        return {
            'global_importance': global_importance,
            'local_explanation': local_explanation
        }

    def _calculate_global_importance(self, shap_values) -> dict:
        """计算全局特征重要性"""
        mean_abs_shap = np.abs(shap_values).mean(axis=0)

        feature_names = (
            [f"V_{i+1}" for i in range(20)] +
            [f"A_{i+1}" for i in range(15)] +
            [f"T_{i+1}" for i in range(25)]
        )

        # 排序
        indices = np.argsort(mean_abs_shap)[::-1]

        importance = {
            feature_names[i]: float(mean_abs_shap[i])
            for i in indices[:20]  # Top 20
        }

        return importance

    def _generate_local_explanation(self, shap_values, features) -> dict:
        """生成局部样本解释"""
        # 取第一个样本
        sample_shap = shap_values[0]
        sample_feat = features[0]

        feature_names = (
            [f"V_{i+1}" for i in range(20)] +
            [f"A_{i+1}" for i in range(15)] +
            [f"T_{i+1}" for i in range(25)]
        )

        # 构建贡献列表
        contributions = []
        for name, shap_val, feat_val in zip(feature_names, sample_shap, sample_feat):
            contributions.append({
                'feature': name,
                'value': float(feat_val),
                'contribution': float(shap_val)
            })

        # 按贡献值排序
        contributions.sort(key=lambda x: abs(x['contribution']), reverse=True)

        return {
            'top_positive': [c for c in contributions if c['contribution'] > 0][:5],
            'top_negative': [c for c in contributions if c['contribution'] < 0][:5]
        }
```

---

## 5. 接口设计

### 5.1 RESTful API设计

#### 5.1.1 API基本规范

- **基础URL**: `https://api.example.com/v1`
- **协议**: HTTPS
- **数据格式**: JSON
- **认证方式**: JWT Bearer Token
- **字符编码**: UTF-8

#### 5.1.2 通用响应格式

```json
{
  "code": 200,
  "message": "success",
  "data": {},
  "timestamp": 1706342400
}
```

#### 5.1.3 错误码定义

| 错误码 | 说明 |
|-------|-----|
| 200 | 成功 |
| 400 | 请求参数错误 |
| 401 | 未认证 |
| 403 | 无权限 |
| 404 | 资源不存在 |
| 409 | 资源冲突 |
| 429 | 请求过于频繁 |
| 500 | 服务器内部错误 |

### 5.2 主要API端点

#### 5.2.1 用户相关API

**POST /auth/register - 用户注册**

请求体：
```json
{
  "username": "zhangsan",
  "email": "zhangsan@example.com",
  "password": "password123"
}
```

响应：
```json
{
  "code": 200,
  "message": "注册成功",
  "data": {
    "user_id": 1,
    "username": "zhangsan",
    "email": "zhangsan@example.com"
  }
}
```

**POST /auth/login - 用户登录**

请求体：
```json
{
  "username": "zhangsan",
  "password": "password123"
}
```

响应：
```json
{
  "code": 200,
  "message": "登录成功",
  "data": {
    "token": "eyJ0eXAiOiJKV1QiLCJhbGc...",
    "user": {
      "id": 1,
      "username": "zhangsan",
      "email": "zhangsan@example.com"
    }
  }
}
```

#### 5.2.2 课程相关API

**POST /courses - 上传课程**

请求头：
```
Authorization: Bearer <token>
Content-Type: multipart/form-data
```

请求体：
```
file: <video file>
name: "高中数学-导数的应用"
teacher_name: "李老师"
subject: "数学"
grade: "高中"
```

响应：
```json
{
  "code": 200,
  "message": "上传成功",
  "data": {
    "course_id": 123,
    "task_id": "abc123-def456",
    "status": "processing"
  }
}
```

**GET /courses - 获取课程列表**

请求参数：
- `page`: 页码（默认1）
- `page_size`: 每页数量（默认20）
- `status`: 筛选状态（可选）

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "total": 50,
    "page": 1,
    "page_size": 20,
    "courses": [
      {
        "id": 123,
        "name": "高中数学-导数的应用",
        "teacher_name": "李老师",
        "status": "completed",
        "created_at": "2026-01-20T10:30:00Z"
      }
    ]
  }
}
```

**GET /courses/{course_id} - 获取课程详情**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "id": 123,
    "name": "高中数学-导数的应用",
    "teacher_name": "李老师",
    "subject": "数学",
    "grade": "高中",
    "video_url": "/api/videos/123/stream",
    "video_duration": 2400,
    "thumbnail": "/uploads/thumbnails/123.jpg",
    "status": "completed",
    "created_at": "2026-01-20T10:30:00Z"
  }
}
```

#### 5.2.3 分析相关API

**GET /courses/{course_id}/analysis - 获取分析结果**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "id": 456,
    "course_id": 123,
    "style_scores": {
      "理论讲授型": 0.78,
      "启发引导型": 0.45,
      "互动导向型": 0.32,
      "逻辑推导型": 0.65,
      "题目驱动型": 0.56,
      "情感表达型": 0.23,
      "耐心细致型": 0.51
    },
    "modality_contrib": {
      "视频": 0.33,
      "音频": 0.28,
      "文本": 0.27,
      "规则": 0.12
    },
    "processing_time": 325.6,
    "finished_at": "2026-01-20T10:35:00Z"
  }
}
```

**GET /courses/{course_id}/segments - 获取典型片段**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "segments": [
      {
        "id": 789,
        "start_time": 323.5,
        "end_time": 345.2,
        "label": "高频提问场景",
        "confidence": 0.89,
        "shap_contrib": 0.31,
        "thumbnail": "/uploads/segments/789.jpg"
      }
    ]
  }
}
```

**GET /courses/{course_id}/shap - 获取SHAP解释**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "global_importance": {
      "V_05": 0.24,
      "A_03": 0.18,
      "T_12": 0.15
    },
    "local_explanation": {
      "top_positive": [
        {
          "feature": "V_05",
          "value": 0.78,
          "contribution": 0.31
        }
      ],
      "top_negative": [
        {
          "feature": "A_08",
          "value": 0.12,
          "contribution": -0.08
        }
      ]
    }
  }
}
```

**GET /courses/{course_id}/feedback - 获取反馈建议**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "smi_score": 0.68,
    "target_style": "启发引导型",
    "suggestions": [
      {
        "title": "增加提问频次",
        "current": "平均每10分钟提问2.3次",
        "target": "每10分钟提问4-6次",
        "strategy": "在知识点讲解后设计2-3个渐进式问题",
        "priority": "high"
      }
    ]
  }
}
```

#### 5.2.4 任务相关API

**GET /tasks/{task_id} - 查询任务状态**

响应：
```json
{
  "code": 200,
  "message": "success",
  "data": {
    "task_id": "abc123-def456",
    "status": "PROCESSING",
    "progress": 65,
    "current_step": "特征提取",
    "estimated_time": 180
  }
}
```

---

## 6. 关键算法设计

### 6.1 DeepSORT追踪算法

DeepSORT结合了外观特征和运动模型，实现稳定的多目标跟踪。

#### 算法流程

```
输入: 当前帧检测框 D_t = {d_1, d_2, ..., d_n}
      历史轨迹 T_{t-1} = {t_1, t_2, ..., t_m}

1. 预测阶段:
   对每条轨迹 t_i，使用卡尔曼滤波预测下一帧位置:
   x_{t|t-1} = F * x_{t-1|t-1}
   P_{t|t-1} = F * P_{t-1|t-1} * F^T + Q

2. 匹配阶段:
   (1) 计算代价矩阵 C:
       C[i,j] = λ * (1 - cos_sim(f_i, f_j)) + (1-λ) * (1 - IoU(b_i, b_j))
       其中 f_i 是外观特征（ResNet50提取的128维向量）
            b_i 是边界框

   (2) 使用匈牙利算法求解最优分配:
       assignment = Hungarian(C)

3. 更新阶段:
   对匹配成功的轨迹，使用卡尔曼滤波更新:
   K_t = P_{t|t-1} * H^T * (H * P_{t|t-1} * H^T + R)^{-1}
   x_{t|t} = x_{t|t-1} + K_t * (z_t - H * x_{t|t-1})
   P_{t|t} = (I - K_t * H) * P_{t|t-1}

4. 轨迹管理:
   - 未匹配的检测框创建新轨迹
   - 未匹配的轨迹进入待定状态
   - 连续N帧未匹配的轨迹删除

输出: 更新后的轨迹 T_t
```

### 6.2 ST-GCN时空图卷积

ST-GCN在时空图上进行卷积，捕获骨架序列的动作模式。

#### 图构建

**空间图**：将33个关键点作为节点，人体骨骼连接作为边。

邻接矩阵 $A_s \in \mathbb{R}^{33 \times 33}$:
$$A_s[i,j] = \begin{cases}
1, & \text{if } (i,j) \text{ connected by bone} \\
0, & \text{otherwise}
\end{cases}$$

**时间图**：相邻帧的同一关键点连接。

邻接矩阵 $A_t \in \mathbb{R}^{T \times T}$:
$$A_t[i,j] = \begin{cases}
1, & \text{if } |i-j| = 1 \\
0, & \text{otherwise}
\end{cases}$$

#### 时空图卷积

$$\mathbf{f}_{out} = \Lambda^{-\frac{1}{2}} (A + I) \Lambda^{-\frac{1}{2}} \mathbf{f}_{in} W$$

其中：
- $A$ 是时空邻接矩阵
- $I$ 是单位矩阵
- $\Lambda$ 是度矩阵
- $W$ 是可学习权重

### 6.3 MMAN跨模态注意力机制

#### Multi-Head Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$

其中：
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

#### AttentionPooling

$$\alpha = \text{softmax}(W_a \cdot \tanh(W_h \cdot H))$$

$$g = \sum_{i=1}^{n} \alpha_i H_i$$

其中：
- $H \in \mathbb{R}^{n \times d}$ 是输入序列
- $W_a, W_h$ 是可学习参数
- $\alpha \in \mathbb{R}^n$ 是注意力权重
- $g \in \mathbb{R}^d$ 是聚合向量

### 6.4 SHAP值计算

SHAP基于Shapley值理论，计算每个特征对模型输出的贡献。

对于特征 $i$，其SHAP值为：

$$\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (|F| - |S| - 1)!}{|F|!} [f(S \cup \{i\}) - f(S)]$$

其中：
- $F$ 是全部特征集合
- $S$ 是特征子集
- $f(S)$ 是使用特征集 $S$ 的模型输出

---

## 7. 性能设计

### 7.1 性能指标

| 指标 | 目标值 | 实际值 |
|-----|--------|--------|
| 单视频分析耗时（10秒片段） | < 2秒 | 1.1秒 |
| 批量处理吞吐量 | > 20节课/小时 | 35节课/小时 |
| 并发用户数 | > 30 | 50 |
| 系统响应时间（API） | < 500ms | 平均230ms |
| GPU显存占用 | < 8GB | 6.5GB |

### 7.2 性能优化策略

#### 7.2.1 模型优化

1. **模型量化**: 将FP32模型量化为INT8，减少显存占用和推理时间
2. **模型剪枝**: 移除冗余参数，减小模型大小
3. **批处理**: 多个样本批量推理，提升GPU利用率
4. **TensorRT加速**: 使用NVIDIA TensorRT优化推理性能

#### 7.2.2 缓存策略

1. **特征缓存**: 将提取的特征向量缓存到Redis，避免重复计算
2. **结果缓存**: 分析结果缓存24小时
3. **模型缓存**: 模型加载后常驻内存

#### 7.2.3 异步处理

1. **Celery任务队列**: 视频分析异步执行
2. **多Worker**: 启动多个Celery Worker并行处理
3. **任务优先级**: 根据视频大小和用户等级设置优先级

---

## 8. 安全设计

### 8.1 认证与授权

- **JWT Token**: 用户登录后颁发JWT令牌，有效期7天
- **刷新Token**: 支持Token刷新机制
- **RBAC权限**: 基于角色的访问控制（管理员/普通用户）

### 8.2 数据安全

- **密码加密**: 使用bcrypt哈希存储密码
- **HTTPS加密**: 全站HTTPS传输
- **SQL注入防护**: 使用ORM参数化查询
- **XSS防护**: 前端输入过滤和输出转义
- **CSRF防护**: CSRF Token验证

### 8.3 频率限制

- **登录限制**: 同一IP每分钟最多5次登录尝试
- **上传限制**: 每用户每天最多上传10个视频
- **API限制**: 每用户每分钟最多100次API请求

---

**文档版本**: V1.0
**最后更新**: 2026年01月27日
**编制单位**: [申请人姓名]
**版权所有**: 保留所有权利
