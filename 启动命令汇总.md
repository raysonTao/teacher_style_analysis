# æ•™å¸ˆé£æ ¼åˆ†æç³»ç»Ÿ - å¯åŠ¨å‘½ä»¤æ±‡æ€»

æœ¬æ–‡æ¡£æ•´ç†äº†å®Œæ•´çš„ç³»ç»Ÿå¯åŠ¨å‘½ä»¤ï¼ŒåŒ…æ‹¬ç‰¹å¾æå–ã€VLMæ ‡æ³¨ã€æ¨¡å‹è®­ç»ƒç­‰æ‰€æœ‰æµç¨‹ã€‚

---

## ğŸ“‹ ç›®å½•

1. [å•è§†é¢‘æµ‹è¯•æµç¨‹](#å•è§†é¢‘æµ‹è¯•æµç¨‹)
2. [æ‰¹é‡å¤„ç†æµç¨‹](#æ‰¹é‡å¤„ç†æµç¨‹)
3. [å®Œæ•´è®­ç»ƒæµç¨‹](#å®Œæ•´è®­ç»ƒæµç¨‹)
4. [æ¨¡å‹æ¨ç†](#æ¨¡å‹æ¨ç†)
5. [éªŒè¯æµ‹è¯•](#éªŒè¯æµ‹è¯•)
6. [å·¥å…·è„šæœ¬](#å·¥å…·è„šæœ¬)

---

## ğŸ¯ å•è§†é¢‘æµ‹è¯•æµç¨‹

### 1. æµ‹è¯•ç‰¹å¾æå–ï¼ˆä»…æ•™å¸ˆæ•°æ®ï¼‰

```bash
# æµ‹è¯•æ–°çš„æ•™å¸ˆè¯†åˆ«åŠŸèƒ½
python test_teacher_detection.py

# é¢„æœŸè¾“å‡ºï¼š
# - è®°å½•æ•°: ~2,143 æ¡ï¼ˆä»17,960å‡å°‘ï¼‰
# - æ–‡ä»¶å¤§å°: ~5-7 MBï¼ˆä»30MBå‡å°‘ï¼‰
# - Yåæ ‡èŒƒå›´: 150-300ï¼ˆæ•™å¸ˆåŒºåŸŸï¼‰
```

### 2. æå–å•ä¸ªè§†é¢‘ç‰¹å¾

```bash
# æ–¹æ³•1: ä½¿ç”¨FeatureExtractorï¼ˆæ¨èï¼‰
python -c "
from src.features.feature_extractor import FeatureExtractor
import json

extractor = FeatureExtractor()
features = extractor.process_video(
    'data/custom/videos/é«˜ä¸­è¯­æ–‡ä¼˜è´¨è¯¾ã€Šå£°å£°æ…¢ã€‹ï¼ˆå«è¯¾ä»¶æ•™æ¡ˆï¼‰.mp4',
    return_raw=True
)

with open('output_features.json', 'w', encoding='utf-8') as f:
    json.dump(features, f, ensure_ascii=False, indent=2)

print('ç‰¹å¾æå–å®Œæˆï¼')
"

# æ–¹æ³•2: ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
python data/custom/tools/extract_features.py \
    --videos_dir data/custom/videos \
    --output data/custom/extracted_features/features_single.json \
    --max_videos 1
```

### 3. VLMæ ‡æ³¨å•ä¸ªè§†é¢‘

```bash
# å¯¹å•ä¸ªè§†é¢‘è¿›è¡ŒVLMæ ‡æ³¨
python data/custom/tools/annotate_videos.py \
    --features_path data/custom/extracted_features/features_single.json \
    --output data/custom/vlm_annotations/annotation_single.json \
    --model opus \
    --max_samples 1

# æŸ¥çœ‹æ ‡æ³¨ç»“æœ
cat data/custom/vlm_annotations/annotation_single.json | jq .
```

---

## ğŸ”„ æ‰¹é‡å¤„ç†æµç¨‹

### 1. æ‰¹é‡æå–ç‰¹å¾ï¼ˆå¤šä¸ªè§†é¢‘ï¼‰

```bash
# æå–ç›®å½•ä¸‹æ‰€æœ‰è§†é¢‘çš„ç‰¹å¾
python data/custom/tools/extract_features.py \
    --videos_dir data/custom/videos \
    --output data/custom/extracted_features/features_batch.json \
    --sample_interval 30 \
    --max_videos 0  # 0è¡¨ç¤ºå¤„ç†æ‰€æœ‰è§†é¢‘

# åªå¤„ç†å‰10ä¸ªè§†é¢‘
python data/custom/tools/extract_features.py \
    --videos_dir data/custom/videos \
    --output data/custom/extracted_features/features_10.json \
    --max_videos 10
```

**å‚æ•°è¯´æ˜**ï¼š
- `--videos_dir`: è§†é¢‘ç›®å½•
- `--output`: è¾“å‡ºJSONæ–‡ä»¶è·¯å¾„
- `--sample_interval`: é‡‡æ ·é—´éš”ï¼ˆé»˜è®¤30å¸§ï¼‰
- `--max_videos`: æœ€å¤§å¤„ç†æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰

### 2. æ‰¹é‡VLMæ ‡æ³¨

```bash
# æ ‡æ³¨æ‰€æœ‰æ ·æœ¬ï¼ˆä½¿ç”¨Opusæ¨¡å‹ï¼Œé«˜è´¨é‡ï¼‰
python data/custom/tools/annotate_videos.py \
    --features_path data/custom/extracted_features/features_batch.json \
    --output data/custom/vlm_annotations/annotations_batch.json \
    --model opus \
    --max_samples 0 \
    --num_keyframes 10

# ä½¿ç”¨Sonnetæ¨¡å‹ï¼ˆæ›´å¿«ï¼Œç¨ä½æˆæœ¬ï¼‰
python data/custom/tools/annotate_videos.py \
    --features_path data/custom/extracted_features/features_batch.json \
    --output data/custom/vlm_annotations/annotations_batch.json \
    --model sonnet \
    --max_samples 0
```

**å‚æ•°è¯´æ˜**ï¼š
- `--features_path`: ç‰¹å¾æ–‡ä»¶è·¯å¾„
- `--output`: æ ‡æ³¨è¾“å‡ºè·¯å¾„
- `--model`: VLMæ¨¡å‹ï¼ˆopus/sonnetï¼‰
- `--max_samples`: æœ€å¤§æ ‡æ³¨æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼‰
- `--num_keyframes`: æ¯ä¸ªæ ·æœ¬ä½¿ç”¨çš„å…³é”®å¸§æ•°é‡ï¼ˆé»˜è®¤10ï¼‰

### 3. ä¸€é”®æ‰¹é‡å¤„ç†è„šæœ¬

```bash
# å®Œæ•´æ‰¹é‡å¤„ç†ï¼šç‰¹å¾æå– + VLMæ ‡æ³¨
bash data/custom/tools/process_custom_videos.sh

# è¯¥è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
# 1. æå–æ‰€æœ‰è§†é¢‘ç‰¹å¾
# 2. ä½¿ç”¨VLMè¿›è¡Œæ ‡æ³¨
# 3. ç”Ÿæˆè®­ç»ƒæ•°æ®æ ¼å¼
```

---

## ğŸ“ å®Œæ•´è®­ç»ƒæµç¨‹

### æ­¥éª¤1: å‡†å¤‡è®­ç»ƒæ•°æ®

```bash
# å°†VLMæ ‡æ³¨ç»“æœè½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
python data/custom/tools/convert_to_training.py \
    --annotations data/custom/vlm_annotations/annotations_batch.json \
    --features data/custom/extracted_features/features_batch.json \
    --output data/custom/training_data/training.json \
    --train_ratio 0.7 \
    --val_ratio 0.15 \
    --test_ratio 0.15 \
    --min_confidence 0.5

# æŸ¥çœ‹è®­ç»ƒæ•°æ®ç»Ÿè®¡
python -c "
import json
with open('data/custom/training_data/training.json') as f:
    data = json.load(f)
    train = [x for x in data if x['split'] == 'train']
    val = [x for x in data if x['split'] == 'val']
    test = [x for x in data if x['split'] == 'test']
    print(f'è®­ç»ƒé›†: {len(train)}')
    print(f'éªŒè¯é›†: {len(val)}')
    print(f'æµ‹è¯•é›†: {len(test)}')
    print(f'æ€»è®¡: {len(data)}')
"
```

**å‚æ•°è¯´æ˜**ï¼š
- `--train_ratio`: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.7ï¼‰
- `--val_ratio`: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼‰
- `--test_ratio`: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼‰
- `--min_confidence`: æœ€ä½VLMç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰

### æ­¥éª¤2: è®­ç»ƒMMANæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python -m src.models.deep_learning.train \
    --data_path data/custom/training_data/training.json \
    --model_config default \
    --batch_size 32 \
    --num_epochs 150 \
    --lr 5e-5 \
    --device cuda \
    --checkpoint_dir ./checkpoints/custom \
    --log_dir ./logs/custom \
    --early_stopping 15

# ä½¿ç”¨é«˜ç²¾åº¦é…ç½®è®­ç»ƒ
python -m src.models.deep_learning.train \
    --data_path data/custom/training_data/training.json \
    --model_config high_accuracy \
    --batch_size 16 \
    --num_epochs 200 \
    --lr 3e-5 \
    --device cuda \
    --checkpoint_dir ./checkpoints/custom_high \
    --log_dir ./logs/custom_high \
    --early_stopping 20

# CPUè®­ç»ƒï¼ˆè¾ƒæ…¢ï¼‰
python -m src.models.deep_learning.train \
    --data_path data/custom/training_data/training.json \
    --model_config default \
    --batch_size 8 \
    --num_epochs 150 \
    --device cpu \
    --checkpoint_dir ./checkpoints/custom_cpu \
    --log_dir ./logs/custom_cpu
```

**å‚æ•°è¯´æ˜**ï¼š
- `--model_config`: æ¨¡å‹é…ç½®ï¼ˆdefault/high_accuracy/lightweightï¼‰
- `--batch_size`: æ‰¹æ¬¡å¤§å°ï¼ˆGPUæ¨è32ï¼ŒCPUæ¨è8ï¼‰
- `--num_epochs`: è®­ç»ƒè½®æ•°
- `--lr`: å­¦ä¹ ç‡
- `--device`: è®¾å¤‡ï¼ˆcuda/cpuï¼‰
- `--early_stopping`: æ—©åœè½®æ•°

### æ­¥éª¤3: ç›‘æ§è®­ç»ƒè¿›åº¦

```bash
# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f logs/custom/training.log

# æŸ¥çœ‹æœ€æ–°çš„è®­ç»ƒæŒ‡æ ‡
tail -20 logs/custom/training.log | grep "Epoch"

# ä½¿ç”¨TensorBoardï¼ˆå¦‚æœå®‰è£…äº†ï¼‰
tensorboard --logdir logs/custom
```

---

## ğŸš€ æ¨¡å‹æ¨ç†

### 1. ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ¨ç†

```bash
# å¯¹æ–°è§†é¢‘è¿›è¡Œæ¨ç†
python -m src.main analyze \
    --video data/custom/videos/new_video.mp4 \
    --mode deep_learning \
    --model_path checkpoints/custom/best_model.pth \
    --device cuda \
    --output results/new_video_result.json

# æ‰¹é‡æ¨ç†
python -m src.main analyze \
    --video_dir data/custom/videos/test_videos/ \
    --mode deep_learning \
    --model_path checkpoints/custom/best_model.pth \
    --device cuda \
    --output_dir results/batch_results/
```

### 2. ç›´æ¥ä½¿ç”¨VLMæ¨ç†ï¼ˆä¸éœ€è¦è®­ç»ƒï¼‰

```bash
# å•ä¸ªè§†é¢‘VLMæ¨ç†
python test_vlm_annotation.py

# ä¿®æ”¹è§†é¢‘è·¯å¾„åè¿è¡Œ
python -c "
from src.annotation.vlm_annotator import VLMAnnotator
from src.features.feature_extractor import FeatureExtractor
import json

# æå–ç‰¹å¾
extractor = FeatureExtractor()
features = extractor.process_video('path/to/video.mp4', return_raw=True)

# VLMæ ‡æ³¨
annotator = VLMAnnotator(model='opus')
annotation = annotator.annotate_single_sample(features)

print(json.dumps(annotation, ensure_ascii=False, indent=2))
"
```

---

## âœ… éªŒè¯æµ‹è¯•

### 1. éªŒè¯æ•™å¸ˆè¯†åˆ«åŠŸèƒ½

```bash
# æµ‹è¯•æ•™å¸ˆè¯†åˆ«
python test_teacher_detection.py

# éªŒè¯ç‚¹ï¼š
# âœ“ æ¯å¸§åªæœ‰1æ¡è®°å½•
# âœ“ Yåæ ‡å¹³å‡å€¼ < 300
# âœ“ æ–‡ä»¶å¤§å°å‡å°‘çº¦80%
```

### 2. éªŒè¯VLMæ ‡æ³¨è´¨é‡

```bash
# æŸ¥çœ‹VLMæ ‡æ³¨çš„ç½®ä¿¡åº¦åˆ†å¸ƒ
python -c "
import json
import numpy as np

with open('data/custom/vlm_annotations/annotations_batch.json') as f:
    data = json.load(f)
    confidences = [item['annotation']['confidence'] for item in data]

    print(f'æ ·æœ¬æ•°é‡: {len(confidences)}')
    print(f'å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.2f}')
    print(f'æœ€ä½ç½®ä¿¡åº¦: {min(confidences):.2f}')
    print(f'æœ€é«˜ç½®ä¿¡åº¦: {max(confidences):.2f}')
    print(f'ç½®ä¿¡åº¦ > 0.7: {sum(c > 0.7 for c in confidences)} ä¸ª')
"
```

### 3. éªŒè¯æ¨¡å‹æ€§èƒ½

```bash
# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
python -c "
from src.models.deep_learning.train import evaluate_model
import torch
import json

# åŠ è½½æ¨¡å‹å’Œæ•°æ®
model = torch.load('checkpoints/custom/best_model.pth')
with open('data/custom/training_data/training.json') as f:
    test_data = [x for x in json.load(f) if x['split'] == 'test']

# è¯„ä¼°
results = evaluate_model(model, test_data)
print(f'æµ‹è¯•é›†å‡†ç¡®ç‡: {results[\"accuracy\"]:.2f}')
print(f'æµ‹è¯•é›†F1åˆ†æ•°: {results[\"f1\"]:.2f}')
"
```

### 4. å¯¹æ¯”VLM vs Studentæ¨¡å‹

```bash
# åˆ›å»ºå¯¹æ¯”æµ‹è¯•è„šæœ¬
python data/custom/tools/validate_student_model.py \
    --model_path checkpoints/custom/best_model.pth \
    --test_videos data/custom/videos/test_videos/ \
    --output comparison_results.json

# æŸ¥çœ‹ä¸€è‡´æ€§
cat comparison_results.json | jq '.agreement_rate'
```

---

## ğŸ› ï¸ å·¥å…·è„šæœ¬

### 1. æ¸…ç†ä¸­é—´æ–‡ä»¶

```bash
# æ¸…ç†æå–çš„ç‰¹å¾
rm -rf data/custom/extracted_features/*.json

# æ¸…ç†VLMæ ‡æ³¨
rm -rf data/custom/vlm_annotations/*.json

# æ¸…ç†è®­ç»ƒæ•°æ®
rm -rf data/custom/training_data/*.json

# æ¸…ç†æ¨¡å‹æ£€æŸ¥ç‚¹
rm -rf checkpoints/custom/*

# æ¸…ç†æ—¥å¿—
rm -rf logs/custom/*
```

### 2. æ£€æŸ¥æ–‡ä»¶å¤§å°

```bash
# æ£€æŸ¥ç‰¹å¾æ–‡ä»¶å¤§å°
du -h data/custom/extracted_features/*.json

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤§å°
du -h checkpoints/custom/*.pth

# æ£€æŸ¥ç›®å½•æ€»å¤§å°
du -sh data/custom/extracted_features/
du -sh data/custom/vlm_annotations/
du -sh checkpoints/custom/
```

### 3. ç»Ÿè®¡è§†é¢‘æ•°é‡

```bash
# ç»Ÿè®¡è§†é¢‘æ–‡ä»¶
find data/custom/videos/ -name "*.mp4" | wc -l

# åˆ—å‡ºæ‰€æœ‰è§†é¢‘
find data/custom/videos/ -name "*.mp4" -exec basename {} \;

# ç»Ÿè®¡æ€»æ—¶é•¿ï¼ˆéœ€è¦ffprobeï¼‰
find data/custom/videos/ -name "*.mp4" -exec ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 {} \; | awk '{s+=$1} END {print s/60 " åˆ†é’Ÿ"}'
```

### 4. Gitæäº¤

```bash
# æŸ¥çœ‹ä¿®æ”¹
git status

# æ·»åŠ ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆä¸åŒ…æ‹¬å¤§æ–‡ä»¶ï¼‰
git add src/features/video_feature_extractor.py
git add test_teacher_detection.py
git add æ•™å¸ˆè¯†åˆ«åŠŸèƒ½è¯´æ˜.md
git add å¯åŠ¨å‘½ä»¤æ±‡æ€».md

# æäº¤
git commit -m "å®ç°æ•™å¸ˆè¯†åˆ«åŠŸèƒ½ï¼Œä¼˜åŒ–ç‰¹å¾æ–‡ä»¶å¤§å°

- æ–°å¢æ•™å¸ˆé€‰æ‹©ç®—æ³•ï¼ˆä½ç½®+å¤§å°åŠ æƒè¯„åˆ†ï¼‰
- æ¯å¸§åªä¿ç•™1æ¡æ•™å¸ˆè®°å½•ï¼Œæ•°æ®å‹ç¼©88%
- æ–‡ä»¶å¤§å°ä»30MBå‡å°‘åˆ°~5-7MB
- æ·»åŠ æµ‹è¯•è„šæœ¬å’Œæ–‡æ¡£

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

# æ¨é€åˆ°è¿œç¨‹
git push origin fix/mediapipe-v0.10-api-upgrade
```

---

## ğŸ“Š å¸¸ç”¨ç»„åˆå‘½ä»¤

### åœºæ™¯1: å¿«é€Ÿæµ‹è¯•å•ä¸ªè§†é¢‘

```bash
# ä¸€æ¡å‘½ä»¤æµ‹è¯•å®Œæ•´æµç¨‹
python -c "
from src.features.feature_extractor import FeatureExtractor
from src.annotation.vlm_annotator import VLMAnnotator
import json

# æå–ç‰¹å¾
print('1. æå–ç‰¹å¾...')
extractor = FeatureExtractor()
features = extractor.process_video('data/custom/videos/test.mp4', return_raw=True)

# VLMæ ‡æ³¨
print('2. VLMæ ‡æ³¨...')
annotator = VLMAnnotator(model='opus')
annotation = annotator.annotate_single_sample(features)

# è¾“å‡ºç»“æœ
print('3. ç»“æœ:')
print(f'   é£æ ¼: {annotation[\"annotation\"][\"style\"]}')
print(f'   ç½®ä¿¡åº¦: {annotation[\"annotation\"][\"confidence\"]:.2f}')
print(f'   ç†ç”±: {annotation[\"annotation\"][\"reasoning\"][:100]}...')
"
```

### åœºæ™¯2: å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆ500ä¸ªè§†é¢‘ï¼‰

```bash
# æ­¥éª¤1: æ‰¹é‡å¤„ç†
bash data/custom/tools/process_custom_videos.sh

# æ­¥éª¤2: è½¬æ¢è®­ç»ƒæ ¼å¼
python data/custom/tools/convert_to_training.py \
    --annotations data/custom/vlm_annotations/annotations.json \
    --features data/custom/extracted_features/features.json \
    --output data/custom/training_data/training.json \
    --min_confidence 0.5

# æ­¥éª¤3: è®­ç»ƒæ¨¡å‹
python -m src.models.deep_learning.train \
    --data_path data/custom/training_data/training.json \
    --model_config high_accuracy \
    --device cuda \
    --checkpoint_dir ./checkpoints/custom_500 \
    --log_dir ./logs/custom_500

# æ­¥éª¤4: è¯„ä¼°æ¨¡å‹
python data/custom/tools/validate_student_model.py \
    --model_path checkpoints/custom_500/best_model.pth \
    --test_videos data/custom/videos/test_videos/
```

### åœºæ™¯3: å¢é‡è®­ç»ƒï¼ˆæ·»åŠ æ›´å¤šæ•°æ®ï¼‰

```bash
# å¤„ç†æ–°è§†é¢‘
python data/custom/tools/extract_features.py \
    --videos_dir data/custom/videos/new_batch \
    --output data/custom/extracted_features/features_new.json

# æ ‡æ³¨æ–°è§†é¢‘
python data/custom/tools/annotate_videos.py \
    --features_path data/custom/extracted_features/features_new.json \
    --output data/custom/vlm_annotations/annotations_new.json \
    --model opus

# åˆå¹¶æ•°æ®
python -c "
import json

# åŠ è½½æ—§æ•°æ®
with open('data/custom/training_data/training.json') as f:
    old_data = json.load(f)

# è½¬æ¢æ–°æ•°æ®
# ... (è¿è¡Œconvert_to_training.py)

# åŠ è½½æ–°æ•°æ®
with open('data/custom/training_data/training_new.json') as f:
    new_data = json.load(f)

# åˆå¹¶
combined = old_data + new_data

# ä¿å­˜
with open('data/custom/training_data/training_combined.json', 'w') as f:
    json.dump(combined, f, ensure_ascii=False, indent=2)

print(f'æ€»æ ·æœ¬æ•°: {len(combined)}')
"

# é‡æ–°è®­ç»ƒ
python -m src.models.deep_learning.train \
    --data_path data/custom/training_data/training_combined.json \
    --model_config high_accuracy \
    --device cuda \
    --checkpoint_dir ./checkpoints/custom_incremental
```

---

## âš™ï¸ ç¯å¢ƒé…ç½®

### ç¡®è®¤ä¾èµ–å®‰è£…

```bash
# æ£€æŸ¥Pythonç¯å¢ƒ
python --version  # éœ€è¦ Python 3.8+

# æ£€æŸ¥å…³é”®ä¾èµ–
python -c "
import torch
import cv2
import mediapipe
import whisper
import anthropic
print('âœ“ æ‰€æœ‰ä¾èµ–å·²å®‰è£…')
"

# å¦‚æœç¼ºå°‘ä¾èµ–ï¼Œå®‰è£…
pip install -r requirements.txt
```

### GPUæ£€æŸ¥

```bash
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
python -c "
import torch
print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'GPUæ•°é‡: {torch.cuda.device_count()}')
    print(f'GPUåç§°: {torch.cuda.get_device_name(0)}')
"

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
nvidia-smi
```

### APIé…ç½®

```bash
# è®¾ç½®Anthropic APIå¯†é’¥
export ANTHROPIC_API_KEY="your-api-key-here"

# æˆ–åˆ›å»º.envæ–‡ä»¶
echo "ANTHROPIC_API_KEY=your-api-key-here" > .env

# éªŒè¯API
python -c "
import anthropic
import os
client = anthropic.Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY'))
print('âœ“ APIé…ç½®æ­£ç¡®')
"
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **æŠ€æœ¯æ–‡æ¡£**: `data/custom/çŸ¥è¯†è’¸é¦æŠ€æœ¯æ–‡æ¡£.md`
- **å¿«é€Ÿå¼€å§‹**: `data/custom/QUICKSTART.md`
- **æµ‹è¯•ç»“æœ**: `data/custom/æµ‹è¯•è¿è¡Œç»“æœè¯´æ˜.md`
- **æ•™å¸ˆè¯†åˆ«**: `æ•™å¸ˆè¯†åˆ«åŠŸèƒ½è¯´æ˜.md`
- **APIé…ç½®**: `data/custom/API_CONFIG.md`
- **å®Œæ•´README**: `data/custom/README.md`

---

## ğŸ†˜ æ•…éšœæ’æŸ¥

### é—®é¢˜1: CUDA out of memory

```bash
# å‡å°batch_size
python -m src.models.deep_learning.train \
    --batch_size 8 \  # ä»32å‡å°‘åˆ°8
    --device cuda

# æˆ–ä½¿ç”¨CPU
python -m src.models.deep_learning.train \
    --device cpu
```

### é—®é¢˜2: VLM APIé”™è¯¯

```bash
# æ£€æŸ¥APIå¯†é’¥
echo $ANTHROPIC_API_KEY

# æ£€æŸ¥é…é¢
python -c "
import anthropic
import os
client = anthropic.Anthropic(api_key=os.environ.get('ANTHROPIC_API_KEY'))
# å°è¯•ç®€å•è¯·æ±‚
response = client.messages.create(
    model='claude-opus-4-5-20251101',
    max_tokens=100,
    messages=[{'role': 'user', 'content': 'test'}]
)
print('âœ“ APIå·¥ä½œæ­£å¸¸')
"
```

### é—®é¢˜3: ç‰¹å¾æå–å¤±è´¥

```bash
# æ£€æŸ¥è§†é¢‘æ–‡ä»¶
ffprobe data/custom/videos/test.mp4

# æ£€æŸ¥MediaPipe
python -c "
import mediapipe as mp
print(f'MediaPipeç‰ˆæœ¬: {mp.__version__}')
"

# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
python test_teacher_detection.py 2>&1 | tee debug.log
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æ›´æ–°æ—¶é—´**: 2026-01-13
**é€‚ç”¨ç‰ˆæœ¬**: æ•™å¸ˆé£æ ¼åˆ†æç³»ç»Ÿ v0.10+
